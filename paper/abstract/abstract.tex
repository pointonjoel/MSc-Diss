\addcontentsline{toc}{chapter}{Abstract}

\begin{abstract} %200-500 words

%An abstract (200-500 words) that summarises what the project is about, what it has achieved, and how it contributes to the development of research and/or technology in the area. This should also include  5 to 10 keywords you think would help someone trying to find your dissertation (e.g., in a web search). Please be careful to enter specific keywords relevant to your dissertation, donâ€™t be too general. We recommend that you include full names and acronyms where appropriate, and separate key word with semi-colons e.g. "Keywords: Human Computer Interaction (HCI); Internet of Things (IoT); autonomous vehicles; user study; qualitative study"

The recently released ChatGPT stunned users with its human-sounding content and ability to assist with a wide range of tasks. However, \acrlong{llm}s (\acrshort{llm}s) such as \acrshort{gpt} generate content in a probabilistic manner and so, at present, cannot guarantee factual answers or provide sources. This paper presents a framework to achieve this, by using a document store to pass relevant information to a chatbot which has been fine-tuned for question-answering. The document store is generated automatically from chosen documents which are split into chunks of 800 tokens.

This paper takes a novel approach to creating a question-answering framework. By adapting existing summarisation fine-tuning techniques, and enhancing Google's long-form Natural Questions dataset using \acrshort{gpt}-3.5 Turbo, a model is fine-tuned to create a generative (and not extractive) question-answering chatbot. Further, this paper contributes a long-form, generative answer variant of the Natural Questions dataset to the literature.

The proposed framework and provided model can, with future development, be used in countless applications. The long-form, closed-domain question-answering chatbot created by this paper can be used to answer questions from any domain, provided that context is given from a document store. Therefore, future adaptations to complete the model could enable it to be used in production, to benefit academics and students by streamlining \acrshort{qa} processes. Other applications such as customer service, legal advice, and other applications are also possible, though not explored.

The model achieves excellent results on the validation set: \acrshort{rouge} scores of between 43 and 45, a \acrshort{bleu} score of 34.2, and a \acrshort{meteor} score of 0.405. Validation answers attain an average cosine similarity of 0.763, with a 10.4\% false negative rate and 30.2\% false positive rate.

All code can be found in the following GitHub repository: \url{https://github.com/pointonjoel/MSc-Diss}, and the proposed model can be found in the following HuggingFace repository: \url{https://huggingface.co/psxjp5/mt5-small}.

\textbf{Keywords}: \acrshort{llm}; \acrfull{nlp}; \acrfull{gpt}; \acrfull{mlm}; Fine-Tuning

\end{abstract}