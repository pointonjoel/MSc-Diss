\documentclass[aspectratio=169,12pt]{beamer} 
\usepackage[utf8]{inputenc}
\usepackage{../beamer_preamble}
\usepackage{import}
\bibliographystyle{ecta} %for harvard
% \setbeamertemplate{section in toc}[sections numbered]
% \setbeamertemplate{subsection in toc}[subsections numbered]

% 12 minute (2-3 minutes for questions)
% Your talk should cover:
% •	The aim(s) of your project 
% •	What has been achieved (including significance and contribution)
% •	How you achieved these outcomes 

% Keep it simple/big picture, don't dive into the technical details too much and ensure someone can understand it without reading your presentation.

\begin{document}
%Title----------------------------------------------
\section{Introduction}
\title{The Use of AI in Education}
\subtitle{Main findings}
\author[Pointon, Joel]{J.~Pointon\\\footnotesize{\textit{Supervised by T.~Chandesa\\Supported by I. Yaqub}}\\ \vspace{20pt}\includegraphics[width=2.3cm]{../images/nottingham-logo.png}}

\date[September 2023] % (optional)
{September 2023}

\frame{\titlepage}

\addtobeamertemplate{frametitle}{}{%
\begin{textblock*}{110mm}(0.935\textwidth,0.91cm)
\includegraphics[height=30.5pt]{../images/nottingham-logo.png}
\end{textblock*}}

%TOC----------------------------------------------
\begin{frame}
\frametitle{Overview} 
    \tableofcontents
\end{frame}

%2 - Theory----------------------------------------------
\begin{frame}[t]{Introduction}
\onslide<1->{
    \begin{itemize}
        \item ChatGPT set a new standard for chatbots in late-2022
        \item Chatbots benefit staff through streamlining Q\&A processes
    \end{itemize}
}
\onslide<2>{
    \begin{itemize}
        \item Generative chatbots can confidently give false answers and incorrectly summarise documents
        \item Students and other users require reliable sources to be used
    \end{itemize}
}
\end{frame}

%3 - Literature----------------------------------------------
\begin{frame}[t]{Literature}
\onslide<1->{
    \begin{itemize}
        \item \citeauthor{vaswani2017attention}'s `Attention is All You Need' paper (\citeyear{vaswani2017attention}) introduced the Transformer model, with many variations since (\acrshort{gpt}, \acrshort{bart}, T5 etc.)
        \item Two approaches to learning: using model weights (\acrfull{mlm}) or model inputs \citep{documentStoreGuu}
        \item Convert text to a vector embedding and use the cosine similarity between the query and documents to feed context via the model input \citep{openai_cookbook_qa_embeddings}
    \end{itemize} %lack of recent research and over time
}
\end{frame}

%4 - Methodology----------------------------------------------
\section{Methodology}
\begin{frame}[t]{Methodology}
\onslide<1->{
    \begin{itemize}
        \item Document store was created using two COMP-2032 course textbooks, split into 800 token sections \citep{gonzalez2018digital, solomon2010fundamentals}
        \item T5 model was fine-tuned for question answering using the Natural Questions Dataset \citep{NQdataset}
        \item Poor answer quality requred data enhancement using \acrshort{gpt}-3.5 Turbo (one-off dependency)
    \end{itemize}
}
% Digital Image Processing textbook \citep{gonzalez2018digital} and validation metrics were computer using the Fundamentals of Digital Image Processing textbook \citep{solomon2010fundamentals}
\onslide<2>{
    \begin{itemize}
	\item Also fine-tuned a model using \acrshort{mlm} for comparison with two textbooks
        \item A document-store with a fine-tuned \acrshort{gpt}-3.5 Turbo provides a benchmark for performance
    \end{itemize}
}

\end{frame}


%7/8 - Results----------------------------------------------
\section{Results}
\begin{frame}[t]{Demonstration}
\onslide<1->{
    \begin{itemize}
	\item Demonstration using \hyperlink{https://colab.research.google.com}{Google Colab} \citep{Bisong2019}
    \end{itemize}
}

\end{frame}


%11 - Conclusion----------------------------------------------
\section{Concluding Remarks}
\begin{frame}{Contributions}
    \begin{itemize}
        \item Promising results
        \item Document store framework with Q\&A chatbot allows sources to be provided
        \item The fine-tuned model can have many applications - simply provide different \acrshort{pdf}s/documents
        \item No ongoing dependencies (such as \acrshort{gpt}-3.5 Turbo)
        \item Provision of a dataset for training long-form, closed-domain question answering
    \end{itemize}
\end{frame}

%11 - Conclusion----------------------------------------------
\begin{frame}{Future Research}
    \begin{itemize}
        \item Adapt for tables, lists, formulas, images
        \item Compare fine-tuned T5 to \acrshort{bart} and other models (the provided code supports this) 
        \item Combined \acrshort{mlm} and question-answering fine-tuning
        \item Research the efficacy, reliability and impact of chatbots in the classroom
        \item All code can be found in the following GitHub repository: \url{https://github.com/pointonjoel/MSc-Diss}.
    \end{itemize}
\end{frame}


%11 - References----------------------------------------------
\begin{frame}[allowframebreaks]{Bibliography}
% \footnotesize{
% Berthoud, R. (2000): `Ethnic employment penalties in Britain,' \textit{Ethnic and Migration
% Studies, 26:3, 389-416, DOI: 10.1080/713680490}.
% \bigskip
% \\Blackaby, D., D. Leslie, P. Murphy, and N. O'Leary (2005): `Born in Britain: How are native ethnic minorities faring in the British labour market?' \textit{Economics Letters, 88, 370-375}.
% }
\footnotesize{
    \bibliography{paper/bibliography/bibliography}
}
\end{frame}

%11 - Glossary----------------------------------------------
\begin{frame}{Glossary}
    \printnoidxglossaries
\end{frame}
\end{document}