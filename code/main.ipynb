{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GPT-3.5-Turbo Model\n",
    "Creating a question answering chatbot using GPT-3.5. Adapted from: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from modules.config import *\n",
    "from modules.knowledge import *\n",
    "from modules.chatbot import *\n",
    "from modules.embedding_functions import *\n",
    "from modules.query import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "filename = 'assets/knowledge/Digital_Image_Processing_Textbook.pdf'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Open the PDF file\n",
    "with open(filename, 'rb') as file:\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Extract text from each page\n",
    "    text = \"\"\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "\n",
    "print(text)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL\n",
      " \n",
      "EDITION\n",
      "This is a special edition of an established \n",
      "title widely used by colleges and universities \n",
      "throughout the world. Pearson published this \n",
      "exclusive edition for the benefit of students \n",
      "outside the United States and Canada. If \n",
      "you purchased this book within the United \n",
      "States or Canada, you should be aware that \n",
      "it has been imported without the approval of \n",
      "the Publisher or Author. The Global Edition \n",
      "is not supported in the United States and \n",
      "Canada.\n",
      "Pearson Global Edition\n",
      "GLOBAL\n",
      " \n",
      "EDITION\n",
      "For these Global Editions, the editorial team at Pearson has \n",
      "collaborated with educators across the world to address a \n",
      "wide range of subjects and requirements, equipping students \n",
      "with the best possible learning tools. This Global Edition \n",
      "preserves the cutting-edge approach and pedagogy of the \n",
      "original, but also features alterations, customization, and \n",
      "adaptation from the North American version.\n",
      "D\n",
      "igital Image Processing\n",
      " FOURTH EDITION\n",
      " Rafael C. Gonzalez • Richard E. Woods\n",
      "Digital Image Processing\n",
      "Gonzalez \n",
      "Woods\n",
      "FOURTH\n",
      " \n",
      "EDITION\n",
      "GLOBAL\n",
      " \n",
      "EDITION\n",
      "Gonzalez_04_1292223049_Final.indd   1\n",
      "11/08/17   5:27 PM\n",
      "www.EBooksWorld.irSupport Package for \n",
      "Digital\n",
      " \n",
      "Image Processing\n",
      "Your new textbook provides access to support packages that may include reviews in areas \n",
      "like probability and vectors, tutorials on topics relevant to the material in the book, an image \n",
      "database, and more. Refer to the Preface in the textbook for a detailed list of resources.\n",
      "Follow the instructions below to register for the Companion Website for Rafael C. Gonzalez and \n",
      "Richard E. Woods’ \n",
      "Digital Image Processing\n",
      ", Fourth Edition, Global Edition.\n",
      "1.\n",
      " \n",
      "Go to \n",
      "www.ImageProcessingPlace.com\n",
      "2.\n",
      " \n",
      "Find the title of your te\n",
      "xtbook.\n",
      "3.\n",
      "  \n",
      "Cl\n",
      "ick Support Materials and follow the on-screen instructions to create a login name and \n",
      "password.\n",
      "Use the login name and password you created during registration to start using the \n",
      "digital resources that accompany your textbook.\n",
      "IMPORTANT:\n",
      "This serial code can only be used once. This subscription is not transferrable.\n",
      "Gonzalez_04_1292223049_ifc_Final.indd   1\n",
      "11/08/17   5:33 PM\n",
      "www.EBooksWorld.irProcessing\n",
      "igital Image\n",
      "4\n",
      "D\n",
      "FOURTH\n",
      "EDITION\n",
      "Rafael C. Gonzalez\n",
      "University of Tennessee\n",
      "Richard E. Woods\n",
      "Interapptics\n",
      "330 Hudson Street, New York, NY 10013\n",
      "Global Edition\n",
      "DIP4E_GLOBAL_Print_Ready.indb   1\n",
      "7/6/2017   10:55:08 AM\n",
      "www.EBooksWorld.irSenior Vice President Courseware Portfolio Management:\n",
      " Marcia J. Horton\n",
      "Director, Portfolio Management: Engineering, Computer Science & Global Editions:\n",
      " Julian Partridge\n",
      "Portfolio Manager:\n",
      " Julie Bai \n",
      "Field Marketing Manager:\n",
      " Demetrius Hall \n",
      "Product Marketing Manager:\n",
      " Yvonne Vannatta \n",
      "Marketing Assistant:\n",
      " Jon Bryant \n",
      "Content Managing Producer, ECS and Math:\n",
      " Scott Disanno \n",
      "Content Producer:\n",
      " Michelle Bayman \n",
      "Project Manager:\n",
      " Rose Kernan\n",
      "Assistant Project Editor, Global Editions:\n",
      " Vikash Tiwari\n",
      "Operations Specialist:\n",
      " Maura Zaldivar-Garcia \n",
      "Manager, Rights and Permissions:\n",
      " Ben Ferrini \n",
      "Senior Manufacturing Controller, Global Editions:\n",
      " Trudy Kimber \n",
      "Media Production Manager, Global Editions:\n",
      " Vikram Kumar\n",
      "Cover Designer:\n",
      " Lumina Datamatics \n",
      "Cover Photo:\n",
      " \n",
      "CT image\n",
      "—© zhuravliki.123rf.com/Pearson Asset Library; \n",
      "Gram-negative bacteria\n",
      "—© royaltystockphoto.com/\n",
      "Shutterstock.com; \n",
      "Orion Nebula\n",
      "—© creativemarc/Shutterstock.com; \n",
      "Fingerprints\n",
      "—© Larysa Ray/Shutterstock.com; \n",
      "Cancer \n",
      "cells\n",
      "—© Greenshoots Communications/Alamy Stock Photo\n",
      "MATLAB is a registered trademark of The MathWorks, Inc., 1 Apple Hill Drive, Natick, MA 01760-2098.\n",
      "Pearson Education Limited\n",
      "Edinburgh Gate\n",
      "Harlow\n",
      "Essex CM20 2JE\n",
      "England\n",
      "and Associated Companies throughout the world\n",
      "Visit us on the World Wide Web at: \n",
      "www.pearsonglobaleditions.com\n",
      "© Pearson Education Limited 2018\n",
      "The rights of Rafael C. Gonzalez and Richard E. Woods to be identified as the authors of this work have been asserted by them \n",
      "in accordance with the Copyright, Designs and Patents Act 1988.\n",
      "Authorized adaptation from the United States edition, entitled Digital Image Processing, Fourth Edition, ISBN 978-0-13-335672-4\n",
      ", \n",
      "by Rafael C. Gonzalez and Richard E. Woods, published by Pearson Education © 2018.\n",
      "All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by \n",
      "any means, electronic, mechanical, photocopying, recording or otherwise, without either the prior written permission of the pub\n",
      "-\n",
      "lisher or a license permitting restricted copying in the United Kingdom issued by the Copyright Licensing Agency Ltd, Saffron \n",
      "House, 6–10 Kirby Street, London EC1N 8TS.\n",
      "All trademarks used herein are the property of their respective owners. The use of any trademark in this text does not vest in the \n",
      "author or publisher any trademark ownership rights in such trademarks, nor does the use of such trademarks imply any affiliatio\n",
      "n \n",
      "with or endorsement of this book by such owners.\n",
      "British Library Cataloguing-in-Publication Data\n",
      "A catalogue record for this book is available from the British Library\n",
      "10 9 8 7 6 5 4 3 2 1\n",
      "ISBN 10: 1-292-22304-9\n",
      "ISBN 13: 978-1-292-22304-9\n",
      "Typeset by Richard E. Woods\n",
      "Printed and bound in Malaysia\n",
      "DIP4E_GLOBAL_Print_Ready.indb   2\n",
      "7/6/2017   10:55:08 AM\n",
      "www.EBooksWorld.irTo Connie, Ralph, and Rob \n",
      "and\n",
      "To Janice, David, and Jonathan\n",
      "DIP4E_GLOBAL_Print_Ready.indb   3\n",
      "6/16/2017   2:01:57 PM\n",
      "www.EBooksWorld.irDIP4E_GLOBAL_Print_Ready.indb   4\n",
      "6/16/2017   2:01:57 PMThis page intentionally left blank\n",
      "www.EBooksWorld.irContents\n",
      "Preface  9\n",
      "Acknowledgments  12\n",
      "The Book Website  13\n",
      "The DIP4E Support Packages  13\n",
      "About the Authors  14\n",
      "1\n",
      " Introduction  17\n",
      "What is Digital Image Processing?  18\n",
      "The Origins of Digital Image Processing  19\n",
      "Examples of Fields that Use Digital Image Processing  23\n",
      "Fundamental Steps in Digital Image Processing  41\n",
      "Components of an Image Processing System  44\n",
      "2\n",
      " Digital Image Fundamentals  47\n",
      "Elements of Visual Perception  48\n",
      "Light and the Electromagnetic Spectrum  54\n",
      "Image Sensing and Acquisition  57\n",
      "Image Sampling and Quantization  63\n",
      "Some Basic Relationships Between Pixels  79\n",
      "Introduction to the Basic Mathematical Tools Used in Digital Image \n",
      "Processing  \n",
      "83\n",
      "3\n",
      " Intensity Transformations and Spatial  \n",
      "Filtering  119\n",
      "Background  120\n",
      "Some Basic Intensity Transformation Functions  122\n",
      "Histogram Processing  133\n",
      "Fundamentals of Spatial Filtering  153\n",
      "Smoothing (Lowpass) Spatial Filters  164\n",
      "Sharpening (Highpass) Spatial Filters  175\n",
      "Highpass, Bandreject, and Bandpass Filters from Lowpass Filters  188\n",
      "Combining Spatial Enhancement Methods  191\n",
      "DIP4E_GLOBAL_Print_Ready.indb   5\n",
      "6/16/2017   2:01:57 PM\n",
      "www.EBooksWorld.ir6\n",
      "    \n",
      "Contents\n",
      "4\n",
      " Filtering in the Frequency \n",
      "Domain  203\n",
      "Background  204\n",
      "Preliminary Concepts  207\n",
      "Sampling and the Fourier Transform of Sampled  \n",
      "Functions  215\n",
      "The Discrete Fourier Transform of One Variable  225\n",
      "Extensions to Functions of Two Variables  230\n",
      "Some Properties of the 2-D DFT and IDFT  240\n",
      "The Basics of Filtering in the Frequency Domain  260\n",
      "Image Smoothing Using Lowpass Frequency Domain  \n",
      "Filters  272\n",
      "Image Sharpening Using Highpass Filters  284\n",
      "Selective Filtering  296\n",
      "The Fast Fourier Transform  303\n",
      "5\n",
      " Image Restoration \n",
      "and Reconstruction  317\n",
      "A Model of the Image Degradation/Restoration  \n",
      "process  318\n",
      "Noise Models  318\n",
      "Restoration in the Presence of Noise Only—Spatial Filtering  327\n",
      "Periodic Noise Reduction Using Frequency Domain Filtering  340\n",
      "Linear, Position-Invariant Degradations  348\n",
      "Estimating the Degradation Function  352\n",
      "Inverse Filtering  356\n",
      "Minimum Mean Square Error (Wiener) Filtering  358\n",
      "Constrained Least Squares Filtering  363\n",
      "Geometric Mean Filter  367\n",
      "Image Reconstruction from Projections  368\n",
      "6\n",
      " Color Image Processing  399\n",
      "Color Fundamentals  400\n",
      "Color Models  405\n",
      "Pseudocolor Image Processing  420\n",
      "Basics of Full-Color Image Processing  429\n",
      "Color Transformations  430\n",
      "DIP4E_GLOBAL_Print_Ready.indb   6\n",
      "6/16/2017   2:01:57 PM\n",
      "www.EBooksWorld.irContents\n",
      "    \n",
      "7\n",
      "Color Image Smoothing and Sharpening  442\n",
      "Using Color in Image Segmentation  445\n",
      "Noise in Color Images  452\n",
      "Color Image Compression  455\n",
      "7\n",
      " Wavelet and Other Image Transforms  463\n",
      "Preliminaries  464\n",
      "Matrix-based Transforms  466\n",
      "Correlation  478\n",
      "Basis Functions in the Time-Frequency Plane  479\n",
      "Basis Images  483\n",
      "Fourier-Related Transforms  484\n",
      "Walsh-Hadamard Transforms  496\n",
      "Slant Transform  500\n",
      "Haar Transform  502\n",
      "Wavelet Transforms  \n",
      "504\n",
      "8\n",
      " Image Compression and  \n",
      "Watermarking  539\n",
      "Fundamentals  540\n",
      "Huffman Coding  553\n",
      "Golomb Coding  556\n",
      "Arithmetic Coding  561\n",
      "LZW Coding  564\n",
      "Run-length Coding  566\n",
      "Symbol-based Coding  572\n",
      "Bit-plane Coding  575\n",
      "Block Transform Coding  576\n",
      "Predictive Coding  \n",
      "594\n",
      "Wavelet Coding  \n",
      "614\n",
      "Digital Image Watermarking  \n",
      "624\n",
      "9\n",
      " Morphological Image Processing  635\n",
      "Preliminaries  636\n",
      "Erosion and Dilation  638\n",
      "Opening and Closing  644\n",
      "The Hit-or-Miss Transform  648\n",
      "DIP4E_GLOBAL_Print_Ready.indb   7\n",
      "6/16/2017   2:01:57 PM\n",
      "www.EBooksWorld.ir8\n",
      "    \n",
      "Contents\n",
      "Some Basic Morphological Algorithms  652\n",
      "Morphological Reconstruction  667\n",
      "Summary of Morphological Operations on Binary Images  673\n",
      "Grayscale Morphology  674\n",
      "10\n",
      " Image Segmentation  699\n",
      "Fundamentals  \n",
      "700\n",
      "Point, Line, and Edge Detection  \n",
      "701\n",
      "Thresholding  \n",
      "742\n",
      "Segmentation by Region Growing and by Region Splitting and \n",
      "Merging  \n",
      "764\n",
      "Region Segmentation Using Clustering and  \n",
      "Superpixels  \n",
      "770\n",
      "Region Segmentation Using Graph Cuts\n",
      "  \n",
      "777\n",
      "Segmentation Using Morphological Watersheds\n",
      "  \n",
      "786\n",
      "The Use of Motion in Segmentation  \n",
      "796\n",
      "11\n",
      " Feature Extraction   811\n",
      "Background  \n",
      "812\n",
      "Boundary Preprocessing  \n",
      "814\n",
      "Boundary Feature Descriptors  \n",
      "831\n",
      "Region Feature Descriptors  \n",
      "840\n",
      "Principal Components as Feature Descriptors  \n",
      "859\n",
      "Whole-Image Features  \n",
      "868\n",
      "Scale-Invariant Feature Transform (SIFT)  \n",
      "881\n",
      "12\n",
      " Image Pattern Classification  903\n",
      "Background  \n",
      "904\n",
      "Patterns and Pattern Classes  \n",
      "906\n",
      "Pattern Classification by Prototype Matching  \n",
      "910\n",
      "Optimum (Bayes) Statistical Classifiers  \n",
      "923\n",
      "Neural Networks and Deep Learning  \n",
      "931\n",
      "Deep Convolutional Neural Networks  \n",
      "964\n",
      "Some Additional Details of Implementation  \n",
      "987\n",
      "Bibliography  995\n",
      "Index  1009\n",
      "DIP4E_GLOBAL_Print_Ready.indb   8\n",
      "7/12/2017   10:23:39 AM\n",
      "www.EBooksWorld.irPreface\n",
      "When something can be read without effort, great effort has gone into its writing.\n",
      "Enrique Jardiel Poncela\n",
      "This edition of \n",
      "Digital Image Processing\n",
      " is a major revision of the book.\n",
      " As in \n",
      "the 1977 and 1987 editions by Gonzalez and Wintz, and the 1992, 2002, and 2008  \n",
      "editions by Gonzalez and Woods, this sixth-generation edition was prepared \n",
      "with students and instructors in mind. The principal objectives of the book \n",
      "continue to be to provide an introduction to basic concepts and methodologies \n",
      "applicable to digital image processing, and to develop a foundation that can \n",
      "be used as the basis for further study and research in this field. To achieve \n",
      "these objectives, we focused again on material that we believe is fundamental \n",
      "and whose scope of application is not limited to the solution of specialized \n",
      "problems. The mathematical complexity of the book remains at a level well \n",
      "within the grasp of college seniors and first-year graduate students who have \n",
      "introductory preparation in mathematical analysis, vectors, matrices, probability, \n",
      "statistics, linear systems, and computer programming. The book website pro-\n",
      "vides tutorials to support readers needing a review of this background material.  \n",
      "One of the principal reasons this book has been the world leader in its ﬁeld for \n",
      "40 years is the level of attention we pay to the changing educational needs of our \n",
      "readers. The present edition is based on an extensive survey that involved faculty, \n",
      "students, and independent readers of the book in 150 institutions from 30 countries. \n",
      "The survey revealed a need for coverage of new material that has matured since the \n",
      "last edition of the book. The principal ﬁndings of the survey indicated a need for: \n",
      "• \n",
      "Expanded coverage of the fundamentals of spatial filtering.\n",
      "• \n",
      "A more comprehensive and cohesive coverage of image transforms.\n",
      "• \n",
      "A more complete presentation of finite differences, with a focus on edge detec-\n",
      "tion.\n",
      "• \n",
      "A discussion of clustering, superpixels, and their use in region segmentation. \n",
      "• \n",
      "Coverage of maximally stable extremal regions.\n",
      "• \n",
      "Expanded coverage of feature extraction to include the Scale Invariant Feature \n",
      "T\n",
      "ransform (SIFT).\n",
      "• \n",
      "Expanded coverage of neural networks to include deep neural networks, back-\n",
      "propagation,\n",
      " deep learning, and, especially, deep convolutional neural networks. \n",
      "• \n",
      "More homework exercises at the end of the chapters.\n",
      "T\n",
      "he new and reorganized material that resulted in the present edition is our \n",
      "attempt at providing a reasonable balance between rigor, clarity of presentation, \n",
      "and the ﬁndings of the survey. In addition to new material, earlier portions of the \n",
      "text were updated and clariﬁed. This edition contains 241 new images, 72 new draw-\n",
      "ings, and 135 new exercises.\n",
      "DIP4E_GLOBAL_Print_Ready.indb   9\n",
      "6/16/2017   2:01:57 PM\n",
      "www.EBooksWorld.ir10\n",
      "    \n",
      "Preface\n",
      "New to This Edition\n",
      "The highlights of this edition are as follows.\n",
      "Chapter 1:\n",
      " Some figures were updated, and parts of the text were rewritten to cor-\n",
      "respond to changes in later chapters\n",
      ".\n",
      "Chapter 2:\n",
      " Many of the sections and examples were rewritten for clarity. We \n",
      "added 14 new exercises\n",
      ". \n",
      "Chapter 3:\n",
      " Fundamental concepts of spatial ﬁltering were rewritten to include a \n",
      "discussion on separable ﬁlter kernels\n",
      ", expanded coverage of the properties of low-\n",
      "pass Gaussian kernels, and expanded coverage of highpass, bandreject, and band-\n",
      "pass ﬁlters, including numerous new examples that illustrate their use. In addition to \n",
      "revisions in the text, including 6 new examples, the chapter has 59 new images, 2 new \n",
      "line drawings, and 15 new exercises.\n",
      "Chapter 4:\n",
      " Several of the sections of this chapter were revised to improve the clar-\n",
      "ity of presentation.\n",
      " We replaced dated graphical material with 35 new images and 4 \n",
      "new line drawings. We added 21 new exercises. \n",
      "Chapter 5:\n",
      " Revisions to this chapter were limited to clariﬁcations and a few cor-\n",
      "rections in notation.\n",
      " We added 6 new images and 14 new exercises, \n",
      "Chapter 6:\n",
      " Several sections were clarified, and the explanation of the CMY and \n",
      "CMYK color models was expanded,\n",
      " including 2 new images.\n",
      "Chapter 7:\n",
      " This is a new chapter that brings together wavelets, several new trans-\n",
      "forms\n",
      ", and many of the image transforms that were scattered throughout the book. \n",
      "The emphasis of this new chapter is on the presentation of these transforms from a \n",
      "unified point of view.  We added 24 new images, 20 new drawings, and 25 new exer-\n",
      "cises. \n",
      "Chapter 8:\n",
      " The material was revised with numerous clarifications and several \n",
      "improvements to the presentation.\n",
      "Chapter 9:\n",
      " Revisions of this chapter included a complete rewrite of several sec-\n",
      "tions\n",
      ", including redrafting of several line drawings. We added 16 new exercises\n",
      "Chapter 10:\n",
      " Several of the sections were rewritten for clarity. We updated the \n",
      "chapter by adding coverage of finite differences\n",
      ", \n",
      "K\n",
      "-means clustering, superpixels, \n",
      "and graph cuts. The new topics are illustrated with 4 new examples. In total, we \n",
      "added 29 new images, 3 new drawings, and 6 new exercises.\n",
      "Chapter 11:\n",
      " The chapter was updated with numerous topics, beginning with a more \n",
      "detailed classification of feature types and their uses\n",
      ". In addition to improvements in \n",
      "the clarity of presentation, we added coverage of slope change codes, expanded the \n",
      "explanation of skeletons, medial axes, and the distance transform, and added sev-\n",
      "eral new basic descriptors of compactness, circularity, and eccentricity. New mate-\n",
      "rial includes coverage of the Harris-Stephens corner detector, and a presentation of \n",
      "maximally stable extremal regions. A major addition to the chapter is a comprehen-\n",
      "sive discussion dealing with the Scale-Invariant Feature Transform (SIFT). The new \n",
      "material is complemented by 65 new images, 15 new drawings, and 12 new exercises.\n",
      "DIP4E_GLOBAL_Print_Ready.indb   10\n",
      "6/16/2017   2:01:57 PM\n",
      "www.EBooksWorld.irPreface\n",
      "    \n",
      "11\n",
      "Chapter 12:\n",
      " This chapter underwent a major revision to include an extensive \n",
      "rewrite of neural networks and deep learning\n",
      ", an area that has grown significantly \n",
      "since the last edition of the book. We added a comprehensive discussion on fully \n",
      "connected, deep neural networks that includes derivation of backpropagation start-\n",
      "ing from basic principles. The equations of backpropagation were expressed in “tra-\n",
      "ditional” scalar terms, and then generalized into a compact set of matrix equations \n",
      "ideally suited for implementation of deep neural nets. The effectiveness of fully con-\n",
      "nected networks was demonstrated with several examples that included a compari-\n",
      "son with the Bayes classifier. One of the most-requested topics in the survey was \n",
      "coverage of deep convolutional neural networks. We added an extensive section on \n",
      "this, following the same blueprint we used for deep, fully connected nets. That is, we \n",
      "derived the equations of backpropagation for convolutional nets, and showed how \n",
      "they are different from “traditional” backpropagation. We then illustrated the use of \n",
      "convolutional networks with simple images, and applied them to large image data-\n",
      "bases of numerals and natural scenes.  The written material is complemented by 23 \n",
      "new images, 28 new drawings, and 12 new exercises.\n",
      "Also for the first time, we have created student and faculty support packages that \n",
      "can be downloaded from the book website. The \n",
      "Student Support Package\n",
      " contains \n",
      "many of the original images in the book and answers to selected exercises The \n",
      "Fac-\n",
      "ulty Support Package\n",
      " contains solutions to all exercises, teaching suggestions, and all \n",
      "the art in the book in the form of modifiable PowerPoint slides. One support pack-\n",
      "age is made available with every new book, free of charge. \n",
      "The book website, established during the launch of the 2002 edition, continues to \n",
      "be a success, attracting more than 25,000 visitors each month. The site was upgraded \n",
      "for the launch of this edition. For more details on site features and content, see \n",
      "The \n",
      "Book Website\n",
      ", following the \n",
      "Acknowledgments\n",
      " section.\n",
      "This edition of \n",
      "Digital Image Processing\n",
      " is a reﬂection of how the educational \n",
      "needs of our readers have changed since 2008. As is usual in an endeavor such as \n",
      "this, progress in the ﬁeld continues after work on the manuscript stops. One of the \n",
      "reasons why this book has been so well accepted since it ﬁrst appeared in 1977 is its \n",
      "continued emphasis on fundamental concepts that retain their relevance over time. \n",
      "This approach, among other things, attempts to provide a measure of stability in a \n",
      "rapidly evolving body of knowledge. We have tried to follow the same principle in \n",
      "preparing this edition of the book.\n",
      "R.C.G.\n",
      "R.E.W.\n",
      "DIP4E_GLOBAL_Print_Ready.indb   11\n",
      "6/16/2017   2:01:57 PM\n",
      "www.EBooksWorld.ir12\n",
      "    \n",
      "Acknowledgments\n",
      "Acknowledgments\n",
      "We are indebted to a number of individuals in academic circles, industry, and gov-\n",
      "ernment who have contributed to this edition of the book. In particular, we wish \n",
      "to extend our appreciation to Hairong Qi and her students, Zhifei Zhang and \n",
      "Chengcheng Li, for their valuable review of the material on neural networks, and for \n",
      "their help in generating examples for that material. We also want to thank Ernesto \n",
      "Bribiesca Correa for providing and reviewing material on slope chain codes, and \n",
      "Dirk Padfield for his many suggestions and review of several chapters in the book. \n",
      "We appreciate Michel Kocher’s many thoughtful comments and suggestions over \n",
      "the years on how to improve the book. Thanks also to Steve Eddins for his sugges-\n",
      "tions on MATLAB and related software issues.\n",
      "Numerous individuals have contributed to material carried over from the previ-\n",
      "ous to the current edition of the book. Their contributions have been important in so \n",
      "many different ways that we ﬁnd it difﬁcult to acknowledge them in any other way \n",
      "but alphabetically. We thank Mongi A. Abidi, Yongmin Kim, Bryan Morse, Andrew \n",
      "Oldroyd, Ali M. Reza, Edgardo Felipe Riveron, Jose Ruiz Shulcloper, and Cameron \n",
      "H.G. Wright for their many suggestions on how to improve the presentation and/or \n",
      "the scope of coverage in the book. We are also indebted to Naomi Fernandes at the \n",
      "MathWorks for providing us with MATLAB software and support that were impor-\n",
      "tant in our ability to create many of the examples and experimental results included \n",
      "in this edition of the book.\n",
      "A signiﬁcant percentage of the new images used in this edition (and in some \n",
      "cases their history and interpretation) were obtained through the efforts of indi-\n",
      "viduals whose contributions are sincerely appreciated. In particular, we wish to \n",
      "acknowledge the efforts of Serge Beucher, Uwe Boos, Michael E. Casey, Michael \n",
      "W. Davidson, Susan L. Forsburg, Thomas R. Gest, Daniel A. Hammer, Zhong He, \n",
      "Roger Heady, Juan A. Herrera, John M. Hudak, Michael Hurwitz, Chris J. Johannsen, \n",
      "Rhonda Knighton, Don P . Mitchell, A. Morris, Curtis C. Ober, David. R. Pickens, \n",
      "Michael Robinson, Michael Shaffer, Pete Sites, Sally Stowe, Craig Watson, David \n",
      "K. Wehe, and Robert A. West. We also wish to acknowledge other individuals and \n",
      "organizations cited in the captions of numerous ﬁgures throughout the book for \n",
      "their permission to use that material. \n",
      "We also thank Scott Disanno, Michelle Bayman, Rose Kernan, and Julie Bai for \n",
      "their support and signiﬁcant patience during the production of the book.\n",
      "R.C.G.\n",
      "R.E.W.\n",
      "DIP4E_GLOBAL_Print_Ready.indb   12\n",
      "6/16/2017   2:01:57 PM\n",
      "www.EBooksWorld.irThe Book Website\n",
      "www.ImageProcessingPlace.com\n",
      "Digital Image Processing\n",
      " is a completely self-contained book.\n",
      " However, the compan-\n",
      "ion website offers additional support in a number of important areas.\n",
      "For the Student or Independent Reader the site contains\n",
      "• \n",
      "Reviews in areas such as probability, statistics, vectors, and matrices.\n",
      "• \n",
      "A Tutorials section containing dozens of tutorials on topics relevant to the mate-\n",
      "rial in the book.\n",
      "• \n",
      "An image database containing all the images in the book, as well as many other \n",
      "image databases\n",
      ".\n",
      "For the Instructor the site contains\n",
      "• \n",
      "An Instructor’s Manual with complete solutions to all the problems.\n",
      "• \n",
      "Classroom presentation materials in modifiable PowerPoint format.\n",
      "• \n",
      "Material removed from previous editions, downloadable in convenient PDF \n",
      "format.\n",
      "• \n",
      "Numerous links to other educational resources.\n",
      "F\n",
      "or the Practitioner the site contains additional specialized topics such as\n",
      "• \n",
      "Links to commercial sites.\n",
      "• \n",
      "Selected new references.\n",
      "• \n",
      "Links to commercial image databases.\n",
      "T\n",
      "he website is an ideal tool for keeping the book current between editions by includ-\n",
      "ing new topics, digital images, and other relevant material that has appeared after \n",
      "the book was published. Although considerable care was taken in the production \n",
      "of the book, the website is also a convenient repository for any errors discovered \n",
      "between printings. \n",
      "The DIP4E Support Packages\n",
      "In this edition, we created support packages for students and faculty to organize \n",
      "all the classroom support materials available for the new edition of the book into \n",
      "one easy download. The Student Support Package contains many of the original \n",
      "images in the book, and answers to selected exercises, The Faculty Support Package \n",
      "contains solutions to all exercises, teaching suggestions, and all the art in the book \n",
      "in modifiable PowerPoint slides. One support package is made available with every \n",
      "new book, free of charge. Applications for the support packages are submitted at \n",
      "the book website.\n",
      "DIP4E_GLOBAL_Print_Ready.indb   13\n",
      "6/16/2017   2:01:57 PM\n",
      "www.EBooksWorld.irAbout the Authors\n",
      "RAFAEL C. GONZALEZ\n",
      "R. C. Gonzalez received the B.S.E.E. degree from the University of Miami in 1965 \n",
      "and the M.E. and Ph.D. degrees in electrical engineering from the University of \n",
      "Florida, Gainesville, in 1967 and 1970, respectively. He joined the Electrical and \n",
      "Computer Science Department at the University of Tennessee, Knoxville (UTK) in \n",
      "1970, where he became Associate Professor in 1973, Professor in 1978, and Distin-\n",
      "guished Service Professor in 1984. He served as Chairman of the department from \n",
      "1994 through 1997. He is currently a Professor Emeritus at UTK.\n",
      "Gonzalez is the founder of the Image & Pattern Analysis Laboratory and the \n",
      "Robotics & Computer Vision Laboratory at the University of Tennessee. He also \n",
      "founded Perceptics Corporation in 1982 and was its president until 1992. The last \n",
      "three years of this period were spent under a full-time employment contract with \n",
      "Westinghouse Corporation, who acquired the company in 1989. \n",
      "Under his direction, Perceptics became highly successful in image processing, \n",
      "computer vision, and laser disk storage technology. In its initial ten years, Perceptics \n",
      "introduced a series of innovative products, including: The world’s ﬁrst commercially \n",
      "available computer vision system for automatically reading license plates on moving \n",
      "vehicles; a series of large-scale image processing and archiving systems used by the \n",
      "U.S. Navy at six different manufacturing sites throughout the country to inspect the \n",
      "rocket motors of missiles in the Trident II Submarine Program; the market-leading \n",
      "family of imaging boards for advanced Macintosh computers; and a line of trillion-\n",
      "byte laser disk products.\n",
      "He is a frequent consultant to industry and government in the areas of pattern \n",
      "recognition, image processing, and machine learning. His academic honors for work \n",
      "in these ﬁelds include the 1977 UTK College of Engineering Faculty Achievement \n",
      "Award; the 1978 UTK Chancellor’s Research Scholar Award; the 1980 Magnavox \n",
      "Engineering Professor Award; and the 1980 M.E. Brooks Distinguished Professor \n",
      "Award. In 1981 he became an IBM Professor at the University of Tennessee and \n",
      "in 1984 he was named a Distinguished Service Professor there. He was awarded a \n",
      "Distinguished Alumnus Award by the University of Miami in 1985, the Phi Kappa \n",
      "Phi Scholar Award in 1986, and the University of Tennessee’s Nathan W. Dougherty \n",
      "Award for Excellence in Engineering in 1992.\n",
      "Honors for industrial accomplishment include the 1987 IEEE Outstanding Engi-\n",
      "neer Award for Commercial Development in Tennessee; the 1988 Albert Rose \n",
      "National Award for Excellence in Commercial Image Processing; the 1989 B. Otto \n",
      "Wheeley Award for Excellence in Technology Transfer; the 1989 Coopers and \n",
      "Lybrand Entrepreneur of the Year Award; the 1992 IEEE Region 3 Outstanding \n",
      "Engineer Award; and the 1993 Automated Imaging Association National Award for \n",
      "Technology Development.\n",
      "Gonzalez is author or co-author of over 100 technical articles, two edited books, \n",
      "and four textbooks in the ﬁelds of pattern recognition, image processing, and robot-\n",
      "ics. His books are used in over 1000 universities and research institutions throughout \n",
      "DIP4E_GLOBAL_Print_Ready.indb   14\n",
      "6/16/2017   2:01:57 PM\n",
      "www.EBooksWorld.irthe world. He is listed in the prestigious \n",
      "Marquis Who’s Who in America\n",
      ", \n",
      "Marquis \n",
      "Who’s Who in Engineering\n",
      ", \n",
      "Marquis Who’s Who in the World\n",
      ", and in 10 other national \n",
      "and international biographical citations. He is the co-holder of two U.S. Patents, and \n",
      "has been an associate editor of the \n",
      "IEEE Transactions on Systems, Man and Cyber-\n",
      "netics\n",
      ", and the \n",
      "International Journal of Computer and Information Sciences\n",
      ". He is a \n",
      "member of numerous professional and honorary societies, including Tau Beta Pi, Phi \n",
      "Kappa Phi, Eta Kappa Nu, and Sigma Xi. He is a Fellow of the IEEE.\n",
      "RICHARD E. WOODS\n",
      "R. E. Woods earned his B.S., M.S., and Ph.D. degrees in Electrical Engineering from \n",
      "the University of Tennessee, Knoxville in 1975, 1977, and 1980, respectively. He \n",
      "became an Assistant Professor of Electrical Engineering and Computer Science in \n",
      "1981 and was recognized as a Distinguished Engineering Alumnus in 1986.\n",
      "A veteran hardware and software developer, Dr. Woods has been involved in \n",
      "the founding of several high-technology startups, including Perceptics Corporation, \n",
      "where he was responsible for the development of the company’s quantitative image \n",
      "analysis and autonomous decision-making products; MedData Interactive, a high-\n",
      "technology company specializing in the development of handheld computer systems \n",
      "for medical applications; and Interapptics, an internet-based company that designs \n",
      "desktop and handheld computer applications.\n",
      "Dr. Woods currently serves on several nonproﬁt educational and media-related \n",
      "boards, including Johnson University, and was recently a summer English instructor \n",
      "at the Beijing Institute of Technology. He is the holder of a U.S. Patent in the area \n",
      "of digital image processing and has published two textbooks, as well as numerous \n",
      "articles related to digital signal processing. Dr. Woods is a member of several profes-\n",
      "sional societies, including Tau Beta Pi, Phi Kappa Phi, and the IEEE.\n",
      "DIP4E_GLOBAL_Print_Ready.indb   15\n",
      "6/16/2017   2:01:57 PM\n",
      "www.EBooksWorld.irDIP4E_GLOBAL_Print_Ready.indb   4\n",
      "6/16/2017   2:01:57 PMThis page intentionally left blank\n",
      "www.EBooksWorld.ir171\n",
      "Introduction\n",
      "One picture is worth more than ten thousand words.\n",
      "Anonymous\n",
      "Preview\n",
      "Interest in digital image processing methods stems from two principal application areas: improvement \n",
      "of pictorial information for human interpretation, and processing of image data for tasks such as storage, \n",
      "transmission, and extraction of pictorial information. This chapter has several objectives: (1) to deﬁne \n",
      "the scope of the ﬁeld that we call image processing; (2) to give a historical perspective of the origins of \n",
      "this ﬁeld; (3) to present an overview of the state of the art in image processing by examining some of \n",
      "the principal areas in which it is applied; (4) to discuss brieﬂy the principal approaches used in digital \n",
      "image processing; (5) to give an overview of the components contained in a typical, general-purpose \n",
      "image processing system; and (6) to provide direction to the literature where image processing work is \n",
      "reported. The material in this chapter is extensively illustrated with a range of images that are represen-\n",
      "tative of the images we will be using throughout the book.\n",
      "Upon completion of this chapter, readers should:\n",
      " Understand the concept of a digital image.\n",
      " Have a broad overview of the historical under-\n",
      "pinnings of the ﬁeld of digital image process-\n",
      "ing.\n",
      " Understand the deﬁnition and scope of digi-\n",
      "tal image processing.\n",
      " Know the fundamentals of the electromag-\n",
      "netic spectrum and its relationship to image \n",
      "generation.\n",
      " Be aware of the different ﬁelds in which digi-\n",
      "tal image processing methods are applied.\n",
      " Be familiar with the basic processes involved \n",
      "in image processing.\n",
      " Be familiar with the components that make \n",
      "up a general-purpose digital image process-\n",
      "ing system.\n",
      " Be familiar with the scope of the literature \n",
      "where image processing work is reported.\n",
      "DIP4E_GLOBAL_Print_Ready.indb   17\n",
      "6/16/2017   2:01:58 PM\n",
      "www.EBooksWorld.ir18\n",
      "    \n",
      "Chapter\n",
      " \n",
      "1\n",
      "  \n",
      "Introduction\n",
      "1.1 WHAT IS DIGITAL IMAGE PROCESSING?  \n",
      "An image may be defined as a two-dimensional function, \n",
      "fx y\n",
      "(,\n",
      ") ,\n",
      " where \n",
      "x\n",
      " and \n",
      "y\n",
      " are \n",
      "spatial\n",
      " (plane) coordinates\n",
      ", and the amplitude of \n",
      "f\n",
      " at any pair of coordinates \n",
      "(,)\n",
      "xy\n",
      " \n",
      "is called the \n",
      "intensity\n",
      " or \n",
      "gra\n",
      "y level\n",
      " of the image at that point. When \n",
      "x\n",
      ", \n",
      "y\n",
      ", and the \n",
      "intensity values of \n",
      "f\n",
      " are all finite, discrete quantities, we call the image a \n",
      "digital image\n",
      ". \n",
      "The field of \n",
      "digital image processing\n",
      " refers to processing digital images by means of \n",
      "a digital computer. Note that a digital image is composed of a finite number of ele-\n",
      "ments, each of which has a particular location and value. These elements are called \n",
      "picture elements\n",
      ", \n",
      "image elements\n",
      ", \n",
      "pels\n",
      ", and \n",
      "pixels\n",
      ". \n",
      "Pixel\n",
      " is the term used most widely \n",
      "to denote the elements of a digital image. We will consider these definitions in more \n",
      "formal terms in Chapter 2.\n",
      "Vision is the most advanced of our senses, so it is not surprising that images \n",
      "play the single most important role in human perception. However, unlike humans, \n",
      "who are limited to the visual band of the electromagnetic (EM) spectrum, imaging \n",
      "machines cover almost the entire EM spectrum, ranging from gamma to radio waves. \n",
      "They can operate on images generated by sources that humans are not accustomed \n",
      "to associating with images. These include ultrasound, electron microscopy, and com-\n",
      "puter-generated images. Thus, digital image processing encompasses a wide and var-\n",
      "ied ﬁeld of applications.\n",
      "There is no general agreement among authors regarding where image process-\n",
      "ing stops and other related areas, such as \n",
      "image analysis\n",
      " and \n",
      "computer vision\n",
      ", start. \n",
      "Sometimes, a distinction is made by deﬁning image processing as a discipline in \n",
      "which both the input and output of a process are images. We believe this to be a \n",
      "limiting and somewhat artiﬁcial boundary. For example, under this deﬁnition, even \n",
      "the trivial task of computing the average intensity of an image (which yields a sin-\n",
      "gle number) would not be considered an image processing operation. On the other \n",
      "hand, there are ﬁelds such as computer vision whose ultimate goal is to use comput-\n",
      "ers to emulate human vision, including learning and being able to make inferences \n",
      "and take actions based on visual inputs. This area itself is a branch of \n",
      "artiﬁcial intel-\n",
      "ligence\n",
      " (AI) whose objective is to emulate human intelligence. The ﬁeld of AI is in its \n",
      "earliest stages of infancy in terms of development, with progress having been much \n",
      "slower than originally anticipated. The area of image analysis (also called \n",
      "image \n",
      "understanding\n",
      ") is in between image processing and computer vision.\n",
      "There are no clear-cut boundaries in the continuum from image processing at \n",
      "one end to computer vision at the other. However, one useful paradigm is to con-\n",
      "sider three types of computerized processes in this continuum: low-, mid-, and high-\n",
      "level processes. Low-level processes involve primitive operations such as image \n",
      "preprocessing to reduce noise, contrast enhancement, and image sharpening. A low-\n",
      "level process is characterized by the fact that both its inputs and outputs are images. \n",
      "Mid-level processing of images involves tasks such as segmentation (partitioning \n",
      "an image into regions or objects), description of those objects to reduce them to a \n",
      "form suitable for computer processing, and classiﬁcation (recognition) of individual \n",
      "objects. A mid-level process is characterized by the fact that its inputs generally \n",
      "are images, but its outputs are attributes extracted from those images (e.g., edges, \n",
      "contours, and the identity of individual objects). Finally, higher-level processing \n",
      "1.1\n",
      "DIP4E_GLOBAL_Print_Ready.indb   18\n",
      "6/16/2017   2:01:58 PM\n",
      "www.EBooksWorld.ir1.2\n",
      "  \n",
      "The Origins of Digital Image Processing\n",
      "    \n",
      "19\n",
      "involves “making sense” of an \n",
      "ensemble of recognized objects, as in image analysis, \n",
      "and, at the far end of the continuum, performing the cognitive functions normally \n",
      "associated with human vision.\n",
      "Based on the preceding comments, we see that a logical place of overlap between \n",
      "image processing and image analysis is the area of recognition of individual regions \n",
      "or objects in an image. Thus, what we call in this book \n",
      "digital image processing\n",
      " encom-\n",
      "passes processes whose inputs and outputs are images and, in addition, includes pro-\n",
      "cesses that extract attributes from images up to, and including, the recognition of \n",
      "individual objects. As an illustration to clarify these concepts, consider the area of \n",
      "automated analysis of text. The processes of acquiring an image of the area con-\n",
      "taining the text, preprocessing that image, extracting (segmenting) the individual \n",
      "characters, describing the characters in a form suitable for computer processing, and \n",
      "recognizing those individual characters are in the scope of what we call digital image \n",
      "processing in this book. Making sense of the content of the page may be viewed as \n",
      "being in the domain of image analysis and even computer vision, depending on the \n",
      "level of complexity implied by the statement “making sense of.” As will become \n",
      "evident shortly, digital image processing, as we have deﬁned it, is used routinely in a \n",
      "broad range of areas of exceptional social and economic value. The concepts devel-\n",
      "oped in the following chapters are the foundation for the methods used in those \n",
      "application areas.\n",
      "1.2 THE ORIGINS OF DIGITAL IMAGE PROCESSING  \n",
      "One of the earliest applications of digital images was in the newspaper industry, \n",
      "when pictures were first sent by submarine cable between London and New York. \n",
      "Introduction of the Bartlane cable picture transmission system in the early 1920s \n",
      "reduced the time required to transport a picture across the Atlantic from more than \n",
      "a week to less than three hours. Specialized printing equipment coded pictures for \n",
      "cable transmission, then reconstructed them at the receiving end. Figure 1.1 was \n",
      "transmitted in this way and reproduced on a telegraph printer fitted with typefaces \n",
      "simulating a halftone pattern. \n",
      "Some of the initial problems in improving the visual quality of these early digital \n",
      "pictures were related to the selection of printing procedures and the distribution of \n",
      "1.2\n",
      "FIGURE 1.1\n",
      " A digital picture produced in 1921 from a coded tape by a telegraph printer with \n",
      "special typefaces. (McFarlane.) [References in the bibliography at the end of the book are \n",
      "listed in alphabetical order by authors’ last names.]\n",
      "DIP4E_GLOBAL_Print_Ready.indb   19\n",
      "6/16/2017   2:01:58 PM\n",
      "www.EBooksWorld.ir20\n",
      "    \n",
      "Chapter\n",
      " \n",
      "1\n",
      "  \n",
      "Introduction\n",
      "intensity levels. The printing method used to obtain Fig. 1.1 was abandoned toward \n",
      "the end of 1921 in favor of a technique based on photographic reproduction made \n",
      "from tapes perforated at the telegraph receiving terminal. Figure 1.2 shows an image \n",
      "obtained using this method. The improvements over Fig. 1.1 are evident, both in \n",
      "tonal quality and in resolution.\n",
      "The early Bartlane systems were capable of coding images in ﬁve distinct levels \n",
      "of gray. This capability was increased to 15 levels in 1929. Figure 1.3 is typical of the \n",
      "type of images that could be obtained using the 15-tone equipment. During this \n",
      "period, introduction of a system for developing a ﬁlm plate via light beams that were \n",
      "modulated by the coded picture tape improved the reproduction process consider-\n",
      "ably.\n",
      "Although the examples just cited involve digital images, they are not considered \n",
      "digital image processing results in the context of our deﬁnition, because digital com-\n",
      "puters were not used in their creation. Thus, the history of digital image processing \n",
      "is intimately tied to the development of the digital computer. In fact, digital images \n",
      "require so much storage and computational power that progress in the ﬁeld of digi-\n",
      "tal image processing has been dependent on the development of digital computers \n",
      "and of supporting technologies that include data storage, display, and transmission.\n",
      "FIGURE 1.2\n",
      "A digital picture \n",
      "made in 1922 \n",
      "from a tape \n",
      "punched after \n",
      "the signals had \n",
      "crossed the  \n",
      "Atlantic twice. \n",
      "(McFarlane.)\n",
      "FIGURE 1.3\n",
      "Unretouched \n",
      "cable picture of \n",
      "Generals Pershing \n",
      "(right) and Foch,  \n",
      "transmitted in \n",
      "1929 from  \n",
      "London to New \n",
      "York by 15-tone \n",
      "equipment. \n",
      "(McFarlane.)\n",
      "DIP4E_GLOBAL_Print_Ready.indb   20\n",
      "6/16/2017   2:01:58 PM\n",
      "www.EBooksWorld.ir1.2\n",
      "  \n",
      "The Origins of Digital Image Processing\n",
      "    \n",
      "21\n",
      "The concept of a computer dates back to the invention of the abacus in Asia \n",
      "Minor, more than 5000 years ago. More recently, there have been developments \n",
      "in the past two centuries that are the foundation of what we call a computer today. \n",
      "However, the basis for what we call a \n",
      "modern\n",
      " digital computer dates back to only \n",
      "the 1940s, with the introduction by John von Neumann of two key concepts: (1) a \n",
      "memory to hold a stored program and data, and (2) conditional branching. These \n",
      "two ideas are the foundation of a central processing unit (CPU), which is at the heart \n",
      "of computers today. Starting with von Neumann, there were a series of key advanc-\n",
      "es that led to computers powerful enough to be used for digital image processing. \n",
      "Brieﬂy, these advances may be summarized as follows: (1) the invention of the tran-\n",
      "sistor at Bell Laboratories in 1948; (2) the development in the 1950s and 1960s of \n",
      "the high-level programming languages COBOL (Common Business-Oriented Lan-\n",
      "guage) and FORTRAN (Formula Translator); (3) the invention of the integrated \n",
      "circuit (IC) at Texas Instruments in 1958; (4) the development of operating systems \n",
      "in the early 1960s; (5) the development of the microprocessor (a single chip consist-\n",
      "ing of a CPU, memory, and input and output controls) by Intel in the early 1970s; \n",
      "(6) the introduction by IBM of the personal computer in 1981; and (7) progressive \n",
      "miniaturization of components, starting with large-scale integration (LI) in the late \n",
      "1970s, then very-large-scale integration (VLSI) in the 1980s, to the present use of \n",
      "ultra-large-scale integration (ULSI) and experimental nonotechnologies. Concur-\n",
      "rent with these advances were developments in the areas of mass storage and display \n",
      "systems, both of which are fundamental requirements for digital image processing.\n",
      "The ﬁrst computers powerful enough to carry out meaningful image processing \n",
      "tasks appeared in the early 1960s. The birth of what we call digital image processing \n",
      "today can be traced to the availability of those machines, and to the onset of the \n",
      "space program during that period. It took the combination of those two develop-\n",
      "ments to bring into focus the potential of digital image processing for solving prob-\n",
      "lems of practical signiﬁcance. Work on using computer techniques for improving \n",
      "images from a space probe began at the Jet Propulsion Laboratory (Pasadena, Cali-\n",
      "fornia) in 1964, when pictures of the moon transmitted by \n",
      "Ranger 7\n",
      " were processed \n",
      "by a computer to correct various types of image distortion inherent in the on-board \n",
      "television camera. Figure 1.4 shows the ﬁrst image of the moon taken by \n",
      "Ranger \n",
      "7\n",
      " on July 31, 1964 at 9:09 A.M. Eastern Daylight Time (EDT), about 17 minutes \n",
      "before impacting the lunar surface (the markers, called \n",
      "reseau marks\n",
      ", are used for \n",
      "geometric corrections, as discussed in Chapter 2).This also is the ﬁrst image of the \n",
      "moon taken by a U.S. spacecraft. The imaging lessons learned with \n",
      "Ranger 7\n",
      " served \n",
      "as the basis for improved methods used to enhance and restore images from the Sur-\n",
      "veyor missions to the moon, the \n",
      "Mariner\n",
      " series of ﬂyby missions to Mars, the \n",
      "Apollo \n",
      "manned ﬂights to the moon, and others.\n",
      "In parallel with space applications, digital image processing techniques began in \n",
      "the late 1960s and early 1970s to be used in medical imaging, remote Earth resourc-\n",
      "es observations, and astronomy. The invention in the early 1970s of \n",
      "computerized \n",
      "axial tomography\n",
      " (CAT), also called \n",
      "computerized tomography\n",
      " (CT) for short, is \n",
      "one of the most important events in the application of image processing in medical \n",
      "diagnosis. Computerized axial tomography is a process in which a ring of detectors \n",
      "DIP4E_GLOBAL_Print_Ready.indb   21\n",
      "6/16/2017   2:01:58 PM\n",
      "www.EBooksWorld.ir22\n",
      "    \n",
      "Chapter\n",
      " \n",
      "1\n",
      "  \n",
      "Introduction\n",
      "encircles an object (or patient) and an X-ray source, concentric with the detector \n",
      "ring, rotates about the object. The X-rays pass through the object and are collected \n",
      "at the opposite end by the corresponding detectors in the ring. This procedure is \n",
      "repeated the source rotates. Tomography consists of algorithms that use the sensed \n",
      "data to construct an image that represents a “slice” through the object. Motion of \n",
      "the object in a direction perpendicular to the ring of detectors produces a set of \n",
      "such slices, which constitute a three-dimensional (3-D) rendition of the inside of the \n",
      "object. Tomography was invented independently by Sir Godfrey N. Hounsﬁeld and \n",
      "Professor Allan M. Cormack, who shared the 1979 Nobel Prize in Medicine for their \n",
      "invention. It is interesting to note that X-rays were discovered in 1895 by Wilhelm \n",
      "Conrad Roentgen, for which he received the 1901 Nobel Prize for Physics. These two \n",
      "inventions, nearly 100 years apart, led to some of the most important applications of \n",
      "image processing today.\n",
      "From the 1960s until the present, the ﬁeld of image processing has grown vigor-\n",
      "ously. In addition to applications in medicine and the space program, digital image \n",
      "processing techniques are now used in a broad range of applications. Computer pro-\n",
      "cedures are used to enhance the contrast or code the intensity levels into color for \n",
      "easier interpretation of X-rays and other images used in industry, medicine, and the \n",
      "biological sciences. Geographers use the same or similar techniques to study pollu-\n",
      "tion patterns from aerial and satellite imagery. Image enhancement and restoration \n",
      "procedures are used to process degraded images of unrecoverable objects, or experi-\n",
      "mental results too expensive to duplicate. In archeology, image processing meth-\n",
      "ods have successfully restored blurred pictures that were the only available records \n",
      "of rare artifacts lost or damaged after being photographed. In physics and related \n",
      "ﬁelds, computer techniques routinely enhance images of experiments in areas such \n",
      "as high-energy plasmas and electron microscopy. Similarly successful applications \n",
      "of image processing concepts can be found in astronomy, biology, nuclear medicine, \n",
      "law enforcement, defense, and industry.\n",
      "FIGURE 1.4\n",
      "The ﬁrst picture \n",
      "of the moon by \n",
      "a U.S. spacecraft. \n",
      "Ranger 7\n",
      " took \n",
      "this image on \n",
      "July 31, 1964 at \n",
      "9:09 A.M. EDT, \n",
      "about 17 minutes \n",
      "before impacting \n",
      "the lunar surface. \n",
      "(Courtesy of \n",
      "NASA.) \n",
      "DIP4E_GLOBAL_Print_Ready.indb   22\n",
      "6/16/2017   2:01:59 PM\n",
      "www.EBooksWorld.ir1.3\n",
      "  \n",
      "Examples of Fields that Use Digital Image Processing\n",
      "    \n",
      "23\n",
      "These examples illustrate processing results intended for human interpretation. \n",
      "The second major area of application of digital image processing techniques men-\n",
      "tioned at the beginning of this chapter is in solving problems dealing with machine \n",
      "perception. In this case, interest is on procedures for extracting information from \n",
      "an image, in a form suitable for computer processing. Often, this information bears \n",
      "little resemblance to visual features that humans use in interpreting the content \n",
      "of an image. Examples of the type of information used in machine perception are \n",
      "statistical moments, Fourier transform coefﬁcients, and multidimensional distance \n",
      "measures. Typical problems in machine perception that routinely utilize image pro-\n",
      "cessing techniques are automatic character recognition, industrial machine vision \n",
      "for product assembly and inspection, military recognizance, automatic processing of \n",
      "ﬁngerprints, screening of X-rays and blood samples, and machine processing of aer-\n",
      "ial and satellite imagery for weather prediction and environmental assessment. The \n",
      "continuing decline in the ratio of computer price to performance, and the expansion \n",
      "of networking and communication bandwidth via the internet, have created unprec-\n",
      "edented opportunities for continued growth of digital image processing. Some of \n",
      "these application areas will be illustrated in the following section.\n",
      "1.3 EXAMPLES OF FIELDS THAT USE DIGITAL IMAGE PROCESSING  \n",
      "Today, there is almost no area of technical endeavor that is not impacted in some \n",
      "way by digital image processing. We can cover only a few of these applications in the \n",
      "context and space of the current discussion. However, limited as it is, the material \n",
      "presented in this section will leave no doubt in your mind regarding the breadth and \n",
      "importance of digital image processing. We show in this section numerous areas of \n",
      "application, each of which routinely utilizes the digital image processing techniques \n",
      "developed in the following chapters. Many of the images shown in this section are \n",
      "used later in one or more of the examples given in the book. Most images shown are \n",
      "digital images. \n",
      "The areas of application of digital image processing are so varied that some form \n",
      "of organization is desirable in attempting to capture the breadth of this ﬁeld. One \n",
      "of the simplest ways to develop a basic understanding of the extent of image pro-\n",
      "cessing applications is to categorize images according to their source (e.g., X-ray, \n",
      "visual, infrared, and so on).The principal energy source for images in use today is \n",
      "the electromagnetic energy spectrum. Other important sources of energy include \n",
      "acoustic, ultrasonic, and electronic (in the form of electron beams used in electron \n",
      "microscopy). Synthetic images, used for modeling and visualization, are generated \n",
      "by computer. In this section we will discuss brieﬂy how images are generated in \n",
      "these various categories, and the areas in which they are applied. Methods for con-\n",
      "verting images into digital form will be discussed in the next chapter.\n",
      "Images based on radiation from the EM spectrum are the most familiar, espe-\n",
      "cially images in the X-ray and visual bands of the spectrum. Electromagnetic waves \n",
      "can be conceptualized as propagating sinusoidal waves of varying wavelengths, or \n",
      "they can be thought of as a stream of massless particles, each traveling in a wavelike \n",
      "pattern and moving at the speed of light. Each massless particle contains a certain \n",
      "amount (or bundle) of energy. Each bundle of energy is called a \n",
      "photon\n",
      ". If spectral \n",
      "1.3\n",
      "DIP4E_GLOBAL_Print_Ready.indb   23\n",
      "6/16/2017   2:01:59 PM\n",
      "www.EBooksWorld.ir24\n",
      "    \n",
      "Chapter\n",
      " \n",
      "1\n",
      "  \n",
      "Introduction\n",
      "bands are grouped according to energy per photon, we obtain the spectrum shown \n",
      "in Fig. 1.5, ranging from gamma rays (highest energy) at one end to radio waves \n",
      "(lowest energy) at the other. The bands are shown shaded to convey the fact that \n",
      "bands of the EM spectrum are not distinct, but rather transition smoothly from one \n",
      "to the other.\n",
      "GAMMA-RAY IMAGING\n",
      "Major uses of imaging based on gamma rays include nuclear medicine and astro-\n",
      "nomical observations. In nuclear medicine, the approach is to inject a patient with a \n",
      "radioactive isotope that emits gamma rays as it decays. Images are produced from \n",
      "the emissions collected by gamma-ray detectors. Figure 1.6(a) shows an image of a \n",
      "complete bone scan obtained by using gamma-ray imaging. Images of this sort are \n",
      "used to locate sites of bone pathology, such as infections or tumors. Figure 1.6(b) \n",
      "shows another major modality of nuclear imaging called \n",
      "positron emission tomogra-\n",
      "phy\n",
      " (PET). The principle is the same as with X-ray tomography, mentioned briefly \n",
      "in Section 1.2. However, instead of using an external source of X-ray energy, the \n",
      "patient is given a radioactive isotope that emits positrons as it decays. When a pos-\n",
      "itron meets an electron, both are annihilated and two gamma rays are given off. \n",
      "These are detected and a tomographic image is created using the basic principles of \n",
      "tomography. The image shown in Fig. 1.6(b) is one sample of a sequence that con-\n",
      "stitutes a 3-D rendition of the patient. This image shows a tumor in the brain and \n",
      "another in the lung, easily visible as small white masses.\n",
      "A star in the constellation of Cygnus exploded about 15,000 years ago, generat-\n",
      "ing a superheated, stationary gas cloud (known as the Cygnus Loop) that glows in \n",
      "a spectacular array of colors. Figure 1.6(c) shows an image of the Cygnus Loop in \n",
      "the gamma-ray band. Unlike the two examples in Figs. 1.6(a) and (b), this image was \n",
      "obtained using the natural radiation of the object being imaged. Finally, Fig. 1.6(d) \n",
      "shows an image of gamma radiation from a valve in a nuclear reactor. An area of \n",
      "strong radiation is seen in the lower left side of the image.\n",
      "X-RAY IMAGING\n",
      "X-rays are among the oldest sources of EM radiation used for imaging. The best \n",
      "known use of X-rays is medical diagnostics, but they are also used extensively in \n",
      "industry and other areas, such as astronomy. X-rays for medical and industrial imag-\n",
      "ing are generated using an X-ray tube, which is a vacuum tube with a cathode and \n",
      "anode. The cathode is heated, causing free electrons to be released. These electrons \n",
      "flow at high speed to the positively charged anode. When the electrons strike a \n",
      "10\n",
      "/H11002\n",
      "9\n",
      "10\n",
      "/H11002\n",
      "8\n",
      "10\n",
      "/H11002\n",
      "7\n",
      "10\n",
      "/H11002\n",
      "6\n",
      "10\n",
      "/H11002\n",
      "5\n",
      "10\n",
      "/H11002\n",
      "4\n",
      "10\n",
      "/H11002\n",
      "3\n",
      "10\n",
      "/H11002\n",
      "2\n",
      "10\n",
      "0\n",
      "10\n",
      "/H11002\n",
      "1\n",
      "10\n",
      "1\n",
      "10\n",
      "2\n",
      "10\n",
      "3\n",
      "10\n",
      "4\n",
      "10\n",
      "5\n",
      "10\n",
      "6\n",
      "Energy of one photon (electron volts)\n",
      "Gamma rays X-rays Ultraviolet Visible Infrared Microwaves\n",
      "Radio waves\n",
      "FIGURE 1.5\n",
      " The electromagnetic spectrum arranged according to energy per photon.\n",
      "DIP4E_GLOBAL_Print_Ready.indb   24\n",
      "6/16/2017   2:01:59 PM\n",
      "www.EBooksWorld.ir1.3\n",
      "  \n",
      "Examples of Fields that Use Digital Image Processing\n",
      "    \n",
      "25\n",
      "nucleus, energy is released in the form of X-ray radiation. The energy (penetrat-\n",
      "ing power) of X-rays is controlled by a voltage applied across the anode, and by a \n",
      "current applied to the filament in the cathode. Figure 1.7(a) shows a familiar chest \n",
      "X-ray generated simply by placing the patient between an X-ray source and a film \n",
      "sensitive to X-ray energy. The intensity of the X-rays is modified by absorption as \n",
      "they pass through the patient, and the resulting energy falling on the film develops it, \n",
      "much in the same way that light develops photographic film. In digital radiography, \n",
      "b a\n",
      "d c\n",
      "FIGURE 1.6\n",
      "Examples of \n",
      "gamma-ray  \n",
      "imaging.  \n",
      "(a) Bone scan.  \n",
      "(b) PET image. \n",
      "(c) Cygnus Loop. \n",
      "(d) Gamma radia-\n",
      "tion (bright spot) \n",
      "from a reactor \n",
      "valve.  \n",
      "(Images  \n",
      "courtesy of  \n",
      "(a) G.E. Medical \n",
      "Systems; (b) Dr. \n",
      "Michael E. Casey, \n",
      "CTI PET Systems; \n",
      "(c) NASA;  \n",
      "(d) Professors \n",
      "Zhong He and \n",
      "David K. Wehe,  \n",
      "University of \n",
      "Michigan.) \n",
      "DIP4E_GLOBAL_Print_Ready.indb   25\n",
      "6/16/2017   2:01:59 PM\n",
      "www.EBooksWorld.ir26\n",
      "    \n",
      "Chapter\n",
      " \n",
      "1\n",
      "  \n",
      "Introduction\n",
      "digital images are obtained by one of two methods: (1) by digitizing X-ray films; or; \n",
      "(2) by having the X-rays that pass through the patient fall directly onto devices (such \n",
      "as a phosphor screen) that convert X-rays to light. The light signal in turn is captured \n",
      "by a light-sensitive digitizing system. We will discuss digitization in more detail in \n",
      "Chapters 2 and 4.\n",
      "b\n",
      "a\n",
      "d\n",
      "c\n",
      "e\n",
      "FIGURE 1.7\n",
      "Examples of \n",
      "X-ray imaging.  \n",
      "(a) Chest X-ray. \n",
      "(b) Aortic  \n",
      "angiogram.  \n",
      "(c) Head CT.  \n",
      "(d) Circuit boards. \n",
      "(e) Cygnus Loop. \n",
      "(Images courtesy \n",
      "of (a) and (c) Dr. \n",
      "David R. Pickens, \n",
      "Dept. of  \n",
      "Radiology & \n",
      "Radiological  \n",
      "Sciences,  \n",
      "Vanderbilt  \n",
      "University  \n",
      "Medical Center; \n",
      "(b) Dr. Thomas \n",
      "R. Gest, Division \n",
      "of Anatomical \n",
      "Sciences, Univ. of \n",
      "Michigan Medical \n",
      "School;  \n",
      "(d) Mr. Joseph \n",
      "E. Pascente, Lixi, \n",
      "Inc.; and  \n",
      "(e) NASA.) \n",
      "DIP4E_GLOBAL_Print_Ready.indb   26\n",
      "6/16/2017   2:01:59 PM\n",
      "www.EBooksWorld.ir1.3\n",
      "  \n",
      "Examples of Fields that Use Digital Image Processing\n",
      "    \n",
      "27\n",
      "Angiography is another major application in an area called contrast enhancement \n",
      "radiography. This procedure is used to obtain images of blood vessels, called \n",
      "angio-\n",
      "grams\n",
      ". A catheter (a small, ﬂexible, hollow tube) is inserted, for example, into an \n",
      "artery or vein in the groin. The catheter is threaded into the blood vessel and guided \n",
      "to the area to be studied. When the catheter reaches the site under investigation, \n",
      "an X-ray contrast medium is injected through the tube. This enhances the contrast \n",
      "of the blood vessels and enables a radiologist to see any irregularities or blockages. \n",
      "Figure 1.7(b) shows an example of an aortic angiogram. The catheter can be seen \n",
      "being inserted into the large blood vessel on the lower left of the picture. Note the \n",
      "high contrast of the large vessel as the contrast medium ﬂows up in the direction of \n",
      "the kidneys, which are also visible in the image. As we will discuss further in Chapter 2, \n",
      "angiography is a major area of digital image processing, where image subtraction is \n",
      "used to further enhance the blood vessels being studied.\n",
      "Another important use of X-rays in medical imaging is computerized axial tomog-\n",
      "raphy (CAT). Due to their resolution and 3-D capabilities, CAT scans revolution-\n",
      "ized medicine from the moment they ﬁrst became available in the early 1970s. As \n",
      "noted in Section 1.2, each CAT image is a “slice” taken perpendicularly through \n",
      "the patient. Numerous slices are generated as the patient is moved in a longitudinal \n",
      "direction. The ensemble of such images constitutes a 3-D rendition of the inside of \n",
      "the body, with the longitudinal resolution being proportional to the number of slice \n",
      "images taken. Figure 1.7(c) shows a typical CAT slice image of a human head.\n",
      "Techniques similar to the ones just discussed, but generally involving higher \n",
      "energy X-rays, are applicable in industrial processes. Figure 1.7(d) shows an X-ray \n",
      "image of an electronic circuit board. Such images, representative of literally hundreds \n",
      "of industrial applications of X-rays, are used to examine circuit boards for ﬂaws in \n",
      "manufacturing, such as missing components or broken traces. Industrial CAT scans \n",
      "are useful when the parts can be penetrated by X-rays, such as in plastic assemblies, \n",
      "and even large bodies, such as solid-propellant rocket motors. Figure 1.7(e) shows an \n",
      "example of X-ray imaging in astronomy. This image is the Cygnus Loop of Fig. 1.6(c), \n",
      "but imaged in the X-ray band.\n",
      "IMAGING IN THE ULTRAVIOLET BAND\n",
      "Applications of ultraviolet “light” are varied. They include lithography, industrial \n",
      "inspection, microscopy, lasers, biological imaging, and astronomical observations. \n",
      "We illustrate imaging in this band with examples from microscopy and astronomy.\n",
      "Ultraviolet light is used in ﬂuorescence microscopy, one of the fastest growing \n",
      "areas of microscopy. Fluorescence is a phenomenon discovered in the middle of the \n",
      "nineteenth century, when it was ﬁrst observed that the mineral ﬂuorspar ﬂuoresces \n",
      "when ultraviolet light is directed upon it. The ultraviolet light itself is not visible, but \n",
      "when a photon of ultraviolet radiation collides with an electron in an atom of a ﬂuo-\n",
      "rescent material, it elevates the electron to a higher energy level. Subsequently, the \n",
      "excited electron relaxes to a lower level and emits light in the form of a lower-energy \n",
      "photon in the visible (red) light region. Important tasks performed with a ﬂuores-\n",
      "cence microscope are to use an excitation light to irradiate a prepared specimen, \n",
      "and then to separate the much weaker radiating ﬂuorescent light from the brighter \n",
      "DIP4E_GLOBAL_Print_Ready.indb   27\n",
      "6/16/2017   2:01:59 PM\n",
      "www.EBooksWorld.ir28\n",
      "    \n",
      "Chapter\n",
      " \n",
      "1\n",
      "  \n",
      "Introduction\n",
      "excitation light. Thus, only the emission light reaches the eye or other detector. The \n",
      "resulting ﬂuorescing areas shine against a dark background with sufﬁcient contrast \n",
      "to permit detection. The darker the background of the nonﬂuorescing material, the \n",
      "more efﬁcient the instrument.\n",
      "Fluorescence microscopy is an excellent method for studying materials that can be \n",
      "made to ﬂuoresce, either in their natural form (primary ﬂuorescence) or when treat-\n",
      "ed with chemicals capable of ﬂuorescing (secondary ﬂuorescence). Figures 1.8(a) \n",
      "and (b) show results typical of the capability of ﬂuorescence microscopy. Figure \n",
      "1.8(a) shows a ﬂuorescence microscope image of normal corn, and Fig. 1.8(b) shows \n",
      "corn infected by “smut,” a disease of cereals, corn, grasses, onions, and sorghum that \n",
      "can be caused by any one of more than 700 species of parasitic fungi. Corn smut is \n",
      "particularly harmful because corn is one of the principal food sources in the world. \n",
      "As another illustration, Fig. 1.8(c) shows the Cygnus Loop imaged in the high-energy \n",
      "region of the ultraviolet band.\n",
      "IMAGING IN THE VISIBLE AND INFRARED BANDS\n",
      "Considering that the visual band of the electromagnetic spectrum is the most famil-\n",
      "iar in all our activities, it is not surprising that imaging in this band outweighs by far \n",
      "all the others in terms of breadth of application. The infrared band often is used in \n",
      "conjunction with visual imaging, so we have grouped the visible and infrared bands \n",
      "in this section for the purpose of illustration. We consider in the following discus-\n",
      "sion applications in light microscopy, astronomy, remote sensing, industry, and law \n",
      "enforcement.\n",
      "Figure 1.9 shows several examples of images obtained with a light microscope. \n",
      "The examples range from pharmaceuticals and microinspection to materials char-\n",
      "acterization. Even in microscopy alone, the application areas are too numerous to \n",
      "detail here. It is not difﬁcult to conceptualize the types of processes one might apply \n",
      "to these images, ranging from enhancement to measurements.\n",
      "b a\n",
      "c\n",
      "FIGURE 1.8\n",
      " Examples of ultraviolet imaging. (a) Normal corn. (b) Corn infected by smut. (c) Cygnus Loop. (Images \n",
      "(a) and (b) courtesy of Dr. Michael W. Davidson, Florida State University, (c) NASA.) \n",
      "DIP4E_GLOBAL_Print_Ready.indb   28\n",
      "6/16/2017   2:01:59 PM\n",
      "www.EBooksWorld.ir1.3\n",
      "  \n",
      "Examples of Fields that Use Digital Image Processing\n",
      "    \n",
      "29\n",
      "Another major area of visual processing is remote sensing, which usually includes \n",
      "several bands in the visual and infrared regions of the spectrum. Table 1.1 shows the \n",
      "so-called \n",
      "thematic bands\n",
      " in NASA’s LANDSAT satellites. The primary function of \n",
      "LANDSAT is to obtain and transmit images of the Earth from space, for purposes \n",
      "of monitoring environmental conditions on the planet. The bands are expressed in \n",
      "terms of wavelength, with \n",
      "1\n",
      "m\n",
      "m\n",
      " being equal to \n",
      "10\n",
      "6\n",
      "−\n",
      " m (we will discuss the wave-\n",
      "length regions of the electromagnetic spectrum in more detail in Chapter 2). Note \n",
      "the characteristics and uses of each band in Table 1.1.\n",
      "In order to develop a basic appreciation for the power of this type of multispec-\n",
      "tral imaging, consider Fig. 1.10, which shows one image for each of the spectral bands \n",
      "in Table 1.1. The area imaged is Washington D.C., which includes features such as \n",
      "buildings, roads, vegetation, and a major river (the Potomac) going though the city. \n",
      "b a\n",
      "c\n",
      "e\n",
      "d\n",
      "f\n",
      "FIGURE 1.9\n",
      "Examples of light  \n",
      "microscopy images.  \n",
      "(a) Taxol (antican-\n",
      "cer agent), magni-\n",
      "ﬁed \n",
      "250\n",
      "×\n",
      ". \n",
      "(b) Cholesterol—\n",
      "40\n",
      "×\n",
      ".  \n",
      "(c) Microproces-\n",
      "sor—\n",
      "60\n",
      "×\n",
      ".  \n",
      "(d) Nickel oxide \n",
      "thin ﬁlm—\n",
      "600\n",
      "×\n",
      ". \n",
      "(e) Surface of audio \n",
      "CD—\n",
      "1750\n",
      "×\n",
      ".  \n",
      "(f) Organic super\n",
      "-\n",
      "conductor— \n",
      "450\n",
      "×\n",
      ". \n",
      "(Images courtesy of \n",
      "Dr\n",
      ". Michael W.  \n",
      "Davidson, Florida \n",
      "State University.) \n",
      "DIP4E_GLOBAL_Print_Ready.indb   29\n",
      "6/16/2017   2:02:00 PM\n",
      "www.EBooksWorld.ir30\n",
      "    \n",
      "Chapter\n",
      " \n",
      "1\n",
      "  \n",
      "Introduction\n",
      "Images of population centers are used over time to assess population growth and \n",
      "shift patterns, pollution, and other factors affecting the environment. The differenc-\n",
      "es between visual and infrared image features are quite noticeable in these images. \n",
      "Observe, for example, how well deﬁned the river is from its surroundings in Bands \n",
      "4 and 5.\n",
      "Weather observation and prediction also are major applications of multispectral \n",
      "imaging from satellites. For example, Fig. 1.11 is an image of Hurricane Katrina, one \n",
      "of the most devastating storms in recent memory in the Western Hemisphere. This \n",
      "image was taken by a National Oceanographic and Atmospheric Administration \n",
      "(NOAA) satellite using sensors in the visible and infrared bands. The eye of the hur-\n",
      "ricane is clearly visible in this image.\n",
      "Band No. Name\n",
      "Wavelength \n",
      "(\n",
      "M\n",
      "m)\n",
      "Characteristics and Uses\n",
      "1 Visible blue\n",
      "0.45– 0.52 Maximum water penetration\n",
      "2 Visible green\n",
      "0.53– 0.61 Measures plant vigor\n",
      "3 Visible red\n",
      "0.63– 0.69 Vegetation discrimination\n",
      "4 Near infrared\n",
      "0.78– 0.90 Biomass and shoreline mapping\n",
      "5 Middle infrared 1.55–1.75 Moisture content: soil/vegetation\n",
      "6 Thermal infrared 10.4–12.5 Soil moisture; thermal mapping\n",
      "7 Short-wave infrared 2.09–2.35 Mineral mapping\n",
      "TABLE \n",
      "1.1\n",
      "Thematic bands \n",
      "of NASA’s \n",
      "LANDSAT  \n",
      "satellite.\n",
      "123\n",
      "4567\n",
      "FIGURE 1.10\n",
      " LANDSAT satellite images of the Washington, D.C. area. The numbers refer to the thematic bands in \n",
      "Table 1.1. (Images courtesy of NASA.)\n",
      "DIP4E_GLOBAL_Print_Ready.indb   30\n",
      "6/16/2017   2:02:00 PM\n",
      "www.EBooksWorld.ir1.3\n",
      "  \n",
      "Examples of Fields that Use Digital Image Processing\n",
      "    \n",
      "31\n",
      "Figures 1.12 and 1.13 show an application of infrared imaging. These images are \n",
      "part of the Nighttime Lights of the World data set, which provides a global inventory \n",
      "of human settlements. The images were generated by an infrared imaging system \n",
      "mounted on a NOAA/DMSP (Defense Meteorological Satellite Program) satel-\n",
      "lite. The infrared system operates in the band 10.0 to 13.4 \n",
      "m\n",
      "m,\n",
      " and has the unique \n",
      "capability to observe faint sources of visible\n",
      ", near infrared emissions present on the  \n",
      "Earth’s surface, including cities, towns, villages, gas ﬂares, and ﬁres. Even without \n",
      "formal training in image processing, it is not difﬁcult to imagine writing a computer \n",
      "program that would use these images to estimate the relative percent of total electri-\n",
      "cal energy used by various regions of the world.\n",
      "A major area of imaging in the visible spectrum is in automated visual inspection \n",
      "of manufactured goods. Figure 1.14 shows some examples. Figure 1.14(a) is a con-\n",
      "troller board for a CD-ROM drive. A typical image processing task with products \n",
      "such as this is to inspect them for missing parts (the black square on the top, right \n",
      "quadrant of the image is an example of a missing component).\n",
      "Figure 1.14(b) is an imaged pill container. The objective here is to have a machine \n",
      "look for missing, incomplete, or deformed pills. Figure 1.14(c) shows an application \n",
      "in which image processing is used to look for bottles that are not ﬁlled up to an \n",
      "acceptable level. Figure 1.14(d) shows a clear plastic part with an unacceptable num-\n",
      "ber of air pockets in it. Detecting anomalies like these is a major theme of industrial \n",
      "inspection that includes other products, such as wood and cloth. Figure 1.14(e) shows \n",
      "a batch of cereal during inspection for color and the presence of anomalies such as \n",
      "burned ﬂakes. Finally, Fig. 1.14(f) shows an image of an intraocular implant (replace-\n",
      "ment lens for the human eye). A “structured light” illumination technique was used \n",
      "to highlight deformations toward the center of the lens, and other imperfections. For \n",
      "example, the markings at 1 o’clock and 5 o’clock are tweezer damage. Most of the \n",
      "other small speckle detail is debris. The objective in this type of inspection is to ﬁnd \n",
      "damaged or incorrectly manufactured implants automatically, prior to packaging.\n",
      "FIGURE 1.11\n",
      "Satellite image of \n",
      "Hurricane Katrina \n",
      "taken on August \n",
      "29, 2005.  \n",
      "(Courtesy of \n",
      "NOAA.)\n",
      "DIP4E_GLOBAL_Print_Ready.indb   31\n",
      "6/16/2017   2:02:00 PM\n",
      "www.EBooksWorld.ir32\n",
      "    \n",
      "Chapter\n",
      " \n",
      "1\n",
      "  \n",
      "Introduction\n",
      "Figure 1.15 illustrates some additional examples of image processing in the vis-\n",
      "ible spectrum. Figure 1.15(a) shows a thumb print. Images of ﬁngerprints are rou-\n",
      "tinely processed by computer, either to enhance them or to ﬁnd features that aid \n",
      "in the automated search of a database for potential matches. Figure 1.15(b) shows \n",
      "an image of paper currency. Applications of digital image processing in this area \n",
      "FIGURE 1.12\n",
      "Infrared  \n",
      "satellite images of \n",
      "the Americas. The \n",
      "small shaded map \n",
      "is provided for  \n",
      "reference.  \n",
      "(Courtesy of \n",
      "NOAA.) \n",
      "DIP4E_GLOBAL_Print_Ready.indb   32\n",
      "6/16/2017   2:02:00 PM\n",
      "www.EBooksWorld.ir1.3\n",
      "  \n",
      "Examples of Fields that Use Digital Image Processing\n",
      "    \n",
      "33\n",
      "include automated counting and, in law enforcement, the reading of the serial num-\n",
      "ber for the purpose of tracking and identifying currency bills. The two vehicle images \n",
      "shown in Figs. 1.15(c) and (d) are examples of automated license plate reading. The \n",
      "light rectangles indicate the area in which the imaging system detected the plate. \n",
      "The black rectangles show the results of automatically reading the plate content by \n",
      "the system. License plate and other applications of character recognition are used \n",
      "extensively for trafﬁc monitoring and surveillance.\n",
      "IMAGING IN THE MICROWAVE BAND\n",
      "The principal application of imaging in the microwave band is radar. The unique \n",
      "feature of imaging radar is its ability to collect data over virtually any region at any \n",
      "time, regardless of weather or ambient lighting conditions. Some radar waves can \n",
      "penetrate clouds, and under certain conditions, can also see through vegetation, ice, \n",
      "and dry sand. In many cases, radar is the only way to explore inaccessible regions of \n",
      "the Earth’s surface. An imaging radar works like a flash camera in that it provides \n",
      "its own illumination (microwave pulses) to illuminate an area on the ground and \n",
      "FIGURE 1.13\n",
      "Infrared  \n",
      "satellite images \n",
      "of the remaining \n",
      "populated parts \n",
      "of the world. The \n",
      "small shaded map \n",
      "is provided for \n",
      "reference.  \n",
      "(Courtesy of \n",
      "NOAA.) \n",
      "DIP4E_GLOBAL_Print_Ready.indb   33\n",
      "6/16/2017   2:02:00 PM\n",
      "www.EBooksWorld.ir34\n",
      "    \n",
      "Chapter\n",
      " \n",
      "1\n",
      "  \n",
      "Introduction\n",
      "take a snapshot image. Instead of a camera lens, a radar uses an antenna and digital \n",
      "computer processing to record its images. In a radar image, one can see only the \n",
      "microwave energy that was reflected back toward the radar antenna.\n",
      "Figure 1.16 shows a spaceborne radar image covering a rugged mountainous area \n",
      "of southeast Tibet, about 90 km east of the city of Lhasa. In the lower right cor-\n",
      "ner is a wide valley of the Lhasa River, which is populated by Tibetan farmers and \n",
      "yak herders, and includes the village of Menba. Mountains in this area reach about \n",
      "5800 m (19,000 ft) above sea level, while the valley ﬂoors lie about 4300 m (14,000 ft) \n",
      "above sea level. Note the clarity and detail of the image, unencumbered by clouds or \n",
      "other atmospheric conditions that normally interfere with images in the visual band.\n",
      "IMAGING IN THE RADIO BAND\n",
      "As in the case of imaging at the other end of the spectrum (gamma rays), the major \n",
      "applications of imaging in the radio band are in medicine and astronomy. In medicine, \n",
      "radio waves are used in magnetic resonance imaging (MRI). This technique places a \n",
      "b a\n",
      "c\n",
      "e\n",
      "d\n",
      "f\n",
      "FIGURE 1.14\n",
      " Some examples of manufactured goods checked using digital image processing. (a) Circuit board con-\n",
      "troller. (b) Packaged pills. (c) Bottles. (d) Air bubbles in a clear plastic product. (e) Cereal. (f) Image of intraocular \n",
      "implant. (Figure (f) courtesy of Mr. Pete Sites, Perceptics Corporation.) \n",
      "DIP4E_GLOBAL_Print_Ready.indb   34\n",
      "6/16/2017   2:02:00 PM\n",
      "www.EBooksWorld.ir1.3\n",
      "  \n",
      "Examples of Fields that Use Digital Image Processing\n",
      "    \n",
      "35\n",
      "patient in a powerful magnet and passes radio waves through the individual’s body \n",
      "in short pulses. Each pulse causes a responding pulse of radio waves to be emitted \n",
      "by the patient’s tissues. The location from which these signals originate and their \n",
      "strength are determined by a computer, which produces a two-dimensional image \n",
      "of a section of the patient. MRI can produce images in any plane. Figure 1.17 shows \n",
      "MRI images of a human knee and spine.\n",
      "The rightmost image in Fig. 1.18 is an image of the Crab Pulsar in the radio band. \n",
      "Also shown for an interesting comparison are images of the same region, but taken \n",
      "in most of the bands discussed earlier. Observe that each image gives a totally dif-\n",
      "ferent “view” of the pulsar.\n",
      "OTHER IMAGING MODALITIES\n",
      "Although imaging in the electromagnetic spectrum is dominant by far, there are a \n",
      "number of other imaging modalities that are also important. Specifically, we discuss \n",
      "b\n",
      "a\n",
      "d\n",
      "c\n",
      "FIGURE 1.15\n",
      "Some additional \n",
      "examples of  \n",
      "imaging in the  \n",
      "visible spectrum. \n",
      "(a) Thumb print. \n",
      "(b) Paper  \n",
      "currency.  \n",
      "(c) and (d) Auto-\n",
      "mated license \n",
      "plate reading.  \n",
      "(Figure (a) \n",
      "courtesy of the \n",
      "National  \n",
      "Institute of  \n",
      "Standards and \n",
      "Technology.  \n",
      "Figures (c) and \n",
      "(d) courtesy of \n",
      "Dr. Juan  \n",
      "Herrera,  \n",
      "Perceptics  \n",
      "Corporation.) \n",
      "DIP4E_GLOBAL_Print_Ready.indb   35\n",
      "6/16/2017   2:02:00 PM\n",
      "www.EBooksWorld.ir36\n",
      "    \n",
      "Chapter\n",
      " \n",
      "1\n",
      "  \n",
      "Introduction\n",
      "in this section acoustic imaging, electron microscopy, and synthetic (computer-gen-\n",
      "erated) imaging. \n",
      "Imaging using “sound” ﬁnds application in geological exploration, industry, and \n",
      "medicine. Geological applications use sound in the low end of the sound spectrum \n",
      "(hundreds of Hz) while imaging in other areas use ultrasound (millions of Hz). The \n",
      "most important commercial applications of image processing in geology are in min-\n",
      "eral and oil exploration. For image acquisition over land, one of the main approaches \n",
      "is to use a large truck and a large ﬂat steel plate. The plate is pressed on the \n",
      "ground by \n",
      "FIGURE 1.16\n",
      "Spaceborne radar \n",
      "image of  \n",
      "mountainous \n",
      "region in  \n",
      "southeast Tibet. \n",
      "(Courtesy of \n",
      "NASA.)\n",
      "b a\n",
      "FIGURE 1.17\n",
      " MRI images of a human (a) knee, and (b) spine. (Figure (a) courtesy of Dr. Thom-\n",
      "as R. Gest, Division of Anatomical Sciences, University of Michigan Medical School, and \n",
      "(b) courtesy of Dr. David R. Pickens, Department of Radiology and Radiological Sciences, \n",
      "Vanderbilt University Medical Center.)\n",
      "DIP4E_GLOBAL_Print_Ready.indb   36\n",
      "6/16/2017   2:02:01 PM\n",
      "www.EBooksWorld.ir1.3\n",
      "  \n",
      "Examples of Fields that Use Digital Image Processing\n",
      "    \n",
      "37\n",
      "the truck, and the truck is vibrated through a frequency spectrum up to 100 Hz. The \n",
      "strength and speed of the returning sound waves are determined by the composi-\n",
      "tion of the Earth below the surface. These are analyzed by computer, and images are \n",
      "generated from the resulting analysis.\n",
      "For marine image acquisition, the energy source consists usually of two air guns \n",
      "towed behind a ship. Returning sound waves are detected by hydrophones placed \n",
      "in cables that are either towed behind the ship, laid on the bottom of the ocean, \n",
      "or hung from buoys (vertical cables). The two air guns are alternately pressurized  \n",
      "to ~2000 psi and then set off. The constant motion of the ship provides a transversal \n",
      "direction of motion that, together with the returning sound waves, is used to gener-\n",
      "ate a 3-D map of the composition of the Earth below the bottom of the ocean.\n",
      "Figure 1.19 shows a cross-sectional image of a well-known 3-D model against \n",
      "which the performance of seismic imaging algorithms is tested. The arrow points to a \n",
      "hydrocarbon (oil and/or gas) trap. This target is brighter than the surrounding layers \n",
      "because the change in density in the target region is larger. Seismic interpreters look \n",
      "for these “bright spots” to ﬁnd oil and gas. The layers above also are bright, but their \n",
      "brightness does not vary as strongly across the layers. Many seismic reconstruction \n",
      "algorithms have difﬁculty imaging this target because of the faults above it.\n",
      "Although ultrasound imaging is used routinely in manufacturing, the best known \n",
      "applications of this technique are in medicine, especially in obstetrics, where fetuses \n",
      "are imaged to determine the health of their development. A byproduct of this \n",
      "Gamma\n",
      "X-ray\n",
      "Optical\n",
      "Infrared\n",
      "Radio\n",
      "FIGURE 1.18\n",
      " Images of the Crab Pulsar (in the center of each image) covering the electromagnetic spectrum. (Cour-\n",
      "tesy of NASA.)\n",
      "FIGURE 1.19\n",
      "Cross-sectional \n",
      "image of a  \n",
      "seismic model. \n",
      "The arrow points \n",
      "to a hydrocarbon \n",
      "(oil and/or gas) \n",
      "trap. (Courtesy of \n",
      "Dr. Curtis Ober, \n",
      "Sandia National \n",
      "Laboratories.)\n",
      "DIP4E_GLOBAL_Print_Ready.indb   37\n",
      "6/16/2017   2:02:01 PM\n",
      "www.EBooksWorld.ir38\n",
      "    \n",
      "Chapter\n",
      " \n",
      "1\n",
      "  \n",
      "Introduction\n",
      "examination is determining the sex of the baby. Ultrasound images are generated \n",
      "using the following basic procedure:\n",
      "1. \n",
      "The ultrasound system (a computer, ultrasound probe consisting of a source, a \n",
      "receiver\n",
      ", and a display) transmits high-frequency (1 to 5 MHz) sound pulses \n",
      "into the body.\n",
      "2. \n",
      "The sound waves travel into the body and hit a boundary between tissues (e.g., \n",
      "between ﬂuid and soft tissue\n",
      ", soft tissue and bone). Some of the sound waves \n",
      "are reﬂected back to the probe, while some travel on further until they reach \n",
      "another boundary and are reﬂected.\n",
      "3. \n",
      "The reﬂected waves are picked up by the probe and relayed to the computer.\n",
      "4. \n",
      "The machine calculates the distance from the probe to the tissue or organ bound-\n",
      "aries using the speed of sound in tissue (1540 m/s) and the time of each echo’\n",
      "s \n",
      "return.\n",
      "5. \n",
      "The system displays the distances and intensities of the echoes on the screen, \n",
      "forming a two-dimensional image\n",
      ".\n",
      "In a typical ultrasound image, millions of pulses and echoes are sent and received \n",
      "each second. The probe can be moved along the surface of the body and angled to \n",
      "obtain various views. Figure 1.20 shows several examples of medical uses of ultra-\n",
      "sound. \n",
      "We continue the discussion on imaging modalities with some examples of elec-\n",
      "tron microscopy. Electron microscopes function as their optical counterparts, except \n",
      "b a\n",
      "d c\n",
      "FIGURE 1.20\n",
      "Examples of \n",
      "ultrasound  \n",
      "imaging. (a) A \n",
      "fetus. (b) Another \n",
      "view of the fetus.  \n",
      "(c) Thyroids.  \n",
      "(d) Muscle layers \n",
      "showing lesion. \n",
      "(Courtesy of \n",
      "Siemens  \n",
      "Medical Systems, \n",
      "Inc., Ultrasound \n",
      "Group.)\n",
      "DIP4E_GLOBAL_Print_Ready.indb   38\n",
      "6/16/2017   2:02:01 PM\n",
      "www.EBooksWorld.ir1.3\n",
      "  \n",
      "Examples of Fields that Use Digital Image Processing\n",
      "    \n",
      "39\n",
      "that they use a focused beam of electrons instead of light to image a specimen. The \n",
      "operation of electron microscopes involves the following basic steps: A stream \n",
      "of electrons is produced by an electron source and accelerated toward the speci-\n",
      "men using a positive electrical potential. This \n",
      "stream is conﬁned and focused using \n",
      "metal apertures and magnetic lenses into a thin, monochromatic beam. \n",
      "This beam is \n",
      "focused onto the sample using a magnetic lens. Interactions occur inside the irradi-\n",
      "ated sample, affecting the electron beam. These interactions and effects are detected \n",
      "and transformed into an image, much in the same way that light is reﬂected from, \n",
      "or absorbed by, objects in a scene. These basic steps are carried out in all electron \n",
      "microscopes.\n",
      "A \n",
      "transmission electron microscope\n",
      " (TEM) works much like a slide projector. A \n",
      "projector transmits a beam of light through a slide; as the light passes through the \n",
      "slide, it is modulated by the contents of the slide. This transmitted beam is then \n",
      "projected onto the viewing screen, forming an enlarged image of the slide. TEMs \n",
      "work in the same way, except that they shine a beam of electrons through a spec-\n",
      "imen (analogous to the slide). The fraction of the beam transmitted through the \n",
      "specimen is projected onto a phosphor screen. The interaction of the electrons with \n",
      "the phosphor produces light and, therefore, a viewable image. A \n",
      "scanning electron \n",
      "microscope\n",
      " (SEM), on the other hand, actually scans the electron beam and records \n",
      "the interaction of beam and sample at each location. This produces one dot on a \n",
      "phosphor screen. A complete image is formed by a raster scan of the beam through \n",
      "the sample, much like a TV camera. The electrons interact with a phosphor screen \n",
      "and produce light. SEMs are suitable for “bulky” samples, while TEMs require very \n",
      "thin samples.\n",
      "Electron microscopes are capable of very high magniﬁcation. While light micros-\n",
      "copy is limited to magniﬁcations on the order of \n",
      "1000\n",
      "×\n",
      ",\n",
      " electron microscopes can \n",
      "achieve magniﬁcation of \n",
      "10 000 ,\n",
      "×\n",
      " or more. Figure 1.21 shows two SEM images of \n",
      "specimen failures due to thermal overload.\n",
      "W\n",
      "e conclude the discussion of imaging modalities by looking brieﬂy at images \n",
      "that are not obtained from physical objects. Instead, they are generated by computer. \n",
      "Fractals are striking examples of computer-generated images. Basically, a fractal is \n",
      "nothing more than an iterative reproduction of a basic pattern according to some \n",
      "mathematical rules. For instance, tiling is one of the simplest ways to generate a frac-\n",
      "tal image. A square can be subdivided into four square subregions, each of which can \n",
      "be further subdivided into four smaller square regions, and so on. Depending on the \n",
      "complexity of the rules for ﬁlling each subsquare, some beautiful tile images can be \n",
      "generated using this method. Of course, the geometry can be arbitrary. For instance, \n",
      "the fractal image could be grown radially out of a center point. Figure 1.22(a) shows \n",
      "a fractal grown in this way. Figure 1.22(b) shows another fractal (a “moonscape”) \n",
      "that provides an interesting analogy to the images of space used as illustrations in \n",
      "some of the preceding sections.\n",
      "A more structured approach to image generation by computer lies in 3-D model-\n",
      "ing. This is an area that provides an important intersection between image process-\n",
      "ing and computer graphics, and is the basis for many 3-D visualization systems (e.g., \n",
      "ﬂight simulators). Figures 1.22(c) and (d) show examples of computer-generated \n",
      "images. Because the original \n",
      "object is created in 3-D, images can be generated in any \n",
      "DIP4E_GLOBAL_Print_Ready.indb   39\n",
      "6/16/2017   2:02:01 PM\n",
      "www.EBooksWorld.ir40\n",
      "    \n",
      "Chapter\n",
      " \n",
      "1\n",
      "  \n",
      "Introduction\n",
      "perspective from plane projections of the 3-D volume. Images of this type can be \n",
      "used for medical training and for a host of other applications, such as criminal foren-\n",
      "sics and special effects.\n",
      "b a\n",
      "FIGURE 1.21\n",
      " (a) \n",
      "250\n",
      "×\n",
      " SEM image of a tungsten ﬁlament following thermal failure (note the \n",
      "shattered pieces on the lower left).\n",
      " (b) \n",
      "2500\n",
      "×\n",
      " SEM image of a damaged integrated circuit. \n",
      "The white ﬁbers are oxides resulting from thermal destruction. (Figure (a) courtesy of Mr. \n",
      "Michael Shaffer, Department of Geological Sciences, University of Oregon, Eugene; (b) cour-\n",
      "tesy of Dr. J. M. Hudak, McMaster University, Hamilton, Ontario, Canada.) \n",
      "b a\n",
      "d c\n",
      "FIGURE 1.22\n",
      "(a) and (b) Fractal \n",
      "images.  \n",
      "(c) and (d) Images \n",
      "generated from \n",
      "3-D computer \n",
      "models of the \n",
      "objects shown. \n",
      "(Figures (a) and \n",
      "(b) courtesy of \n",
      "Ms. Melissa D. \n",
      "Binde,  \n",
      "Swarthmore \n",
      "College; (c) and \n",
      "(d) courtesy of \n",
      "NASA.)\n",
      "DIP4E_GLOBAL_Print_Ready.indb   40\n",
      "6/16/2017   2:02:01 PM\n",
      "www.EBooksWorld.ir1.4\n",
      "  \n",
      "Fundamental Steps in Digital Image Processing\n",
      "    \n",
      "41\n",
      "1.4 FUNDAMENTAL STEPS IN DIGITAL IMAGE PROCESSING  \n",
      "It is helpful to divide the material covered in the following chapters into the two \n",
      "broad categories defined in Section 1.1: methods whose input and output are images, \n",
      "and methods whose inputs may be images, but whose outputs are attributes extract-\n",
      "ed from those images. This organization is summarized in Fig. 1.23. The diagram \n",
      "does not imply that every process is applied to an image. Rather, the intention is to \n",
      "convey an idea of all the methodologies that can be applied to images for different \n",
      "purposes, and possibly with different objectives. The discussion in this section may \n",
      "be viewed as a brief overview of the material in the remainder of the book. \n",
      "Image acquisition\n",
      " is the ﬁrst process in Fig. 1.23. The discussion in Section 1.3 \n",
      "gave some hints regarding the origin of digital images. This topic will be considered \n",
      "in much more detail in Chapter 2, where we also introduce a number of basic digital \n",
      "image concepts that are used throughout the book. Acquisition could be as simple as \n",
      "being given an image that is already in digital form. Generally, the image acquisition \n",
      "stage involves preprocessing, such as scaling.\n",
      "Image enhancement\n",
      " is the process of manipulating an image so the result is more \n",
      "suitable than the original for a speciﬁc application. The word \n",
      "speciﬁc\n",
      " is important \n",
      "here, because it establishes at the outset that enhancement techniques are problem \n",
      "oriented. Thus, for example, a method that is quite useful for enhancing X-ray images \n",
      "may not be the best approach for enhancing satellite images taken in the infrared \n",
      "band of the electromagnetic spectrum.\n",
      "There is no general “theory” of image enhancement. When an image is processed \n",
      "for visual interpretation, the viewer is the ultimate judge of how well a particular \n",
      "1.4\n",
      "Knowledge base\n",
      "CHAPTER 7\n",
      "Wavelets and\n",
      "other image\n",
      "transforms\n",
      "Outputs of these processes generally are images\n",
      "CHAPTER 5\n",
      "Image\n",
      "restoration\n",
      "CHAPTERS 3 & 4\n",
      "Image\n",
      "filtering and \n",
      "enhancement\n",
      "Problem\n",
      "domain\n",
      "Outputs of these processes generally are image attributes\n",
      "CHAPTER 8\n",
      "Compression and\n",
      "watermarking\n",
      "CHAPTER 2\n",
      "Image\n",
      "acquisition\n",
      "CHAPTER 9\n",
      "Morphological\n",
      "processing\n",
      "    CHAPTERS 10 \n",
      "Segmentation\n",
      "  CHAPTER 11\n",
      "Feature \n",
      "extraction\n",
      "CHAPTER 12\n",
      "Image \n",
      "pattern\n",
      "classification\n",
      "Wavelets and\n",
      "multiresolution\n",
      "processing\n",
      "Color Image\n",
      "Processing\n",
      "CHAPTER 6\n",
      "FIGURE 1.23\n",
      "Fundamental \n",
      "steps in digital \n",
      "image processing. \n",
      "The chapter(s) \n",
      "indicated in the \n",
      "boxes is where \n",
      "the material \n",
      "described in the \n",
      "box is discussed.\n",
      "DIP4E_GLOBAL_Print_Ready.indb   41\n",
      "6/16/2017   2:02:01 PM\n",
      "www.EBooksWorld.ir42\n",
      "    \n",
      "Chapter\n",
      " \n",
      "1\n",
      "  \n",
      "Introduction\n",
      "method works. Enhancement techniques are so varied, and use so many different \n",
      "image processing approaches, that it is difﬁcult to assemble a meaningful body of \n",
      "techniques suitable for enhancement in one chapter without extensive background \n",
      "development. For this reason, and also because beginners in the ﬁeld of image pro-\n",
      "cessing generally ﬁnd enhancement applications visually appealing, interesting, and \n",
      "relatively simple to understand, we will use image enhancement as examples when \n",
      "introducing new concepts in parts of Chapter 2 and in Chapters 3 and 4. The mate-\n",
      "rial in the latter two chapters span many of the methods used traditionally for image \n",
      "enhancement. Therefore, using examples from image enhancement to introduce new \n",
      "image processing methods developed in these early chapters not only saves having \n",
      "an extra chapter in the book dealing with image enhancement but, more importantly, \n",
      "is an effective approach for introducing newcomers to the details of processing tech-\n",
      "niques early in the book. However, as you will see in progressing through the rest \n",
      "of the book, the material developed in Chapters 3 and 4 is applicable to a much \n",
      "broader class of problems than just image enhancement.\n",
      "Image restoration\n",
      " is an area that also deals with improving the appearance of \n",
      "an image. However, unlike enhancement, which is subjective, image restoration \n",
      "is objective, in the sense that restoration techniques tend to be based on mathe-\n",
      "matical or probabilistic models of image degradation. Enhancement, on the other \n",
      "hand, is based on human subjective preferences regarding what constitutes a “good” \n",
      "enhancement result.\n",
      "Color image processing\n",
      " is an area that has been gaining in importance because of \n",
      "the signiﬁcant increase in the use of digital images over the internet. Chapter 6 cov-\n",
      "ers a number of fundamental concepts in color models and basic color processing \n",
      "in a digital domain. Color is used also as the basis for extracting features of interest \n",
      "in an image.\n",
      "Wavelets\n",
      " are the foundation for representing images in various degrees of reso-\n",
      "lution. In particular, this material is used in the book for image data compression \n",
      "and for pyramidal representation, in which images are subdivided successively into \n",
      "smaller regions. The material in Chapters 4 and 5 is based mostly on the Fourier \n",
      "transform. In addition to wavelets, we will also discuss in Chapter 7 a number of \n",
      "other transforms that are used routinely in image processing.\n",
      "Compression\n",
      ", as the name implies, deals with techniques for reducing the storage \n",
      "required to save an image, or the bandwidth required to transmit it. Although stor-\n",
      "age technology has improved signiﬁcantly over the past decade, the same cannot be \n",
      "said for transmission capacity. This is true particularly in uses of the internet, which \n",
      "are characterized by signiﬁcant pictorial content. Image compression is familiar \n",
      "(perhaps inadvertently) to most users of computers in the form of image ﬁle exten-\n",
      "sions, such as the jpg ﬁle extension used in the JPEG (Joint Photographic Experts \n",
      "Group) image compression standard.\n",
      "Morphological\n",
      " processing deals with tools for extracting image components that \n",
      "are useful in the representation and description of shape. The material in this chap-\n",
      "ter begins a transition from processes that output images to processes that output \n",
      "image attributes, as indicated in Section 1.1.\n",
      "Segmentation\n",
      " partitions an image into its constituent parts or objects. In gen-\n",
      "eral, autonomous segmentation is one of the most difﬁcult tasks in digital image \n",
      "DIP4E_GLOBAL_Print_Ready.indb   42\n",
      "6/16/2017   2:02:01 PM\n",
      "www.EBooksWorld.ir1.4\n",
      "  \n",
      "Fundamental Steps in Digital Image Processing\n",
      "    \n",
      "43\n",
      "processing. A rugged segmentation procedure brings the process a long way toward \n",
      "successful solution of imaging problems that require objects to be identiﬁed indi-\n",
      "vidually. On the other hand, weak or erratic segmentation algorithms almost always \n",
      "guarantee eventual failure. In general, the more accurate the segmentation, the \n",
      "more likely automated object classiﬁcation is to succeed.\n",
      "Feature extraction\n",
      " almost always follows the output of a segmentation stage, which \n",
      "usually is raw pixel data, constituting either the boundary of a region (i.e., the set \n",
      "of pixels separating one image region from another) or all the points in the region \n",
      "itself. Feature extraction consists of feature detection and feature description. \n",
      "Fea-\n",
      "ture detection\n",
      " refers to ﬁnding the features in an image, region, or boundary. \n",
      "Feature \n",
      "description\n",
      " assigns quantitative attributes to the detected features. For example, we \n",
      "might detect corners in a region, and describe those corners by their orientation \n",
      "and location; both of these descriptors are quantitative attributes. Feature process-\n",
      "ing methods discussed in this chapter are subdivided into three principal categories, \n",
      "depending on whether they are applicable to boundaries, regions, or whole images. \n",
      "Some features are applicable to more than one category. Feature descriptors should \n",
      "be as insensitive as possible to variations in parameters such as scale, translation, \n",
      "rotation, illumination, and viewpoint. \n",
      "Image pattern classiﬁcation\n",
      " is the process that assigns a label (e.g., “vehicle”) to an \n",
      "object based on its feature descriptors. In the last chapter of the book, we will discuss  \n",
      "methods of image pattern classiﬁcation ranging from “classical” approaches such as \n",
      "minimum-distance\n",
      ", \n",
      "correlation\n",
      ", and \n",
      "Bayes classiﬁers\n",
      ", to more modern approaches \n",
      "implemented using \n",
      "deep neural networks\n",
      ". In particular, we will discuss in detail \n",
      "deep\n",
      " \n",
      "convolutional neural networks\n",
      ", which are ideally suited for image processing work.\n",
      "So far, we have said nothing about the need for prior knowledge or about the \n",
      "interaction between the knowledge base and the processing modules in Fig. 1.23. \n",
      "Knowledge\n",
      " about a problem domain is coded into an image processing system in the \n",
      "form of a knowledge database. This knowledge may be as simple as detailing regions \n",
      "of an image where the information of interest is known to be located, thus limiting \n",
      "the search that has to be conducted in seeking that information. The knowledge base \n",
      "can also be quite complex, such as an interrelated list of all major possible defects \n",
      "in a materials inspection problem, or an image database containing high-resolution \n",
      "satellite images of a region in connection with change-detection applications. In \n",
      "addition to guiding the operation of each processing module, the knowledge base \n",
      "also controls the interaction between modules. This distinction is made in Fig. 1.23 \n",
      "by the use of double-headed arrows between the processing modules and the knowl-\n",
      "edge base, as opposed to single-headed arrows linking the processing modules.\n",
      "Although we do not discuss image display explicitly at this point, it is important to \n",
      "keep in mind that viewing the results of image processing can take place at the out-\n",
      "put of any stage in Fig. 1.23. We also note that not all image processing applications \n",
      "require the complexity of interactions implied by Fig. 1.23. In fact, not even all those \n",
      "modules are needed in many cases. For example, image enhancement for human \n",
      "visual interpretation seldom requires use of any of the other stages in Fig. 1.23. In \n",
      "general, however, as the complexity of an image processing task increases, so does \n",
      "the number of processes required to solve the problem.\n",
      "DIP4E_GLOBAL_Print_Ready.indb   43\n",
      "6/16/2017   2:02:02 PM\n",
      "www.EBooksWorld.ir44\n",
      "    \n",
      "Chapter\n",
      " \n",
      "1\n",
      "  \n",
      "Introduction\n",
      "1.5 COMPONENTS OF AN IMAGE PROCESSING SYSTEM  \n",
      "As recently as the mid-1980s, numerous models of image processing systems being \n",
      "sold throughout the world were rather substantial peripheral devices that attached \n",
      "to equally substantial host computers. Late in the 1980s and early in the 1990s, the \n",
      "market shifted to image processing hardware in the form of single boards designed \n",
      "to be compatible with industry standard buses and to ﬁt into engineering work-\n",
      "station cabinets and personal computers. In the late 1990s and early 2000s, a new \n",
      "class of add-on boards, called graphics processing units (GPUs) were introduced for \n",
      "work on 3-D applications, such as games and other 3-D graphics applications. It was \n",
      "not long before GPUs found their way into image processing applications involving \n",
      "large-scale matrix implementations, such as training deep convolutional networks. \n",
      "In addition to lowering costs, the market shift from substantial peripheral devices to \n",
      "add-on processing boards also served as a catalyst for a signiﬁcant number of new \n",
      "companies specializing in the development of software written speciﬁcally for image \n",
      "processing. \n",
      "The trend continues toward miniaturizing and blending of general-purpose small \n",
      "computers with specialized image processing hardware and software. Figure 1.24 \n",
      "shows the basic components comprising a typical general-purpose system used for \n",
      "digital image processing. The function of each component will be discussed in the \n",
      "following paragraphs, starting with image sensing.\n",
      "Two subsystems are required to acquire digital images. The ﬁrst is a physical \n",
      "sen-\n",
      "sor\n",
      " that responds to the energy radiated by the object we wish to image. The second, \n",
      "called a \n",
      "digitizer\n",
      ", is a device for converting the output of the physical sensing device \n",
      "into digital form. For instance, in a digital video camera, the sensors (CCD chips) \n",
      "produce an electrical output proportional to light intensity. The digitizer converts \n",
      "these outputs to digital data. These topics will be covered in Chapter 2.\n",
      "Specialized image processing hardware usually consists of the digitizer just men-\n",
      "tioned, plus hardware that performs other primitive operations, such as an \n",
      "arithme-\n",
      "tic logic unit\n",
      " (ALU), that performs arithmetic and logical operations in parallel on \n",
      "entire images. One example of how an ALU is used is in averaging images as quickly \n",
      "as they are digitized, for the purpose of noise reduction. This type of hardware some-\n",
      "times is called a \n",
      "front-end subsystem\n",
      ", and its most distinguishing characteristic is \n",
      "speed. In other words, this unit performs functions that require fast data through-\n",
      "puts (e.g., digitizing and averaging video images at 30 frames/s) that the typical main \n",
      "computer cannot handle. One or more GPUs (see above) also are common in image \n",
      "processing systems that perform intensive matrix operations.\n",
      "The \n",
      "computer\n",
      " in an image processing system is a general-purpose computer and \n",
      "can range from a PC to a supercomputer. In dedicated applications, sometimes cus-\n",
      "tom computers are used to achieve a required level of performance, but our interest \n",
      "here is on general-purpose image processing systems. In these systems, almost any \n",
      "well-equipped PC-type machine is suitable for off-line image processing tasks.\n",
      "Software\n",
      " for image processing consists of specialized modules that perform \n",
      "speciﬁc tasks. A well-designed package also includes the capability for the user to \n",
      "write code that, as a minimum, utilizes the specialized modules. More sophisticated \n",
      "1.5\n",
      "DIP4E_GLOBAL_Print_Ready.indb   44\n",
      "6/16/2017   2:02:02 PM\n",
      "www.EBooksWorld.ir1.5\n",
      "  \n",
      "Components of an Image Processing System\n",
      "45\n",
      "software packages allow the integration of those modules and general-purpose \n",
      "software commands from at least one computer language. Commercially available \n",
      "image processing software, such as the well-known MATLAB\n",
      "®\n",
      " Image Processing \n",
      "Toolbox, is also common in a well-equipped image processing system. \n",
      "Mass storage\n",
      " is a must in image processing applications. An image of size \n",
      "1024 1024\n",
      "×\n",
      "pixels, in which the intensity of each pixel is an 8-bit quantity,  requires one megabyte  \n",
      "of storage space if the image is not compressed.\n",
      " When dealing with image databases \n",
      "that contain thousands, or even millions, of images, providing adequate storage in \n",
      "an image processing system can be a challenge. Digital storage for image processing \n",
      "applications falls into three principal categories: (1) short-term storage for use dur-\n",
      "ing processing; (2) on-line storage for relatively fast recall; and (3) archival storage, \n",
      "characterized by infrequent access. Storage is measured in bytes (eight bits), Kbytes \n",
      "(10\n",
      "3\n",
      " bytes), Mbytes (\n",
      "10\n",
      "6\n",
      " bytes), Gbytes (\n",
      "10\n",
      "9\n",
      " bytes), and Tbytes (\n",
      "10\n",
      "12\n",
      " bytes).\n",
      "oud\n",
      "ud\n",
      "Cloud\n",
      "Image displays\n",
      "Computer\n",
      "Mass storage\n",
      "Hardcopy\n",
      "Specialized\n",
      "image processing\n",
      "hardware\n",
      "Image sensors\n",
      "Problem\n",
      "domain\n",
      "Image processing\n",
      "software\n",
      "Network\n",
      "Cloud\n",
      "FIGURE 1.24\n",
      "Components of a \n",
      "general-purpose \n",
      "image processing \n",
      "system. \n",
      "DIP4E_GLOBAL_Print_Ready.indb   45\n",
      "6/16/2017   2:02:02 PM\n",
      "www.EBooksWorld.ir46\n",
      "    \n",
      "Chapter\n",
      " \n",
      "1\n",
      "  \n",
      "Introduction\n",
      "One method of providing short-term storage is computer memory. Another is by \n",
      "specialized boards, called \n",
      "frame buffers\n",
      ", that store one or more images and can be \n",
      "accessed rapidly, usually at video rates (e.g., at 30 complete images per second). The \n",
      "latter method allows virtually instantaneous image \n",
      "zoom\n",
      ", as well as \n",
      "scroll\n",
      " (vertical \n",
      "shifts) and \n",
      "pan\n",
      " (horizontal shifts). Frame buffers usually are housed in the special-\n",
      "ized image processing hardware unit in Fig. 1.24. On-line storage generally takes \n",
      "the form of magnetic disks or optical-media storage. The key factor characterizing \n",
      "on-line storage is frequent access to the stored data. Finally, archival storage is char-\n",
      "acterized by massive storage requirements but infrequent need for access. Magnetic \n",
      "tapes and optical disks housed in “jukeboxes” are the usual media for archival appli-\n",
      "cations.\n",
      "Image displays\n",
      " in use today are mainly color, ﬂat screen monitors. Monitors are \n",
      "driven by the outputs of image and graphics display cards that are an integral part of \n",
      "the computer system. Seldom are there requirements for image display applications \n",
      "that cannot be met by display cards and GPUs available commercially as part of the \n",
      "computer system. In some cases, it is necessary to have stereo displays, and these are \n",
      "implemented in the form of headgear containing two small displays embedded in \n",
      "goggles worn by the user.\n",
      "Hardcopy\n",
      " devices for recording images include laser printers, ﬁlm cameras, heat-\n",
      "sensitive devices, ink-jet units, and digital units, such as optical and CD-ROM disks. \n",
      "Film provides the highest possible resolution, but paper is the obvious medium of \n",
      "choice for written material. For presentations, images are displayed on ﬁlm trans-\n",
      "parencies or in a digital medium if image projection equipment is used. The latter \n",
      "approach is gaining acceptance as the standard for image presentations.\n",
      "Networking\n",
      " and \n",
      "cloud\n",
      " communication are almost default functions in any com-\n",
      "puter system in use today. Because of the large amount of data inherent in image \n",
      "processing applications, the key consideration in image transmission is \n",
      "bandwidth\n",
      ". In \n",
      "dedicated networks, this typically is not a problem, but communications with remote \n",
      "sites via the internet are not always as efﬁcient. Fortunately, transmission bandwidth \n",
      "is improving quickly as a result of optical ﬁber and other broadband technologies. \n",
      "Image data compression continues to play a major role in the transmission of large \n",
      "amounts of image data.\n",
      "Summary, References, and Further Reading\n",
      "  \n",
      "The main purpose of the material presented in this chapter is to provide a sense of perspective about the origins \n",
      "of digital image processing and, more important, about current and future areas of application of this technology. \n",
      "Although the coverage of these topics in this chapter was necessarily incomplete due to space limitations, it should \n",
      "have left you with a clear impression of the breadth and practical scope of digital image processing. As we proceed \n",
      "in the following chapters with the development of image processing theory and applications, numerous examples \n",
      "are provided to keep a clear focus on the utility and promise of these techniques. Upon concluding the study of the \n",
      "ﬁnal chapter, a reader of this book will have arrived at a level of understanding that is the foundation for most of \n",
      "the work currently underway in this ﬁeld. \n",
      "In past editions, we have provided a long list of journals and books to give readers an idea of the breadth of the \n",
      "image processing literature, and where this literature is reported. The list has been updated, and it has become so \n",
      "extensive that it is more practical to include it in the book website: \n",
      "www.ImageProcessingPlace.com\n",
      ", in the section \n",
      "entitled \n",
      "Publications\n",
      ".\n",
      "DIP4E_GLOBAL_Print_Ready.indb   46\n",
      "6/16/2017   2:02:02 PM\n",
      "www.EBooksWorld.ir472\n",
      "Digital Image Fundamentals\n",
      "Preview\n",
      "This chapter is an introduction to a number of basic concepts in digital image processing that are used \n",
      "throughout the book. Section 2.1 summarizes some important aspects of the human visual system, includ-\n",
      "ing image formation in the eye and its capabilities for brightness adaptation and discrimination. Section \n",
      "2.2 discusses light, other components of the electromagnetic spectrum, and their imaging characteristics. \n",
      "Section 2.3 discusses imaging sensors and how they are used to generate digital images. Section 2.4 intro-\n",
      "duces the concepts of uniform image sampling and intensity quantization. Additional topics discussed \n",
      "in that section include digital image representation, the effects of varying the number of samples and \n",
      "intensity levels in an image, the concepts of spatial and intensity resolution, and the principles of image \n",
      "interpolation. Section 2.5 deals with a variety of basic relationships between pixels. Finally, Section 2.6 \n",
      "is an introduction to the principal mathematical tools we use throughout the book. A second objective \n",
      "of that section is to help you begin developing a “feel” for how these tools are used in a variety of basic \n",
      "image processing tasks. \n",
      "Upon completion of this chapter, readers should:\n",
      " Have an understanding of some important \n",
      "functions and limitations of human vision.\n",
      " Be familiar with the electromagnetic energy \n",
      "spectrum, including basic properties of light.\n",
      " Know how digital images are generated and \n",
      "represented.\n",
      " Understand the basics of image sampling and \n",
      "quantization.\n",
      " Be familiar with spatial and intensity resolu-\n",
      "tion and their effects on image appearance.\n",
      " Have an understanding of basic geometric \n",
      "relationships between image pixels.\n",
      " Be familiar with the principal mathematical \n",
      "tools used in digital image processing.\n",
      " Be able to apply a variety of introductory dig-\n",
      "ital image processing techniques.\n",
      "Those who wish to succeed must ask the right preliminary \n",
      "questions.\n",
      "Aristotle\n",
      "DIP4E_GLOBAL_Print_Ready.indb   47\n",
      "6/16/2017   2:02:02 PM\n",
      "www.EBooksWorld.ir48\n",
      "Chapter\n",
      " \n",
      "2\n",
      "  \n",
      "Digital Image Fundamentals\n",
      "2.1 ELEMENTS OF VISUAL PERCEPTION  \n",
      "Although the field of digital image processing is built on a foundation of mathemat-\n",
      "ics, human intuition and analysis often play a role in the choice of one technique \n",
      "versus another, and this choice often is made based on subjective, visual judgments. \n",
      "Thus, developing an understanding of basic characteristics of human visual percep-\n",
      "tion as a first step in our journey through this book is appropriate. In particular, our \n",
      "interest is in the elementary mechanics of how images are formed and perceived \n",
      "by humans. We are interested in learning the physical limitations of human vision \n",
      "in terms of factors that also are used in our work with digital images. Factors such \n",
      "as how human and electronic imaging devices compare in terms of resolution and \n",
      "ability to adapt to changes in illumination are not only interesting, they are also \n",
      "important from a practical point of view.\n",
      "STRUCTURE OF THE HUMAN EYE\n",
      "Figure 2.1 shows a simplified cross section of the human eye. The eye is nearly a \n",
      "sphere (with a diameter of about 20 mm) enclosed by three membranes: the \n",
      "cornea\n",
      " \n",
      "and \n",
      "sclera\n",
      " outer cover; the \n",
      "choroid\n",
      "; and the \n",
      "retina\n",
      ". The cornea is a tough, transparent \n",
      "tissue that covers the anterior surface of the eye. Continuous with the cornea, the \n",
      "sclera is an opaque membrane that encloses the remainder of the optic globe.\n",
      "The choroid lies directly below the sclera. This membrane contains a network of \n",
      "blood vessels that serve as the major source of nutrition to the eye. Even superﬁcial \n",
      "2.1\n",
      "Retina\n",
      "Blind spot\n",
      "Sclera\n",
      "Choroid\n",
      "Nerve & sheath\n",
      "Fovea\n",
      "Vitreous humor\n",
      "Visual axis\n",
      "Ciliary fibers\n",
      "Ciliary muscle\n",
      "Iris\n",
      "Cornea\n",
      "Lens\n",
      "Anterior chamber\n",
      "Ciliary body\n",
      "FIGURE 2.1\n",
      "Simpliﬁed  \n",
      "diagram of a  \n",
      "cross section of \n",
      "the human eye.\n",
      "DIP4E_GLOBAL_Print_Ready.indb   48\n",
      "6/16/2017   2:02:02 PM\n",
      "www.EBooksWorld.ir2.1\n",
      "  \n",
      "Elements of Visual Perception\n",
      "    \n",
      "49\n",
      "injury to the choroid can lead to severe eye damage as a result of inﬂammation that \n",
      "restricts blood ﬂow. The choroid coat is heavily pigmented, which helps reduce the \n",
      "amount of extraneous light entering the eye and the backscatter within the optic \n",
      "globe. At its anterior extreme, the choroid is divided into the \n",
      "ciliary body\n",
      " and the \n",
      "iris\n",
      ". The latter contracts or expands to control the amount of light that enters the eye. \n",
      "The central opening of the iris (the \n",
      "pupil\n",
      ") varies in diameter from approximately 2 \n",
      "to 8 mm. The front of the iris contains the visible pigment of the eye, whereas the \n",
      "back contains a black pigment.\n",
      "The \n",
      "lens\n",
      " consists of concentric layers of ﬁbrous cells and is suspended by ﬁbers \n",
      "that attach to the ciliary body. It is composed of 60% to 70% water, about 6% fat, \n",
      "and more protein than any other tissue in the eye. The lens is colored by a slightly \n",
      "yellow pigmentation that increases with age. In extreme cases, excessive clouding of \n",
      "the lens, referred to as \n",
      "cataracts\n",
      ", can lead to poor color discrimination and loss of \n",
      "clear vision. The lens absorbs approximately 8% of the visible light spectrum, with \n",
      "higher absorption at shorter wavelengths. Both infrared and ultraviolet light are \n",
      "absorbed by proteins within the lens and, in excessive amounts, can damage the eye.\n",
      "The innermost membrane of the eye is the \n",
      "retina\n",
      ", which lines the inside of the \n",
      "wall’s entire posterior portion. When the eye is focused, light from an object is \n",
      "imaged on the retina. Pattern vision is afforded by discrete light receptors distrib-\n",
      "uted over the surface of the retina. There are two types of receptors: \n",
      "cones\n",
      " and \n",
      "rods\n",
      ". \n",
      "There are between 6 and 7 million cones in each eye. They are located primarily in \n",
      "the central portion of the retina, called the \n",
      "fovea\n",
      ", and are highly sensitive to color. \n",
      "Humans can resolve ﬁne details because each cone is connected to its own nerve end. \n",
      "Muscles rotate the eye until the image of a region of interest falls on the fovea. Cone \n",
      "vision is called \n",
      "photopic\n",
      " or \n",
      "bright-light\n",
      " vision.\n",
      "The number of rods is much larger: Some 75 to 150 million are distributed over \n",
      "the retina. The larger area of distribution, and the fact that several rods are connect-\n",
      "ed to a single nerve ending, reduces the amount of detail discernible by these recep-\n",
      "tors. Rods capture an overall image of the ﬁeld of view. They are not involved in \n",
      "color vision, and are sensitive to low levels of illumination. For example, objects that \n",
      "appear brightly colored in daylight appear as colorless forms in moonlight because \n",
      "only the rods are stimulated. This phenomenon is known as \n",
      "scotopic\n",
      " or \n",
      "dim-light\n",
      " \n",
      "vision.\n",
      "Figure 2.2\n",
      " \n",
      "shows the density of rods and cones for a cross section of the right eye, \n",
      "passing through the region where the optic nerve emerges from the eye\n",
      ". The absence \n",
      "of receptors in this area causes the so-called \n",
      "blind spot\n",
      " (see Fig. 2.1). Except for this \n",
      "region, the distribution of receptors is radially symmetric about the fovea. Receptor \n",
      "density is measured in degrees from the visual axis. Note in Fig. 2.2 that cones are \n",
      "most dense in the center area of the fovea, and that rods increase in density from \n",
      "the center out to approximately 20° off axis. Then, their density decreases out to the \n",
      "periphery of the retina.\n",
      "The fovea itself is a circular indentation in the retina of about 1.5 mm in diameter, \n",
      "so it has an area of approximately 1.77 \n",
      "mm\n",
      "2\n",
      ".\n",
      " As Fig. 2.2 shows, the density of cones \n",
      "in that area of the retina is on the order of 150,000 elements per \n",
      "mm\n",
      "2\n",
      ". Based on \n",
      "these ﬁgures, the number of cones in the fovea, which is the region of highest acuity \n",
      "DIP4E_GLOBAL_Print_Ready.indb   49\n",
      "6/16/2017   2:02:03 PM\n",
      "www.EBooksWorld.ir50\n",
      "    \n",
      "Chapter\n",
      " \n",
      "2\n",
      "  \n",
      "Digital Image Fundamentals\n",
      "in the eye, is about 265,000 elements. Modern electronic imaging chips exceed this \n",
      "number by a large factor. While the ability of humans to integrate intelligence and \n",
      "experience with vision makes purely quantitative comparisons somewhat superﬁcial, \n",
      "keep in mind for future discussions that electronic imaging sensors can easily exceed \n",
      "the capability of the eye in resolving image detail.\n",
      "IMAGE FORMATION IN THE EYE\n",
      "In an ordinary photographic camera, the lens has a fixed focal length. Focusing at \n",
      "various distances is achieved by varying the distance between the lens and the imag-\n",
      "ing plane, where the film (or imaging chip in the case of a digital camera) is located. \n",
      "In the human eye, the converse is true; the distance between the center of the lens \n",
      "and the imaging sensor (the retina) is fixed, and the focal length needed to achieve \n",
      "proper focus is obtained by varying the shape of the lens. The fibers in the ciliary \n",
      "body accomplish this by flattening or thickening the lens for distant or near ob-\n",
      "jects, respectively. The distance between the center of the lens and the retina along \n",
      "the visual axis is approximately 17 mm. The range of focal lengths is approximately \n",
      "14 mm to 17 mm, the latter taking place when the eye is relaxed and focused at dis-\n",
      "tances greater than about 3 m. The geometry in Fig. 2.3 illustrates how to obtain the \n",
      "dimensions of an image formed on the retina. For example, suppose that a person \n",
      "is looking at a tree 15 m high at a distance of 100 m. Letting \n",
      "h\n",
      " denote the height \n",
      "of that object in the retinal image, the geometry of Fig. 2.3 yields \n",
      "15 100 17\n",
      "=\n",
      "h\n",
      " or \n",
      "h\n",
      "=\n",
      "25\n",
      ".\n",
      " mm.\n",
      " As indicated earlier in this section, the retinal image is focused primar-\n",
      "ily on the region of the fovea.\n",
      " Perception then takes place by the relative excitation \n",
      "of light receptors, which transform radiant energy into electrical impulses that ulti-\n",
      "mately are decoded by the brain.\n",
      "BRIGHTNESS ADAPTATION AND DISCRIMINATION\n",
      "Because digital images are displayed as sets of discrete intensities, the eye’s abil-\n",
      "ity to discriminate between different intensity levels is an important consideration \n",
      "FIGURE 2.2\n",
      "Distribution of \n",
      "rods and cones in \n",
      "the retina.\n",
      "Blind spot\n",
      "Cones\n",
      "Rods\n",
      "No. of rods or cones per mm\n",
      "2\n",
      "Degrees from visual axis (center of fovea)\n",
      "180,000\n",
      "135,000\n",
      "90,000\n",
      "45,000\n",
      "80\n",
      "/H11034\n",
      "60\n",
      "/H11034\n",
      "40\n",
      "/H11034\n",
      "20\n",
      "/H11034\n",
      "0\n",
      "/H11034\n",
      "20\n",
      "/H11034\n",
      "40\n",
      "/H11034\n",
      "60\n",
      "/H11034\n",
      "80\n",
      "/H11034\n",
      "DIP4E_GLOBAL_Print_Ready.indb   50\n",
      "6/16/2017   2:02:03 PM\n",
      "www.EBooksWorld.ir2.1\n",
      "  \n",
      "Elements of Visual Perception\n",
      "51\n",
      "in presenting image processing results. The range of light intensity levels to which \n",
      "the human visual system can adapt is enormous—on the order of \n",
      "10\n",
      "10\n",
      "— from the \n",
      "scotopic threshold to the glare limit. Experimental evidence indicates that \n",
      "subjec-\n",
      "tive brightness\n",
      " (intensity as perceived by the human visual system) is a logarithmic \n",
      "function of the light intensity incident on the eye. Figure 2.4, a plot of light inten-\n",
      "sity versus subjective brightness, illustrates this characteristic. The long solid curve \n",
      "represents the range of intensities to which the visual system can adapt. In photopic \n",
      "vision alone, the range is about \n",
      "10\n",
      "6\n",
      ". The transition from scotopic to photopic vision \n",
      "is gradual over the approximate range from 0.001 to 0.1 millilambert (\n",
      "−\n",
      "3\n",
      " to \n",
      "−\n",
      "1\n",
      " mL \n",
      "in the log scale),\n",
      " as the double branches of the adaptation curve in this range show.\n",
      "The key point in interpreting the impressive dynamic range depicted in Fig. 2.4 \n",
      "is that the visual system cannot operate over such a range \n",
      "simultaneously\n",
      ". Rather, it \n",
      "accomplishes this large variation by changing its overall sensitivity, a phenomenon \n",
      "known as \n",
      "brightness adaptation\n",
      ". The total range of distinct intensity levels the eye \n",
      "can discriminate simultaneously is rather small when compared with the total adap-\n",
      "tation range. For a given set of conditions, the current sensitivity level of the visual \n",
      "system is called the \n",
      "brightness adaptation level\n",
      ", which may correspond, for example, \n",
      "FIGURE 2.3\n",
      "Graphical  \n",
      "representation of \n",
      "the eye looking at \n",
      "a palm tree. Point \n",
      "C \n",
      "is the focal  \n",
      "center of the lens.\n",
      "15 m\n",
      "C\n",
      "17 mm\n",
      "100 m\n",
      "FIGURE 2.4\n",
      "Range of subjec-\n",
      "tive brightness \n",
      "sensations  \n",
      "showing a  \n",
      "particular  \n",
      "adaptation level, \n",
      "B\n",
      "a\n",
      ".\n",
      "Glare limit\n",
      "Subjective brightness\n",
      "Adaptation range\n",
      "Scotopic\n",
      "threshold\n",
      "Log of intensity (mL)\n",
      "Scotopic\n",
      "Photopic\n",
      "/H11002\n",
      "6\n",
      "/H11002\n",
      "4\n",
      "/H11002\n",
      "20 24\n",
      "B\n",
      "a\n",
      "B\n",
      "b\n",
      "DIP4E_GLOBAL_Print_Ready.indb   51\n",
      "6/16/2017   2:02:03 PM\n",
      "www.EBooksWorld.ir52\n",
      "    \n",
      "Chapter\n",
      " \n",
      "2\n",
      "  \n",
      "Digital Image Fundamentals\n",
      "to brightness \n",
      "B\n",
      "a\n",
      " in Fig. 2.4. The short intersecting curve represents the range of sub-\n",
      "jective brightness that the eye can perceive when adapted to \n",
      "this\n",
      " level. This range is \n",
      "rather restricted, having a level \n",
      "B\n",
      "b\n",
      " at, and below which, all stimuli are perceived as \n",
      "indistinguishable blacks. The upper portion of the curve is not actually restricted but, \n",
      "if extended too far, loses its meaning because much higher intensities would simply \n",
      "raise the adaptation level higher than \n",
      "B\n",
      "a\n",
      ".\n",
      "The ability of the eye to discriminate between \n",
      "c\n",
      "hanges\n",
      " in light intensity at any \n",
      "speciﬁc adaptation level is of considerable interest. A classic experiment used to \n",
      "determine the capability of the human visual system for brightness discrimination \n",
      "consists of having a subject look at a ﬂat, uniformly illuminated area large enough to \n",
      "occupy the entire ﬁeld of view. This area typically is a diffuser, such as opaque glass, \n",
      "illuminated from behind by a light source, \n",
      "I\n",
      ", with variable intensity. To this ﬁeld is \n",
      "added an increment of illumination, \n",
      "/H9004\n",
      "I\n",
      ", in the form of a short-duration ﬂash that \n",
      "appears as a circle in the center of the uniformly illuminated ﬁeld,\n",
      " as Fig. 2.5 shows.\n",
      "If \n",
      "/H9004\n",
      "I\n",
      " is not bright enough, the subject says “no,” indicating no perceivable change. \n",
      "As \n",
      "/H9004\n",
      "I\n",
      " gets stronger, the subject may give a positive response of “ye\n",
      "s,” \n",
      "indicating a \n",
      "perceived change\n",
      ". Finally, when \n",
      "/H9004\n",
      "I\n",
      " is strong enough, the subject will give a response \n",
      "of \n",
      "“yes” all the time. The quantity \n",
      "/H9004\n",
      "II\n",
      "c\n",
      ", where \n",
      "/H9004\n",
      "I\n",
      "c\n",
      " is the increment of illumination \n",
      "discriminable 50% of the time with background illumination \n",
      "I\n",
      ", is called the \n",
      "Weber \n",
      "ratio\n",
      ". A small value of \n",
      "/H9004\n",
      "II\n",
      "c\n",
      " means that a small percentage change in intensity is \n",
      "discriminable. This represents “good” brightness discrimination. Conversely, a large \n",
      "value of \n",
      "/H9004\n",
      "II\n",
      "c\n",
      " means that a large percentage change in intensity is required for the \n",
      "eye to detect the change. This represents “poor” brightness discrimination.\n",
      "A plot of \n",
      "/H9004\n",
      "II\n",
      "c\n",
      " as a function of \n",
      "log\n",
      "I\n",
      " has the characteristic shape shown in Fig. 2.6. \n",
      "T\n",
      "his curve shows that brightness discrimination is poor (the Weber ratio is large) at \n",
      "low levels of illumination, and it improves signiﬁcantly (the Weber ratio decreases) \n",
      "as background illumination increases. The two branches in the curve reﬂect the fact \n",
      "that at low levels of illumination vision is carried out by the rods, whereas, at high \n",
      "levels, vision is a function of cones.\n",
      "If the background illumination is held constant and the intensity of the other \n",
      "source, instead of ﬂashing, is now allowed to vary incrementally from never being \n",
      "perceived to always being perceived, the typical observer can discern a total of one \n",
      "to two dozen different intensity changes. Roughly, this result is related to the num-\n",
      "ber of different intensities a person can see at any one\n",
      " point \n",
      "or\n",
      " small area\n",
      " in a mono-\n",
      "chrome image. This does not mean that an image can be represented by \n",
      "such a \n",
      "small \n",
      "number of intensity values because, as the eye roams about the image, the average \n",
      "FIGURE 2.5  \n",
      "Basic\n",
      "experimental  \n",
      "setup used to \n",
      "characterize \n",
      "brightness  \n",
      "discrimination.\n",
      "I\n",
      "I\n",
      " \n",
      "/H9004\n",
      "I\n",
      "+\n",
      "DIP4E_GLOBAL_Print_Ready.indb   52\n",
      "6/16/2017   2:02:04 PM\n",
      "www.EBooksWorld.ir2.1\n",
      "  \n",
      "Elements of Visual Perception\n",
      "    \n",
      "53\n",
      "background changes, thus allowing a \n",
      "different\n",
      " set of incremental changes to be detect-\n",
      "ed at each new adaptation level. The net result is that the eye is capable of a broader \n",
      "range of \n",
      "overall\n",
      " intensity discrimination. In fact, as we will show in Section 2.4, the eye \n",
      "is capable of detecting objectionable effects in monochrome images whose overall \n",
      "intensity is represented by fewer than approximately two dozen levels.\n",
      "Two phenomena demonstrate that perceived brightness is not a simple function \n",
      "of intensity. The ﬁrst is based on the fact that the visual system tends to undershoot \n",
      "or overshoot around the boundary of regions of different intensities. Figure 2.7(a) \n",
      "shows a striking example of this phenomenon. Although the intensity of the stripes \n",
      "FIGURE 2.6\n",
      "A typical plot of \n",
      "the Weber ratio \n",
      "as a function of \n",
      "intensity.\n",
      "/H11002\n",
      "1.5\n",
      "/H11002\n",
      "2.0\n",
      "/H11002\n",
      "4\n",
      "/H11002\n",
      "3\n",
      "/H11002\n",
      "2\n",
      "/H11002\n",
      "10\n",
      "log \n",
      "I\n",
      "log \n",
      "/H9004\n",
      "I\n",
      "c\n",
      "/\n",
      "I\n",
      "1234\n",
      "/H11002\n",
      "1.0\n",
      "/H11002\n",
      "0.5\n",
      "0.5\n",
      "1.0\n",
      "0\n",
      "Actual intensity\n",
      "Perceived intensity\n",
      "FIGURE 2.7\n",
      "Illustration of the \n",
      "Mach band effect. \n",
      "Perceived  \n",
      "intensity is not a \n",
      "simple function of \n",
      "actual intensity.\n",
      "b\n",
      "a\n",
      "c\n",
      "DIP4E_GLOBAL_Print_Ready.indb   53\n",
      "6/16/2017   2:02:05 PM\n",
      "www.EBooksWorld.ir54\n",
      "    \n",
      "Chapter\n",
      " \n",
      "2\n",
      "  \n",
      "Digital Image Fundamentals\n",
      "is constant [see Fig. 2.7(b)], we actually perceive a brightness pattern that is strongly \n",
      "scalloped near the boundaries, as Fig. 2.7(c) shows. These perceived scalloped bands \n",
      "are called \n",
      "Mach bands\n",
      " after Ernst Mach, who ﬁrst described the phenomenon in 1865.\n",
      "The second phenomenon, called \n",
      "simultaneous contrast\n",
      ", is that a region’s per-\n",
      "ceived brightness does not depend only on its intensity, as Fig. 2.8 demonstrates. All \n",
      "the center squares have exactly the same intensity, but each appears to the eye to \n",
      "become darker as the background gets lighter. A more familiar example is a piece of \n",
      "paper that looks white when lying on a desk, but can appear totally black when used \n",
      "to shield the eyes while looking directly at a bright sky.\n",
      "Other examples of human perception phenomena are \n",
      "optical illusions\n",
      ", in which \n",
      "the eye ﬁlls in nonexisting details or wrongly perceives geometrical properties of \n",
      "objects. Figure 2.9 shows some examples. In Fig. 2.9(a), the outline of a square is \n",
      "seen clearly, despite the fact that no lines deﬁning such a ﬁgure are part of the image. \n",
      "The same effect, this time with a circle, can be seen in Fig. 2.9(b); note how just a few \n",
      "lines are sufﬁcient to give the illusion of a complete circle. The two horizontal line \n",
      "segments in Fig. 2.9(c) are of the same length, but one appears shorter than the other. \n",
      "Finally, all long lines in Fig. 2.9(d) are equidistant and parallel. Yet, the crosshatching \n",
      "creates the illusion that those lines are far from being parallel.\n",
      "2.2 LIGHT AND THE ELECTROMAGNETIC SPECTRUM  \n",
      "The electromagnetic spectrum was introduced in Section 1.3. We now consider this \n",
      "topic in more detail. In 1666, Sir Isaac Newton discovered that when a beam of \n",
      "sunlight passes through a glass prism, the emerging beam of light is not white but \n",
      "consists instead of a continuous spectrum of colors ranging from violet at one end \n",
      "to red at the other. As Fig. 2.10 shows, the range of colors we perceive in visible light \n",
      "is a small portion of the electromagnetic spectrum. On one end of the spectrum are \n",
      "radio waves with wavelengths billions of times longer than those of visible light. On \n",
      "the other end of the spectrum are gamma rays with wavelengths millions of times \n",
      "smaller than those of visible light. We showed examples in Section 1.3 of images in \n",
      "most of the bands in the EM spectrum.\n",
      "2.2\n",
      "b a\n",
      "c\n",
      "FIGURE 2.8\n",
      " Examples of simultaneous contrast. All the inner squares have the same intensity, \n",
      "but they appear progressively darker as the background becomes lighter.\n",
      "DIP4E_GLOBAL_Print_Ready.indb   54\n",
      "6/16/2017   2:02:05 PM\n",
      "www.EBooksWorld.ir2.2\n",
      "  \n",
      "Light and the Electromagnetic Spectrum\n",
      "    \n",
      "55\n",
      "The electromagnetic spectrum can be expressed in terms of wavelength, frequency, \n",
      "or energy. Wavelength (\n",
      "l\n",
      ") and frequency (\n",
      "n\n",
      ") are related by the expression \n",
      " \n",
      "l\n",
      "n\n",
      "=\n",
      "c\n",
      " \n",
      "(2-1)\n",
      "where \n",
      "c\n",
      " is the speed of light (\n",
      "2\n",
      "998 10\n",
      "8\n",
      ".\n",
      "*\n",
      " \n",
      "m/s). Figure 2.11 shows a schematic repre-\n",
      "sentation of one wavelength.\n",
      " \n",
      "The energy of the various components of the electromagnetic spectrum is given \n",
      "by the expression\n",
      " \n",
      "Eh\n",
      "=\n",
      "n\n",
      " \n",
      "(2-2)\n",
      "where \n",
      "h\n",
      " is Planck’\n",
      "s constant. The units of wavelength are meters, with the terms \n",
      "microns\n",
      " (denoted \n",
      "m\n",
      "m\n",
      " and equal to \n",
      "10\n",
      "6\n",
      "−\n",
      " m) and \n",
      "nanometers\n",
      " (denoted nm and equal \n",
      "to 10\n",
      "9\n",
      "−\n",
      " m) being used just as frequently. Frequency is measured in \n",
      "Hertz\n",
      " (Hz), with \n",
      "one Hz being equal to one cycle of a sinusoidal wave per second. A commonly used \n",
      "unit of energy is the \n",
      "electron-volt\n",
      ".\n",
      "Electromagnetic waves can be visualized as propagating sinusoidal waves with \n",
      "wavelength \n",
      "l\n",
      " (Fig. 2.11), or they can be thought of as a stream of massless particles, \n",
      "b a\n",
      "d c\n",
      "FIGURE 2.9  \n",
      "Some \n",
      "well-known  \n",
      "optical illusions.\n",
      "DIP4E_GLOBAL_Print_Ready.indb   55\n",
      "6/16/2017   2:02:06 PM\n",
      "www.EBooksWorld.ir56\n",
      "Chapter\n",
      " \n",
      "2\n",
      "  \n",
      "Digital Image Fundamentals\n",
      "each traveling in a wavelike pattern and moving at the speed of light. Each mass-\n",
      "less particle contains a certain amount (or bundle) of energy, called a \n",
      "photon\n",
      ". We \n",
      "see from Eq. (2-2) that energy is proportional to frequency, so the higher-frequency \n",
      "(shorter wavelength) electromagnetic phenomena carry more energy per photon. \n",
      "Thus, radio waves have photons with low energies, microwaves have more energy \n",
      "than radio waves, infrared still more, then visible, ultraviolet, X-rays, and ﬁnally \n",
      "gamma rays, the most energetic of all. High-energy electromagnetic radiation, espe-\n",
      "cially in the X-ray and gamma ray bands, is particularly harmful to living organisms. \n",
      "Light is a type of electromagnetic radiation that can be sensed by the eye. The \n",
      "visible (color) spectrum is shown expanded in Fig. 2.10 for the purpose of discussion \n",
      "(we will discuss color in detail in Chapter 6). The visible band of the electromag-\n",
      "netic spectrum spans the range from approximately 0.43 \n",
      "m\n",
      "m\n",
      " (violet) to about 0.79 \n",
      "m\n",
      "m\n",
      " (red). For convenience, the color spectrum is divided into six broad regions: \n",
      "violet,\n",
      " blue, green, yellow, orange, and red. No color (or other component of the \n",
      "Radio waves\n",
      "Microwaves\n",
      "Infrared\n",
      "Visible spectrum\n",
      "Ultraviolet\n",
      "Gamma rays X-rays\n",
      "0.4 \n",
      "/H11003\n",
      " 10\n",
      "/H11002\n",
      "6\n",
      "0.5 \n",
      "/H11003\n",
      " 10\n",
      "/H11002\n",
      "6\n",
      "0.6 \n",
      "/H11003\n",
      " 10\n",
      "/H11002\n",
      "6\n",
      "0.7 \n",
      "/H11003\n",
      " 10\n",
      "/H11002\n",
      "6\n",
      "Infrared\n",
      "Ultraviolet Violet Blue Green Yellow Red\n",
      "Orange\n",
      "10\n",
      "5\n",
      "10\n",
      "6\n",
      "10\n",
      "7\n",
      "10\n",
      "8\n",
      "10\n",
      "9\n",
      "10\n",
      "10\n",
      "10\n",
      "11\n",
      "10\n",
      "12\n",
      "10\n",
      "13\n",
      "10\n",
      "14\n",
      "10\n",
      "15\n",
      "10\n",
      "16\n",
      "10\n",
      "17\n",
      "10\n",
      "18\n",
      "10\n",
      "19\n",
      "10\n",
      "20\n",
      "10\n",
      "21\n",
      "Frequency (Hz)\n",
      "10\n",
      "/H11002\n",
      "9\n",
      "10\n",
      "/H11002\n",
      "8\n",
      "10\n",
      "/H11002\n",
      "7\n",
      "10\n",
      "/H11002\n",
      "6\n",
      "10\n",
      "/H11002\n",
      "5\n",
      "10\n",
      "/H11002\n",
      "4\n",
      "10\n",
      "/H11002\n",
      "3\n",
      "10\n",
      "/H11002\n",
      "2\n",
      "10\n",
      "/H11002\n",
      "1\n",
      "1\n",
      "10\n",
      "1\n",
      "10\n",
      "2\n",
      "10\n",
      "3\n",
      "10\n",
      "4\n",
      "10\n",
      "5\n",
      "10\n",
      "6\n",
      "Energy of one photon (electron volts)\n",
      "10\n",
      "3\n",
      "10\n",
      "2\n",
      "10\n",
      "1\n",
      "1\n",
      "10\n",
      "/H11002\n",
      "1\n",
      "10\n",
      "/H11002\n",
      "2\n",
      "10\n",
      "/H11002\n",
      "3\n",
      "10\n",
      "/H11002\n",
      "4\n",
      "10\n",
      "/H11002\n",
      "5\n",
      "10\n",
      "/H11002\n",
      "6\n",
      "10\n",
      "/H11002\n",
      "7\n",
      "10\n",
      "/H11002\n",
      "8\n",
      "10\n",
      "/H11002\n",
      "9\n",
      "10\n",
      "/H11002\n",
      "10\n",
      "10\n",
      "/H11002\n",
      "11\n",
      "10\n",
      "/H11002\n",
      "12\n",
      "Wavelength (meters)\n",
      "FIGURE 2.10\n",
      "  The electromagnetic spectrum. The visible spectrum is shown zoomed to facilitate explanations, but note \n",
      "that it encompasses a very narrow range of the total EM spectrum.\n",
      "l\n",
      "FIGURE 2.11\n",
      "Graphical  \n",
      "representation of \n",
      "one wavelength.\n",
      "DIP4E_GLOBAL_Print_Ready.indb   56\n",
      "6/16/2017   2:02:06 PM\n",
      "www.EBooksWorld.ir2.3\n",
      "  \n",
      "Image Sensing and Acquisition\n",
      "    \n",
      "57\n",
      "electromagnetic spectrum) ends abruptly; rather, each range blends smoothly into \n",
      "the next, as Fig. 2.10 shows.\n",
      "The colors perceived in an object are determined by the nature of the light \n",
      "reﬂect-\n",
      "ed\n",
      " by the object. A body that reﬂects light relatively balanced in all visible wave-\n",
      "lengths appears white to the observer. However, a body that favors reﬂectance in \n",
      "a limited range of the visible spectrum exhibits some shades of color. For example, \n",
      "green objects reﬂect light with wavelengths primarily in the 500 to 570 nm range, \n",
      "while absorbing most of the energy at other wavelengths.\n",
      "Light that is void of color is called \n",
      "monochromatic\n",
      " (or \n",
      "achromatic\n",
      ") light. The \n",
      "only attribute of monochromatic light is its intensity. Because the intensity of mono-\n",
      "chromatic light is perceived to vary from black to grays and ﬁnally to white, the \n",
      "term \n",
      "gray level\n",
      " is used commonly to denote monochromatic intensity (we use the \n",
      "terms \n",
      "intensity\n",
      " and \n",
      "gray level\n",
      " interchangeably in subsequent discussions). The range \n",
      "of values of monochromatic light from black to white is usually called the \n",
      "gray scale\n",
      ", \n",
      "and monochromatic images are frequently referred to as \n",
      "grayscale images\n",
      ".\n",
      "Chromatic\n",
      " (color) light spans the electromagnetic energy spectrum from approxi-\n",
      "mately 0.43 to 0.79 \n",
      "m\n",
      "m,\n",
      " as noted previously. In addition to frequency, three other \n",
      "quantities are used to describe a chromatic light source:\n",
      " radiance, luminance, and \n",
      "brightness. \n",
      "Radiance\n",
      " is the total amount of energy that ﬂows from the light source, \n",
      "and it is usually measured in watts (W). \n",
      "Luminance\n",
      ", measured in lumens (lm), gives \n",
      "a measure of the amount of energy an observer \n",
      "perceives\n",
      " from a light source. For \n",
      "example, light emitted from a source operating in the far infrared region of the \n",
      "spectrum could have signiﬁcant energy (radiance), but an observer would hardly \n",
      "perceive it; its luminance would be almost zero. Finally, as discussed in Section 2.1, \n",
      "brightness\n",
      " is a subjective descriptor of light perception that is practically impossible \n",
      "to measure. It embodies the achromatic notion of intensity and is one of the key fac-\n",
      "tors in describing color sensation.\n",
      "In principle, if a sensor can be developed that is capable of detecting energy \n",
      "radiated in a band of the electromagnetic spectrum, we can image events of inter-\n",
      "est in that band. Note, however, that the wavelength of an electromagnetic wave \n",
      "required to “see” an object must be of the same size as, or smaller than, the object. \n",
      "For example, a water molecule has a diameter on the order of \n",
      "10\n",
      "10\n",
      "−\n",
      " m. Thus, to study \n",
      "these molecules, we would need a source capable of emitting energy in the far (high-\n",
      "energy) ultraviolet band or soft (low-energy) X-ray bands. \n",
      "Although imaging is based predominantly on energy from electromagnetic wave \n",
      "radiation, this is not the only method for generating images. For example, we saw in \n",
      "Section 1.3 that sound reﬂected from objects can be used to form ultrasonic images. \n",
      "Other sources of digital images are electron beams for electron microscopy, and \n",
      "software for generating synthetic images used in graphics and visualization.\n",
      "2.3 IMAGE SENSING AND ACQUISITION  \n",
      "Most of the images in which we are interested are generated by the combination of \n",
      "an “illumination” source and the reflection or absorption of energy from that source \n",
      "by the elements of the “scene” being imaged. We enclose \n",
      "illumination\n",
      " and \n",
      "scene\n",
      " \n",
      "in quotes to emphasize the fact that they are considerably more general than the \n",
      "2.3\n",
      "DIP4E_GLOBAL_Print_Ready.indb   57\n",
      "6/16/2017   2:02:06 PM\n",
      "www.EBooksWorld.ir58\n",
      "    \n",
      "Chapter\n",
      " \n",
      "2\n",
      "  \n",
      "Digital Image Fundamentals\n",
      "familiar situation in which a visible light source illuminates a familiar 3-D scene. For \n",
      "example, the illumination may originate from a source of electromagnetic energy, \n",
      "such as a radar, infrared, or X-ray system. But, as noted earlier, it could originate \n",
      "from less traditional sources, such as ultrasound or even a computer-generated illu-\n",
      "mination pattern. Similarly, the scene elements could be familiar objects, but they \n",
      "can just as easily be molecules, buried rock formations, or a human brain. Depend-\n",
      "ing on the nature of the source, illumination energy is reflected from, or transmitted \n",
      "through, objects. An example in the first category is light reflected from a planar \n",
      "surface. An example in the second category is when X-rays pass through a patient’s \n",
      "body for the purpose of generating a diagnostic X-ray image. In some applications, \n",
      "the reflected or transmitted energy is focused onto a photo converter (e.g., a phos-\n",
      "phor screen) that converts the energy into visible light. Electron microscopy and \n",
      "some applications of gamma imaging use this approach. \n",
      "Figure 2.12 shows the three principal sensor arrangements used to transform inci-\n",
      "dent energy into digital images. The idea is simple: Incoming energy is transformed \n",
      "into a voltage by a combination of the input electrical power and sensor material \n",
      "that is responsive to the type of energy being detected. The output voltage wave-\n",
      "form is the response of the sensor, and a digital quantity is obtained by digitizing that \n",
      "response. In this section, we look at the principal modalities for image sensing and \n",
      "generation. We will discuss image digitizing in Section 2.4.\n",
      "IMAGE ACQUISITION USING A SINGLE SENSING ELEMENT\n",
      "Figure 2.12(a) shows the components of a single sensing element. A familiar sensor \n",
      "of this type is the photodiode, which is constructed of silicon materials and whose \n",
      "output is a voltage proportional to light intensity. Using a filter in front of a sensor \n",
      "improves its selectivity. For example, an optical green-transmission filter favors light \n",
      "in the green band of the color spectrum. As a consequence, the sensor output would \n",
      "be stronger for green light than for other visible light components.\n",
      "In order to generate a 2-D image using a single sensing element, there has to \n",
      "be relative displacements in both the \n",
      "x\n",
      "- and \n",
      "y\n",
      "-directions between the sensor and \n",
      "the area to be imaged. Figure 2.13 shows an arrangement used in high-precision \n",
      "scanning, where a ﬁlm negative is mounted onto a drum whose mechanical rotation \n",
      "provides displacement in one dimension. The sensor is mounted on a lead screw \n",
      "that provides motion in the perpendicular direction. A light source is contained \n",
      "inside the drum. As the light passes through the ﬁlm, its intensity is modiﬁed by \n",
      "the ﬁlm density before it is captured by the sensor. This \"modulation\" of the light \n",
      "intensity causes corresponding variations in the sensor voltage, which are ultimately \n",
      "converted to image intensity levels by digitization. \n",
      "This method is an inexpensive way to obtain high-resolution images because \n",
      "mechanical motion can be controlled with high precision. The main disadvantages \n",
      "of this method are that it is slow and not readily portable. Other similar mechanical \n",
      "arrangements use a ﬂat imaging bed, with the sensor moving in two linear direc-\n",
      "tions. These types of mechanical digitizers sometimes are referred to as \n",
      "transmission\n",
      " \n",
      "microdensitometers\n",
      ". Systems in which light is reﬂected from the medium, instead \n",
      "of passing through it, are called \n",
      "reﬂection microdensitometers\n",
      ". Another example \n",
      "of imaging with a single sensing element places a laser source coincident with the \n",
      "DIP4E_GLOBAL_Print_Ready.indb   58\n",
      "6/16/2017   2:02:06 PM\n",
      "www.EBooksWorld.ir2.3\n",
      "  \n",
      "Image Sensing and Acquisition\n",
      "59\n",
      "Sensing material\n",
      "Voltage waveform out\n",
      "Filter\n",
      "Energy\n",
      "Power in\n",
      "Housing\n",
      "b\n",
      "a\n",
      "c\n",
      "FIGURE 2.12\n",
      "(a) Single sensing \n",
      "element. \n",
      "(b) Line sensor.  \n",
      "(c) Array sensor.\n",
      "Sensor\n",
      "Linear motion\n",
      "One image line out\n",
      "per increment of rotation\n",
      "and full linear displacement\n",
      "of sensor from left to right\n",
      "Film\n",
      "Rotation\n",
      "FIGURE 2.13\n",
      "Combining a \n",
      "single sensing \n",
      "element with \n",
      "mechanical  \n",
      "motion to  \n",
      "generate a 2-D \n",
      "image.\n",
      "DIP4E_GLOBAL_Print_Ready.indb   59\n",
      "6/16/2017   2:02:07 PM\n",
      "www.EBooksWorld.ir\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_file_folder = 'assets/'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       Page Numbers                                            Content  \\\n0                 1                                 GLOBAL \\nEDITION\\n   \n1                 1                         Digital Image Processing\\n   \n2                 1                                   FOURTH EDITION\\n   \n3                 1            Rafael C. Gonzalez * Richard E. Woods\\n   \n4                 1                               www.EBooksWorld.ir\\n   \n...             ...                                                ...   \n19376          1022  This is a special edition of an established \\n...   \n19377          1022                           Pearson Global Edition\\n   \n19378          1022                                 GLOBAL \\nEDITION\\n   \n19379          1022  For these Global Editions, the editorial team ...   \n19380          1022                               www.EBooksWorld.ir\\n   \n\n                                             no_newlines  \n0                                         GLOBAL EDITION  \n1                               Digital Image Processing  \n2                                         FOURTH EDITION  \n3                  Rafael C. Gonzalez * Richard E. Woods  \n4                                     www.EBooksWorld.ir  \n...                                                  ...  \n19376  This is a special edition of an established ti...  \n19377                             Pearson Global Edition  \n19378                                     GLOBAL EDITION  \n19379  For these Global Editions, the editorial team ...  \n19380                                 www.EBooksWorld.ir  \n\n[19381 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Page Numbers</th>\n      <th>Content</th>\n      <th>no_newlines</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>GLOBAL \\nEDITION\\n</td>\n      <td>GLOBAL EDITION</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Digital Image Processing\\n</td>\n      <td>Digital Image Processing</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>FOURTH EDITION\\n</td>\n      <td>FOURTH EDITION</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>Rafael C. Gonzalez * Richard E. Woods\\n</td>\n      <td>Rafael C. Gonzalez * Richard E. Woods</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>www.EBooksWorld.ir\\n</td>\n      <td>www.EBooksWorld.ir</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19376</th>\n      <td>1022</td>\n      <td>This is a special edition of an established \\n...</td>\n      <td>This is a special edition of an established ti...</td>\n    </tr>\n    <tr>\n      <th>19377</th>\n      <td>1022</td>\n      <td>Pearson Global Edition\\n</td>\n      <td>Pearson Global Edition</td>\n    </tr>\n    <tr>\n      <th>19378</th>\n      <td>1022</td>\n      <td>GLOBAL \\nEDITION\\n</td>\n      <td>GLOBAL EDITION</td>\n    </tr>\n    <tr>\n      <th>19379</th>\n      <td>1022</td>\n      <td>For these Global Editions, the editorial team ...</td>\n      <td>For these Global Editions, the editorial team ...</td>\n    </tr>\n    <tr>\n      <th>19380</th>\n      <td>1022</td>\n      <td>www.EBooksWorld.ir\\n</td>\n      <td>www.EBooksWorld.ir</td>\n    </tr>\n  </tbody>\n</table>\n<p>19381 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unidecode import unidecode # parsing symbols\n",
    "import fitz\n",
    "doc = fitz.open(filename)\n",
    "page_num = 0\n",
    "page_numbers = []\n",
    "content = []\n",
    "output = []\n",
    "for page in doc:\n",
    "    page_num += 1\n",
    "    blocks = page.get_text(\"blocks\")\n",
    "    for block in blocks:\n",
    "        if block[6]==0:\n",
    "            block_content = unidecode(block[4])\n",
    "            stripped_block_content = block_content.replace('\\n', '')\n",
    "            if not stripped_block_content.isdigit() and not is_float(stripped_block_content):\n",
    "                content.append(block_content)\n",
    "                page_numbers.append(page_num)\n",
    "            else:\n",
    "                pass\n",
    "content_df = pd.DataFrame(\n",
    "    {'Page Numbers': page_numbers,\n",
    "     'Content': content,\n",
    "    })\n",
    "content_df['no_newlines'] = content_df['Content'].str.replace(r'\\n', '', regex=True)\n",
    "content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "block_dict = {}\n",
    "page_num = 1\n",
    "for page in doc: # Iterate all pages in the document\n",
    "      file_dict = page.get_text('dict') # Get the page dictionary\n",
    "      block = file_dict['blocks'] # Get the block information\n",
    "      block_dict[page_num] = block # Store in block dictionary\n",
    "      page_num += 1 # Increase the page value by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mSSLCertVerificationError\u001B[0m                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:1346\u001B[0m, in \u001B[0;36mAbstractHTTPHandler.do_open\u001B[1;34m(self, http_class, req, **http_conn_args)\u001B[0m\n\u001B[0;32m   1345\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1346\u001B[0m     \u001B[43mh\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1347\u001B[0m \u001B[43m              \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhas_header\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTransfer-encoding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1348\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:1285\u001B[0m, in \u001B[0;36mHTTPConnection.request\u001B[1;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[0;32m   1284\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1285\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:1331\u001B[0m, in \u001B[0;36mHTTPConnection._send_request\u001B[1;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[0;32m   1330\u001B[0m     body \u001B[38;5;241m=\u001B[39m _encode(body, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m-> 1331\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mendheaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:1280\u001B[0m, in \u001B[0;36mHTTPConnection.endheaders\u001B[1;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[0;32m   1279\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CannotSendHeader()\n\u001B[1;32m-> 1280\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:1040\u001B[0m, in \u001B[0;36mHTTPConnection._send_output\u001B[1;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[0;32m   1039\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer[:]\n\u001B[1;32m-> 1040\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1042\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m message_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1043\u001B[0m \n\u001B[0;32m   1044\u001B[0m     \u001B[38;5;66;03m# create a consistent interface to message_body\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:980\u001B[0m, in \u001B[0;36mHTTPConnection.send\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m    979\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_open:\n\u001B[1;32m--> 980\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    981\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:1454\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1452\u001B[0m     server_hostname \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost\n\u001B[1;32m-> 1454\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrap_socket\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1455\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mserver_hostname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\ssl.py:501\u001B[0m, in \u001B[0;36mSSLContext.wrap_socket\u001B[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001B[0m\n\u001B[0;32m    495\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrap_socket\u001B[39m(\u001B[38;5;28mself\u001B[39m, sock, server_side\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    496\u001B[0m                 do_handshake_on_connect\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    497\u001B[0m                 suppress_ragged_eofs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    498\u001B[0m                 server_hostname\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    499\u001B[0m     \u001B[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001B[39;00m\n\u001B[0;32m    500\u001B[0m     \u001B[38;5;66;03m# ctx._wrap_socket()\u001B[39;00m\n\u001B[1;32m--> 501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msslsocket_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43msock\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mserver_side\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mserver_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdo_handshake_on_connect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_handshake_on_connect\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    505\u001B[0m \u001B[43m        \u001B[49m\u001B[43msuppress_ragged_eofs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msuppress_ragged_eofs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    506\u001B[0m \u001B[43m        \u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mserver_hostname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    507\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    508\u001B[0m \u001B[43m        \u001B[49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msession\u001B[49m\n\u001B[0;32m    509\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\ssl.py:1041\u001B[0m, in \u001B[0;36mSSLSocket._create\u001B[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001B[0m\n\u001B[0;32m   1040\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1041\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_handshake\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1042\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mOSError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\ssl.py:1310\u001B[0m, in \u001B[0;36mSSLSocket.do_handshake\u001B[1;34m(self, block)\u001B[0m\n\u001B[0;32m   1309\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msettimeout(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m-> 1310\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_handshake\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1311\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[1;31mSSLCertVerificationError\u001B[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mURLError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m target_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://homepages.inf.ed.ac.uk/rbf/HIPR2/median.htm\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01murllib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrequest\u001B[39;00m  \u001B[38;5;66;03m# the lib that handles the url stuff\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m \u001B[43murllib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget_url\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28mprint\u001B[39m(line\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:214\u001B[0m, in \u001B[0;36murlopen\u001B[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[0m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    213\u001B[0m     opener \u001B[38;5;241m=\u001B[39m _opener\n\u001B[1;32m--> 214\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:517\u001B[0m, in \u001B[0;36mOpenerDirector.open\u001B[1;34m(self, fullurl, data, timeout)\u001B[0m\n\u001B[0;32m    514\u001B[0m     req \u001B[38;5;241m=\u001B[39m meth(req)\n\u001B[0;32m    516\u001B[0m sys\u001B[38;5;241m.\u001B[39maudit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124murllib.Request\u001B[39m\u001B[38;5;124m'\u001B[39m, req\u001B[38;5;241m.\u001B[39mfull_url, req\u001B[38;5;241m.\u001B[39mdata, req\u001B[38;5;241m.\u001B[39mheaders, req\u001B[38;5;241m.\u001B[39mget_method())\n\u001B[1;32m--> 517\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    519\u001B[0m \u001B[38;5;66;03m# post-process response\u001B[39;00m\n\u001B[0;32m    520\u001B[0m meth_name \u001B[38;5;241m=\u001B[39m protocol\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_response\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:534\u001B[0m, in \u001B[0;36mOpenerDirector._open\u001B[1;34m(self, req, data)\u001B[0m\n\u001B[0;32m    531\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[0;32m    533\u001B[0m protocol \u001B[38;5;241m=\u001B[39m req\u001B[38;5;241m.\u001B[39mtype\n\u001B[1;32m--> 534\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_open\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\n\u001B[0;32m    535\u001B[0m \u001B[43m                          \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_open\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result:\n\u001B[0;32m    537\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:494\u001B[0m, in \u001B[0;36mOpenerDirector._call_chain\u001B[1;34m(self, chain, kind, meth_name, *args)\u001B[0m\n\u001B[0;32m    492\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m handler \u001B[38;5;129;01min\u001B[39;00m handlers:\n\u001B[0;32m    493\u001B[0m     func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[1;32m--> 494\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    495\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    496\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:1389\u001B[0m, in \u001B[0;36mHTTPSHandler.https_open\u001B[1;34m(self, req)\u001B[0m\n\u001B[0;32m   1388\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhttps_open\u001B[39m(\u001B[38;5;28mself\u001B[39m, req):\n\u001B[1;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhttp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mHTTPSConnection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1390\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_hostname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_hostname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:1349\u001B[0m, in \u001B[0;36mAbstractHTTPHandler.do_open\u001B[1;34m(self, http_class, req, **http_conn_args)\u001B[0m\n\u001B[0;32m   1346\u001B[0m         h\u001B[38;5;241m.\u001B[39mrequest(req\u001B[38;5;241m.\u001B[39mget_method(), req\u001B[38;5;241m.\u001B[39mselector, req\u001B[38;5;241m.\u001B[39mdata, headers,\n\u001B[0;32m   1347\u001B[0m                   encode_chunked\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39mhas_header(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTransfer-encoding\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m   1348\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n\u001B[1;32m-> 1349\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m URLError(err)\n\u001B[0;32m   1350\u001B[0m     r \u001B[38;5;241m=\u001B[39m h\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[0;32m   1351\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "\u001B[1;31mURLError\u001B[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>"
     ]
    }
   ],
   "source": [
    "target_url = 'https://homepages.inf.ed.ac.uk/rbf/HIPR2/median.htm'\n",
    "import urllib.request  # the lib that handles the url stuff\n",
    "\n",
    "for line in urllib.request.urlopen(target_url):\n",
    "    print(line.decode('utf-8')) #utf-8 or iso8859-1 or whatever the page encoding scheme is"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Subheading</th>\n",
       "      <th>Content</th>\n",
       "      <th>Section</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\nComputer vision tasks include methods for ac...</td>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>286</td>\n",
       "      <td>[-0.01913553662598133, 0.002932898933067918, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Definition</td>\n",
       "      <td></td>\n",
       "      <td>\\nComputer vision is an interdisciplinary fiel...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Definition</td>\n",
       "      <td>158</td>\n",
       "      <td>[-0.021093836054205894, 0.0049119978211820126,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>History</td>\n",
       "      <td></td>\n",
       "      <td>\\nIn the late 1960s, computer vision began at ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;History</td>\n",
       "      <td>507</td>\n",
       "      <td>[-0.011549791321158409, -0.004044382367283106,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Solid-state physics</td>\n",
       "      <td>\\nSolid-state physics is another field that is...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;S...</td>\n",
       "      <td>120</td>\n",
       "      <td>[0.0018743288237601519, 0.011324070394039154, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Neurobiology</td>\n",
       "      <td>\\nNeurobiology has greatly influenced the deve...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;N...</td>\n",
       "      <td>293</td>\n",
       "      <td>[-0.009132628329098225, 0.0011366719845682383,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Signal processing</td>\n",
       "      <td>\\nYet another field related to computer vision...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;S...</td>\n",
       "      <td>103</td>\n",
       "      <td>[-0.027298789471387863, 0.007510432507842779, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Robotic navigation</td>\n",
       "      <td>\\nRobot navigation sometimes deals with autono...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;R...</td>\n",
       "      <td>64</td>\n",
       "      <td>[0.0034529592376202345, -0.014102335087954998,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Other fields</td>\n",
       "      <td>\\nBesides the above-mentioned views on compute...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;O...</td>\n",
       "      <td>119</td>\n",
       "      <td>[0.002435609931126237, -0.003915637265890837, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Distinctions</td>\n",
       "      <td>\\nThe fields most closely related to computer ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;D...</td>\n",
       "      <td>639</td>\n",
       "      <td>[-0.017207426950335503, 0.005905073136091232, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td></td>\n",
       "      <td>\\nApplications range from tasks such as indust...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications</td>\n",
       "      <td>272</td>\n",
       "      <td>[-0.022458024322986603, 0.005672922823578119, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>\\nOne of the most prominent application fields...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Med...</td>\n",
       "      <td>135</td>\n",
       "      <td>[-0.016155855730175972, 0.02248280495405197, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Machine vision</td>\n",
       "      <td>\\nA second application area in computer vision...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Mac...</td>\n",
       "      <td>141</td>\n",
       "      <td>[-0.016629502177238464, 0.002632115501910448, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Military</td>\n",
       "      <td>\\nMilitary applications are probably one of th...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Mil...</td>\n",
       "      <td>129</td>\n",
       "      <td>[-0.02624369040131569, 0.001608214108273387, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Autonomous vehicles</td>\n",
       "      <td>\\nOne of the newer application areas is autono...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Aut...</td>\n",
       "      <td>234</td>\n",
       "      <td>[0.002211927669122815, -0.004606796428561211, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Tactile feedback</td>\n",
       "      <td>\\nMaterials such as rubber and silicon are bei...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Tac...</td>\n",
       "      <td>270</td>\n",
       "      <td>[-0.015194285660982132, 0.023810898885130882, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td></td>\n",
       "      <td>\\nEach of the application areas described abov...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks</td>\n",
       "      <td>161</td>\n",
       "      <td>[-0.018167616799473763, 0.007240524981170893, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Recognition</td>\n",
       "      <td>\\nThe classical problem in computer vision, im...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Re...</td>\n",
       "      <td>691</td>\n",
       "      <td>[-0.018989920616149902, 0.02043752372264862, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Motion analysis</td>\n",
       "      <td>\\nSeveral tasks relate to motion estimation wh...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Mo...</td>\n",
       "      <td>193</td>\n",
       "      <td>[-0.02092411182820797, 0.00222062598913908, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Scene reconstruction</td>\n",
       "      <td>\\nGiven one or (typically) more images of a sc...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Sc...</td>\n",
       "      <td>125</td>\n",
       "      <td>[-0.02498997002840042, 0.015953006222844124, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Image restoration</td>\n",
       "      <td>\\nImage restoration comes into picture when th...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Im...</td>\n",
       "      <td>198</td>\n",
       "      <td>[0.0009744223789311945, 0.03190232068300247, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>System methods</td>\n",
       "      <td></td>\n",
       "      <td>\\nThe organization of a computer vision system...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;System methods</td>\n",
       "      <td>672</td>\n",
       "      <td>[-0.0014013586333021522, 0.026699397712945938,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>System methods</td>\n",
       "      <td>Image-understanding systems</td>\n",
       "      <td>\\nImage-understanding systems (IUS) include th...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;System methods-&gt;I...</td>\n",
       "      <td>192</td>\n",
       "      <td>[0.006758322473615408, -0.004905234090983868, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Hardware</td>\n",
       "      <td></td>\n",
       "      <td>\\nThere are many kinds of computer vision syst...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Hardware</td>\n",
       "      <td>392</td>\n",
       "      <td>[-0.006029689218848944, 0.016216637566685677, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Source         Heading                   Subheading  \\\n",
       "0   Wikipedia (Computer vision)                                                \n",
       "1   Wikipedia (Computer vision)      Definition                                \n",
       "2   Wikipedia (Computer vision)         History                                \n",
       "4   Wikipedia (Computer vision)  Related fields          Solid-state physics   \n",
       "5   Wikipedia (Computer vision)  Related fields                 Neurobiology   \n",
       "6   Wikipedia (Computer vision)  Related fields            Signal processing   \n",
       "7   Wikipedia (Computer vision)  Related fields           Robotic navigation   \n",
       "8   Wikipedia (Computer vision)  Related fields                 Other fields   \n",
       "9   Wikipedia (Computer vision)  Related fields                 Distinctions   \n",
       "10  Wikipedia (Computer vision)    Applications                                \n",
       "11  Wikipedia (Computer vision)    Applications                     Medicine   \n",
       "12  Wikipedia (Computer vision)    Applications               Machine vision   \n",
       "13  Wikipedia (Computer vision)    Applications                     Military   \n",
       "14  Wikipedia (Computer vision)    Applications          Autonomous vehicles   \n",
       "15  Wikipedia (Computer vision)    Applications             Tactile feedback   \n",
       "16  Wikipedia (Computer vision)   Typical tasks                                \n",
       "17  Wikipedia (Computer vision)   Typical tasks                  Recognition   \n",
       "18  Wikipedia (Computer vision)   Typical tasks              Motion analysis   \n",
       "19  Wikipedia (Computer vision)   Typical tasks         Scene reconstruction   \n",
       "20  Wikipedia (Computer vision)   Typical tasks            Image restoration   \n",
       "21  Wikipedia (Computer vision)  System methods                                \n",
       "22  Wikipedia (Computer vision)  System methods  Image-understanding systems   \n",
       "23  Wikipedia (Computer vision)        Hardware                                \n",
       "\n",
       "                                              Content  \\\n",
       "0   \\nComputer vision tasks include methods for ac...   \n",
       "1   \\nComputer vision is an interdisciplinary fiel...   \n",
       "2   \\nIn the late 1960s, computer vision began at ...   \n",
       "4   \\nSolid-state physics is another field that is...   \n",
       "5   \\nNeurobiology has greatly influenced the deve...   \n",
       "6   \\nYet another field related to computer vision...   \n",
       "7   \\nRobot navigation sometimes deals with autono...   \n",
       "8   \\nBesides the above-mentioned views on compute...   \n",
       "9   \\nThe fields most closely related to computer ...   \n",
       "10  \\nApplications range from tasks such as indust...   \n",
       "11  \\nOne of the most prominent application fields...   \n",
       "12  \\nA second application area in computer vision...   \n",
       "13  \\nMilitary applications are probably one of th...   \n",
       "14  \\nOne of the newer application areas is autono...   \n",
       "15  \\nMaterials such as rubber and silicon are bei...   \n",
       "16  \\nEach of the application areas described abov...   \n",
       "17  \\nThe classical problem in computer vision, im...   \n",
       "18  \\nSeveral tasks relate to motion estimation wh...   \n",
       "19  \\nGiven one or (typically) more images of a sc...   \n",
       "20  \\nImage restoration comes into picture when th...   \n",
       "21  \\nThe organization of a computer vision system...   \n",
       "22  \\nImage-understanding systems (IUS) include th...   \n",
       "23  \\nThere are many kinds of computer vision syst...   \n",
       "\n",
       "                                              Section  Tokens  \\\n",
       "0                         Wikipedia (Computer vision)     286   \n",
       "1             Wikipedia (Computer vision)->Definition     158   \n",
       "2                Wikipedia (Computer vision)->History     507   \n",
       "4   Wikipedia (Computer vision)->Related fields->S...     120   \n",
       "5   Wikipedia (Computer vision)->Related fields->N...     293   \n",
       "6   Wikipedia (Computer vision)->Related fields->S...     103   \n",
       "7   Wikipedia (Computer vision)->Related fields->R...      64   \n",
       "8   Wikipedia (Computer vision)->Related fields->O...     119   \n",
       "9   Wikipedia (Computer vision)->Related fields->D...     639   \n",
       "10          Wikipedia (Computer vision)->Applications     272   \n",
       "11  Wikipedia (Computer vision)->Applications->Med...     135   \n",
       "12  Wikipedia (Computer vision)->Applications->Mac...     141   \n",
       "13  Wikipedia (Computer vision)->Applications->Mil...     129   \n",
       "14  Wikipedia (Computer vision)->Applications->Aut...     234   \n",
       "15  Wikipedia (Computer vision)->Applications->Tac...     270   \n",
       "16         Wikipedia (Computer vision)->Typical tasks     161   \n",
       "17  Wikipedia (Computer vision)->Typical tasks->Re...     691   \n",
       "18  Wikipedia (Computer vision)->Typical tasks->Mo...     193   \n",
       "19  Wikipedia (Computer vision)->Typical tasks->Sc...     125   \n",
       "20  Wikipedia (Computer vision)->Typical tasks->Im...     198   \n",
       "21        Wikipedia (Computer vision)->System methods     672   \n",
       "22  Wikipedia (Computer vision)->System methods->I...     192   \n",
       "23              Wikipedia (Computer vision)->Hardware     392   \n",
       "\n",
       "                                            Embedding  \n",
       "0   [-0.01913553662598133, 0.002932898933067918, 0...  \n",
       "1   [-0.021093836054205894, 0.0049119978211820126,...  \n",
       "2   [-0.011549791321158409, -0.004044382367283106,...  \n",
       "4   [0.0018743288237601519, 0.011324070394039154, ...  \n",
       "5   [-0.009132628329098225, 0.0011366719845682383,...  \n",
       "6   [-0.027298789471387863, 0.007510432507842779, ...  \n",
       "7   [0.0034529592376202345, -0.014102335087954998,...  \n",
       "8   [0.002435609931126237, -0.003915637265890837, ...  \n",
       "9   [-0.017207426950335503, 0.005905073136091232, ...  \n",
       "10  [-0.022458024322986603, 0.005672922823578119, ...  \n",
       "11  [-0.016155855730175972, 0.02248280495405197, 0...  \n",
       "12  [-0.016629502177238464, 0.002632115501910448, ...  \n",
       "13  [-0.02624369040131569, 0.001608214108273387, 0...  \n",
       "14  [0.002211927669122815, -0.004606796428561211, ...  \n",
       "15  [-0.015194285660982132, 0.023810898885130882, ...  \n",
       "16  [-0.018167616799473763, 0.007240524981170893, ...  \n",
       "17  [-0.018989920616149902, 0.02043752372264862, 0...  \n",
       "18  [-0.02092411182820797, 0.00222062598913908, -0...  \n",
       "19  [-0.02498997002840042, 0.015953006222844124, 0...  \n",
       "20  [0.0009744223789311945, 0.03190232068300247, 0...  \n",
       "21  [-0.0014013586333021522, 0.026699397712945938,...  \n",
       "22  [0.006758322473615408, -0.004905234090983868, ...  \n",
       "23  [-0.006029689218848944, 0.016216637566685677, ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CompVisionKnowledge = Knowledge(WIKI_PAGE, 'GPT')\n",
    "for page in WIKI_PAGES:\n",
    "    CompVisionKnowledge.append_wikipedia_page(WIKI_PAGE)\n",
    "# save document chunks and embeddings\n",
    "CompVisionKnowledge.export_to_csv(GPT_KNOWLEDGE_FILENAME)\n",
    "CompVisionKnowledge.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Subheading</th>\n",
       "      <th>Content</th>\n",
       "      <th>Section</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\nComputer vision tasks include methods for ac...</td>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>290</td>\n",
       "      <td>[-0.5566069483757019, 0.6151323318481445, 0.70...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Definition</td>\n",
       "      <td></td>\n",
       "      <td>\\nComputer vision is an interdisciplinary fiel...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Definition</td>\n",
       "      <td>162</td>\n",
       "      <td>[-0.18940897285938263, 0.5564344525337219, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>History</td>\n",
       "      <td></td>\n",
       "      <td>\\nIn the late 1960s, computer vision began at ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;History</td>\n",
       "      <td>243</td>\n",
       "      <td>[-0.5624328255653381, 0.35494446754455566, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>History</td>\n",
       "      <td></td>\n",
       "      <td>\\nBy the 1990s, some of the previous research ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;History</td>\n",
       "      <td>264</td>\n",
       "      <td>[-0.8420819044113159, 0.011862404644489288, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Solid-state physics</td>\n",
       "      <td>\\nSolid-state physics is another field that is...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;S...</td>\n",
       "      <td>123</td>\n",
       "      <td>[-0.1766018569469452, 0.5509802103042603, 0.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Neurobiology</td>\n",
       "      <td>\\nNeurobiology has greatly influenced the deve...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;N...</td>\n",
       "      <td>296</td>\n",
       "      <td>[0.07454751431941986, 0.42800015211105347, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Signal processing</td>\n",
       "      <td>\\nYet another field related to computer vision...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;S...</td>\n",
       "      <td>106</td>\n",
       "      <td>[-0.1562240570783615, 0.18830031156539917, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Robotic navigation</td>\n",
       "      <td>\\nRobot navigation sometimes deals with autono...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;R...</td>\n",
       "      <td>65</td>\n",
       "      <td>[0.09209920465946198, 0.5207788944244385, 1.26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Other fields</td>\n",
       "      <td>\\nBesides the above-mentioned views on compute...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;O...</td>\n",
       "      <td>122</td>\n",
       "      <td>[-0.34635502099990845, 0.12120083719491959, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Distinctions</td>\n",
       "      <td>\\nThe fields most closely related to computer ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;D...</td>\n",
       "      <td>244</td>\n",
       "      <td>[-0.011814514175057411, 0.9892112612724304, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Distinctions</td>\n",
       "      <td>\\nImage processing and image analysis tend to ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;D...</td>\n",
       "      <td>395</td>\n",
       "      <td>[-0.25189733505249023, 0.7090330123901367, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td></td>\n",
       "      <td>\\nApplications range from tasks such as indust...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications</td>\n",
       "      <td>294</td>\n",
       "      <td>[0.054026536643505096, 0.536332368850708, 0.80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>\\nOne of the most prominent application fields...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Med...</td>\n",
       "      <td>145</td>\n",
       "      <td>[0.07362960278987885, 0.8047475218772888, 1.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Machine vision</td>\n",
       "      <td>\\nA second application area in computer vision...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Mac...</td>\n",
       "      <td>148</td>\n",
       "      <td>[0.23051360249519348, 0.6428527235984802, 0.69...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Military</td>\n",
       "      <td>\\nMilitary applications are probably one of th...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Mil...</td>\n",
       "      <td>129</td>\n",
       "      <td>[-0.45685380697250366, 0.40300095081329346, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Autonomous vehicles</td>\n",
       "      <td>\\nOne of the newer application areas is autono...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Aut...</td>\n",
       "      <td>258</td>\n",
       "      <td>[-0.5068467855453491, 0.4222138524055481, 1.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Tactile feedback</td>\n",
       "      <td>\\nMaterials such as rubber and silicon are bei...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Tac...</td>\n",
       "      <td>286</td>\n",
       "      <td>[-0.006287522614002228, 0.3353821039199829, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td></td>\n",
       "      <td>\\nEach of the application areas described abov...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks</td>\n",
       "      <td>166</td>\n",
       "      <td>[-0.3646470308303833, 0.5144392848014832, 1.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Recognition</td>\n",
       "      <td>\\nThe classical problem in computer vision, im...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Re...</td>\n",
       "      <td>427</td>\n",
       "      <td>[-0.11414705961942673, 0.8190616965293884, 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Recognition</td>\n",
       "      <td>\\nContent-based image retrieval – finding all ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Re...</td>\n",
       "      <td>264</td>\n",
       "      <td>[-0.3213890492916107, 1.116416335105896, 0.800...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Motion analysis</td>\n",
       "      <td>\\nSeveral tasks relate to motion estimation wh...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Mo...</td>\n",
       "      <td>194</td>\n",
       "      <td>[-0.08507562428712845, 0.058049630373716354, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Scene reconstruction</td>\n",
       "      <td>\\nGiven one or (typically) more images of a sc...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Sc...</td>\n",
       "      <td>115</td>\n",
       "      <td>[-0.6162410378456116, 0.3802846670150757, 1.66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Image restoration</td>\n",
       "      <td>\\nImage restoration comes into picture when th...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Im...</td>\n",
       "      <td>204</td>\n",
       "      <td>[-0.25118744373321533, 0.52935791015625, 1.085...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>System methods</td>\n",
       "      <td></td>\n",
       "      <td>\\nThe organization of a computer vision system...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;System methods</td>\n",
       "      <td>122</td>\n",
       "      <td>[0.13426706194877625, 0.5260908603668213, 1.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>System methods</td>\n",
       "      <td></td>\n",
       "      <td>\\nImage acquisition – A digital image is produ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;System methods</td>\n",
       "      <td>253</td>\n",
       "      <td>[0.13420583307743073, 0.20847204327583313, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>System methods</td>\n",
       "      <td></td>\n",
       "      <td>\\nLocalized interest points such as corners, b...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;System methods</td>\n",
       "      <td>297</td>\n",
       "      <td>[-0.2144298553466797, -0.15774108469486237, 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>System methods</td>\n",
       "      <td>Image-understanding systems</td>\n",
       "      <td>\\nImage-understanding systems (IUS) include th...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;System methods-&gt;I...</td>\n",
       "      <td>202</td>\n",
       "      <td>[-0.3703722059726715, 0.7370752692222595, 0.51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Hardware</td>\n",
       "      <td></td>\n",
       "      <td>\\nThere are many kinds of computer vision syst...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Hardware</td>\n",
       "      <td>409</td>\n",
       "      <td>[0.02258257381618023, 0.39461076259613037, 1.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Source         Heading                   Subheading  \\\n",
       "0   Wikipedia (Computer vision)                                                \n",
       "1   Wikipedia (Computer vision)      Definition                                \n",
       "2   Wikipedia (Computer vision)         History                                \n",
       "3   Wikipedia (Computer vision)         History                                \n",
       "5   Wikipedia (Computer vision)  Related fields          Solid-state physics   \n",
       "6   Wikipedia (Computer vision)  Related fields                 Neurobiology   \n",
       "7   Wikipedia (Computer vision)  Related fields            Signal processing   \n",
       "8   Wikipedia (Computer vision)  Related fields           Robotic navigation   \n",
       "9   Wikipedia (Computer vision)  Related fields                 Other fields   \n",
       "10  Wikipedia (Computer vision)  Related fields                 Distinctions   \n",
       "11  Wikipedia (Computer vision)  Related fields                 Distinctions   \n",
       "12  Wikipedia (Computer vision)    Applications                                \n",
       "13  Wikipedia (Computer vision)    Applications                     Medicine   \n",
       "14  Wikipedia (Computer vision)    Applications               Machine vision   \n",
       "15  Wikipedia (Computer vision)    Applications                     Military   \n",
       "16  Wikipedia (Computer vision)    Applications          Autonomous vehicles   \n",
       "17  Wikipedia (Computer vision)    Applications             Tactile feedback   \n",
       "18  Wikipedia (Computer vision)   Typical tasks                                \n",
       "19  Wikipedia (Computer vision)   Typical tasks                  Recognition   \n",
       "20  Wikipedia (Computer vision)   Typical tasks                  Recognition   \n",
       "21  Wikipedia (Computer vision)   Typical tasks              Motion analysis   \n",
       "22  Wikipedia (Computer vision)   Typical tasks         Scene reconstruction   \n",
       "23  Wikipedia (Computer vision)   Typical tasks            Image restoration   \n",
       "24  Wikipedia (Computer vision)  System methods                                \n",
       "25  Wikipedia (Computer vision)  System methods                                \n",
       "26  Wikipedia (Computer vision)  System methods                                \n",
       "27  Wikipedia (Computer vision)  System methods  Image-understanding systems   \n",
       "28  Wikipedia (Computer vision)        Hardware                                \n",
       "\n",
       "                                              Content  \\\n",
       "0   \\nComputer vision tasks include methods for ac...   \n",
       "1   \\nComputer vision is an interdisciplinary fiel...   \n",
       "2   \\nIn the late 1960s, computer vision began at ...   \n",
       "3   \\nBy the 1990s, some of the previous research ...   \n",
       "5   \\nSolid-state physics is another field that is...   \n",
       "6   \\nNeurobiology has greatly influenced the deve...   \n",
       "7   \\nYet another field related to computer vision...   \n",
       "8   \\nRobot navigation sometimes deals with autono...   \n",
       "9   \\nBesides the above-mentioned views on compute...   \n",
       "10  \\nThe fields most closely related to computer ...   \n",
       "11  \\nImage processing and image analysis tend to ...   \n",
       "12  \\nApplications range from tasks such as indust...   \n",
       "13  \\nOne of the most prominent application fields...   \n",
       "14  \\nA second application area in computer vision...   \n",
       "15  \\nMilitary applications are probably one of th...   \n",
       "16  \\nOne of the newer application areas is autono...   \n",
       "17  \\nMaterials such as rubber and silicon are bei...   \n",
       "18  \\nEach of the application areas described abov...   \n",
       "19  \\nThe classical problem in computer vision, im...   \n",
       "20  \\nContent-based image retrieval – finding all ...   \n",
       "21  \\nSeveral tasks relate to motion estimation wh...   \n",
       "22  \\nGiven one or (typically) more images of a sc...   \n",
       "23  \\nImage restoration comes into picture when th...   \n",
       "24  \\nThe organization of a computer vision system...   \n",
       "25  \\nImage acquisition – A digital image is produ...   \n",
       "26  \\nLocalized interest points such as corners, b...   \n",
       "27  \\nImage-understanding systems (IUS) include th...   \n",
       "28  \\nThere are many kinds of computer vision syst...   \n",
       "\n",
       "                                              Section  Tokens  \\\n",
       "0                         Wikipedia (Computer vision)     290   \n",
       "1             Wikipedia (Computer vision)->Definition     162   \n",
       "2                Wikipedia (Computer vision)->History     243   \n",
       "3                Wikipedia (Computer vision)->History     264   \n",
       "5   Wikipedia (Computer vision)->Related fields->S...     123   \n",
       "6   Wikipedia (Computer vision)->Related fields->N...     296   \n",
       "7   Wikipedia (Computer vision)->Related fields->S...     106   \n",
       "8   Wikipedia (Computer vision)->Related fields->R...      65   \n",
       "9   Wikipedia (Computer vision)->Related fields->O...     122   \n",
       "10  Wikipedia (Computer vision)->Related fields->D...     244   \n",
       "11  Wikipedia (Computer vision)->Related fields->D...     395   \n",
       "12          Wikipedia (Computer vision)->Applications     294   \n",
       "13  Wikipedia (Computer vision)->Applications->Med...     145   \n",
       "14  Wikipedia (Computer vision)->Applications->Mac...     148   \n",
       "15  Wikipedia (Computer vision)->Applications->Mil...     129   \n",
       "16  Wikipedia (Computer vision)->Applications->Aut...     258   \n",
       "17  Wikipedia (Computer vision)->Applications->Tac...     286   \n",
       "18         Wikipedia (Computer vision)->Typical tasks     166   \n",
       "19  Wikipedia (Computer vision)->Typical tasks->Re...     427   \n",
       "20  Wikipedia (Computer vision)->Typical tasks->Re...     264   \n",
       "21  Wikipedia (Computer vision)->Typical tasks->Mo...     194   \n",
       "22  Wikipedia (Computer vision)->Typical tasks->Sc...     115   \n",
       "23  Wikipedia (Computer vision)->Typical tasks->Im...     204   \n",
       "24        Wikipedia (Computer vision)->System methods     122   \n",
       "25        Wikipedia (Computer vision)->System methods     253   \n",
       "26        Wikipedia (Computer vision)->System methods     297   \n",
       "27  Wikipedia (Computer vision)->System methods->I...     202   \n",
       "28              Wikipedia (Computer vision)->Hardware     409   \n",
       "\n",
       "                                            Embedding  \n",
       "0   [-0.5566069483757019, 0.6151323318481445, 0.70...  \n",
       "1   [-0.18940897285938263, 0.5564344525337219, 0.5...  \n",
       "2   [-0.5624328255653381, 0.35494446754455566, 0.6...  \n",
       "3   [-0.8420819044113159, 0.011862404644489288, 0....  \n",
       "5   [-0.1766018569469452, 0.5509802103042603, 0.11...  \n",
       "6   [0.07454751431941986, 0.42800015211105347, 0.1...  \n",
       "7   [-0.1562240570783615, 0.18830031156539917, 0.4...  \n",
       "8   [0.09209920465946198, 0.5207788944244385, 1.26...  \n",
       "9   [-0.34635502099990845, 0.12120083719491959, 0....  \n",
       "10  [-0.011814514175057411, 0.9892112612724304, 0....  \n",
       "11  [-0.25189733505249023, 0.7090330123901367, 1.0...  \n",
       "12  [0.054026536643505096, 0.536332368850708, 0.80...  \n",
       "13  [0.07362960278987885, 0.8047475218772888, 1.09...  \n",
       "14  [0.23051360249519348, 0.6428527235984802, 0.69...  \n",
       "15  [-0.45685380697250366, 0.40300095081329346, 0....  \n",
       "16  [-0.5068467855453491, 0.4222138524055481, 1.11...  \n",
       "17  [-0.006287522614002228, 0.3353821039199829, 0....  \n",
       "18  [-0.3646470308303833, 0.5144392848014832, 1.08...  \n",
       "19  [-0.11414705961942673, 0.8190616965293884, 0.9...  \n",
       "20  [-0.3213890492916107, 1.116416335105896, 0.800...  \n",
       "21  [-0.08507562428712845, 0.058049630373716354, 1...  \n",
       "22  [-0.6162410378456116, 0.3802846670150757, 1.66...  \n",
       "23  [-0.25118744373321533, 0.52935791015625, 1.085...  \n",
       "24  [0.13426706194877625, 0.5260908603668213, 1.00...  \n",
       "25  [0.13420583307743073, 0.20847204327583313, 0.6...  \n",
       "26  [-0.2144298553466797, -0.15774108469486237, 1....  \n",
       "27  [-0.3703722059726715, 0.7370752692222595, 0.51...  \n",
       "28  [0.02258257381618023, 0.39461076259613037, 1.0...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CompVisionKnowledgeBERT = Knowledge(WIKI_PAGE, 'BERT')\n",
    "CompVisionKnowledgeBERT.append_wikipedia_page(WIKI_PAGE)\n",
    "# save document chunks and embeddings\n",
    "CompVisionKnowledgeBERT.export_to_csv(BERT_KNOWLEDGE_FILENAME)\n",
    "CompVisionKnowledgeBERT.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Search\n",
    "Now we'll define a search function that:\n",
    "\n",
    "Takes a user query and a dataframe with text & embedding columns\n",
    "Embeds the user query with the OpenAI API\n",
    "Uses distance between query embedding and text embeddings to rank the texts\n",
    "Returns two lists:\n",
    "The top N texts, ranked by relevance\n",
    "Their corresponding relevance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "370d7486832f4481859fe77fbfc7a969"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36m<cell line: 256>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# CompVisionBERT = ChatBot(\"Computer Vision\", 'assets/' + BERT_KNOWLEDGE_FILENAME)\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# print(Query.ask_bert('When did universities begin teaching Computer Vision?', CompVisionBERT))\u001B[39;00m\n\u001B[0;32m    251\u001B[0m \n\u001B[0;32m    252\u001B[0m \u001B[38;5;66;03m# CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\u001B[39;00m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;66;03m# print(Query.ask('Who is Boris Johnson', CompVisionGPT, show_source=True))\u001B[39;00m\n\u001B[0;32m    255\u001B[0m CompVisionGPT \u001B[38;5;241m=\u001B[39m ChatBot(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mComputer Vision\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124massets/\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m GPT_KNOWLEDGE_FILENAME)\n\u001B[1;32m--> 256\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mQuery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mask_bart\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mWhen did Universities begin teaching Computer Vision?\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCompVisionGPT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_source\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.ask_bart\u001B[1;34m(cls, query_text, chatbot_instance, show_source, confidence_level)\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQuestion is too long, please try again with a shorter question.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    214\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(query_text, chatbot_instance)\n\u001B[1;32m--> 215\u001B[0m response_message \u001B[38;5;241m=\u001B[39m \u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_bart_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfidence_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfidence_level\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m show_source \u001B[38;5;129;01mand\u001B[39;00m response_message\u001B[38;5;241m!=\u001B[39mANSWER_NOT_FOUND_MSG: \u001B[38;5;66;03m# Display the sources used:\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     response_message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m query\u001B[38;5;241m.\u001B[39mshow_source_message()\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.get_bart_output\u001B[1;34m(self, encoding_model, bert_model, confidence_level)\u001B[0m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_bart_output\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    171\u001B[0m                     \u001B[38;5;66;03m# chatbot_instance: ChatBot,\u001B[39;00m\n\u001B[0;32m    172\u001B[0m                     \u001B[38;5;66;03m# embedding_model: str = BART_EMBEDDING_MODEL,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    175\u001B[0m                     confidence_level: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m,\n\u001B[0;32m    176\u001B[0m                     ):\n\u001B[1;32m--> 177\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mknowledge_ranked_by_similarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfidence_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfidence_level\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mknowledge_used)\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m ANSWER_NOT_FOUND_MSG\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.knowledge_ranked_by_similarity\u001B[1;34m(self, max_num_sections, confidence_level, embedding_model)\u001B[0m\n\u001B[0;32m     28\u001B[0m query_embedding_response \u001B[38;5;241m=\u001B[39m get_embedding(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontent, embedding_model\u001B[38;5;241m=\u001B[39mBERT_EMBEDDING_MODEL)\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m embedding_model \u001B[38;5;241m==\u001B[39m GPT_EMBEDDING_MODEL:\n\u001B[1;32m---> 30\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[43mquery_embedding_response\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# knowledge_with_similarities[\"similarity\"] = knowledge_with_similarities[\"Embedding\"].apply(lambda x: self.similarity(query_embedding, x))\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(query_embedding_response)\n",
      "\u001B[1;31mIndexError\u001B[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# CompVisionBERT = ChatBot(\"Computer Vision\", 'assets/' + BERT_KNOWLEDGE_FILENAME)\n",
    "# print(Query.ask_bert('When did universities begin teaching Computer Vision?', CompVisionBERT))\n",
    "\n",
    "# CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\n",
    "# print(Query.ask('Who is Boris Johnson', CompVisionGPT, show_source=True))\n",
    "\n",
    "CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\n",
    "print(Query.ask_bart('When did Universities begin teaching Computer Vision?', CompVisionGPT, show_source=True)) # What if the GPT knowledge sections are longer than 1024 tokens?? Need to account for this!\n",
    "\n",
    "# Todo:\n",
    "# I need to make it more efficient on the number of tokens.\n",
    "# Adapt it for more sources (e.g. PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\point\\AppData\\Local\\Temp\\ipykernel_23968\\3384959035.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Index'] = np.arange(len(self.knowledge_used))+1\n",
      "C:\\Users\\point\\AppData\\Local\\Temp\\ipykernel_23968\\3384959035.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Cumulative_tokens'] = self.knowledge_used['Tokens'].cumsum()\n",
      "C:\\Users\\point\\AppData\\Local\\Temp\\ipykernel_23968\\3384959035.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Cumulative_tokens'] += message_and_question_tokens # add the inital number of tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could not find an answer in the text I've been provided, sorry! Please try again.\n",
      "\n",
      "Total tokens used: 721\n"
     ]
    }
   ],
   "source": [
    "CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\n",
    "print(Query.ask('Who is Boris Johnson', CompVisionGPT, show_source=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d07a56b31944c4bb14a999b57fd3c4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (768,) (1536,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# CompVisionBERT = ChatBot(\"Computer Vision\", 'assets/' + BERT_KNOWLEDGE_FILENAME)\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mQuery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mask_bert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mWhen did universities begin teaching Computer Vision?\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCompVisionGPT\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.ask_bert\u001B[1;34m(cls, query_text, chatbot_instance, embedding_model, encoding_model, bert_model, show_source)\u001B[0m\n\u001B[0;32m    126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQuestion is too long, please try again with a shorter question.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    127\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(query_text, chatbot_instance)\n\u001B[1;32m--> 128\u001B[0m response_message, answer_index \u001B[38;5;241m=\u001B[39m \u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_bert_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membedding_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbert_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbert_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m show_source \u001B[38;5;129;01mand\u001B[39;00m response_message\u001B[38;5;241m!=\u001B[39mANSWER_NOT_FOUND_MSG: \u001B[38;5;66;03m# Display the sources used:\u001B[39;00m\n\u001B[0;32m    131\u001B[0m     response_message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m query\u001B[38;5;241m.\u001B[39mshow_source_message(answer_index\u001B[38;5;241m=\u001B[39manswer_index)\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.get_bert_output\u001B[1;34m(self, embedding_model, encoding_model, bert_model)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_bert_output\u001B[39m(\n\u001B[0;32m     77\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     78\u001B[0m         embedding_model: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m     79\u001B[0m         encoding_model: BertTokenizer \u001B[38;5;241m=\u001B[39m BERT_ENCODING,\n\u001B[0;32m     80\u001B[0m         bert_model: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m BERT_MODEL\n\u001B[0;32m     81\u001B[0m ):\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;124;03m\"\"\"Uses the most relevant texts from the knowledge dataframe to construct a message that can then be fed into GPT.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 83\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mknowledge_ranked_by_similarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membedding_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m     answer_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     86\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.knowledge_ranked_by_similarity\u001B[1;34m(self, max_num_sections, confidence_level, embedding_model)\u001B[0m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# knowledge_with_similarities[\"similarity\"] = knowledge_with_similarities[\"Embedding\"].apply(lambda x: self.similarity(query_embedding, x))\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(query_embedding_response)\n\u001B[1;32m---> 34\u001B[0m knowledge_with_similarities[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mknowledge_with_similarities\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEmbedding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_embedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     36\u001B[0m knowledge_with_similarities\u001B[38;5;241m.\u001B[39msort_values(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     37\u001B[0m top_n_sections \u001B[38;5;241m=\u001B[39m knowledge_with_similarities\u001B[38;5;241m.\u001B[39mhead(max_num_sections)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[0;32m   4323\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4324\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4325\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4328\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4329\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4330\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4331\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4332\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4431\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4432\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4433\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1078\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m   1079\u001B[0m     \u001B[38;5;66;03m# if we are a string, try to dispatch\u001B[39;00m\n\u001B[0;32m   1080\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[1;32m-> 1082\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1131\u001B[0m         values \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values\n\u001B[0;32m   1132\u001B[0m         \u001B[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001B[39;00m\n\u001B[0;32m   1133\u001B[0m         \u001B[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001B[39;00m\n\u001B[0;32m   1134\u001B[0m         \u001B[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001B[39;00m\n\u001B[0;32m   1135\u001B[0m         \u001B[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001B[39;00m\n\u001B[0;32m   1136\u001B[0m         \u001B[38;5;66;03m# \"Callable[[Any], Any]\"\u001B[39;00m\n\u001B[1;32m-> 1137\u001B[0m         mapped \u001B[38;5;241m=\u001B[39m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1138\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1139\u001B[0m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m   1140\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1141\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1144\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1145\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1146\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.knowledge_ranked_by_similarity.<locals>.<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# knowledge_with_similarities[\"similarity\"] = knowledge_with_similarities[\"Embedding\"].apply(lambda x: self.similarity(query_embedding, x))\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(query_embedding_response)\n\u001B[1;32m---> 34\u001B[0m knowledge_with_similarities[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m knowledge_with_similarities[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmbedding\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_embedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     36\u001B[0m knowledge_with_similarities\u001B[38;5;241m.\u001B[39msort_values(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     37\u001B[0m top_n_sections \u001B[38;5;241m=\u001B[39m knowledge_with_similarities\u001B[38;5;241m.\u001B[39mhead(max_num_sections)\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.similarity\u001B[1;34m(query_embedding, knowledge_embedding)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msimilarity\u001B[39m(query_embedding: \u001B[38;5;28mlist\u001B[39m,\n\u001B[0;32m     13\u001B[0m                knowledge_embedding: \u001B[38;5;28mlist\u001B[39m\n\u001B[0;32m     14\u001B[0m                ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;124;03m\"\"\"Calculates the cosine similarity score between the query and knowledge embedding vectors.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m1\u001B[39m\u001B[38;5;241m-\u001B[39m \u001B[43mspatial\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcosine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_embedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mknowledge_embedding\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:670\u001B[0m, in \u001B[0;36mcosine\u001B[1;34m(u, v, w)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;124;03mCompute the Cosine distance between 1-D arrays.\u001B[39;00m\n\u001B[0;32m    630\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    665\u001B[0m \n\u001B[0;32m    666\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    667\u001B[0m \u001B[38;5;66;03m# cosine distance is also referred to as 'uncentered correlation',\u001B[39;00m\n\u001B[0;32m    668\u001B[0m \u001B[38;5;66;03m#   or 'reflective correlation'\u001B[39;00m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;66;03m# clamp the result to 0-2\u001B[39;00m\n\u001B[1;32m--> 670\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mmin\u001B[39m(\u001B[43mcorrelation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mu\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcentered\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m, \u001B[38;5;241m2.0\u001B[39m))\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:619\u001B[0m, in \u001B[0;36mcorrelation\u001B[1;34m(u, v, w, centered)\u001B[0m\n\u001B[0;32m    617\u001B[0m     u \u001B[38;5;241m=\u001B[39m u \u001B[38;5;241m-\u001B[39m umu\n\u001B[0;32m    618\u001B[0m     v \u001B[38;5;241m=\u001B[39m v \u001B[38;5;241m-\u001B[39m vmu\n\u001B[1;32m--> 619\u001B[0m uv \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39maverage(\u001B[43mu\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m, weights\u001B[38;5;241m=\u001B[39mw)\n\u001B[0;32m    620\u001B[0m uu \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39maverage(np\u001B[38;5;241m.\u001B[39msquare(u), weights\u001B[38;5;241m=\u001B[39mw)\n\u001B[0;32m    621\u001B[0m vv \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39maverage(np\u001B[38;5;241m.\u001B[39msquare(v), weights\u001B[38;5;241m=\u001B[39mw)\n",
      "\u001B[1;31mValueError\u001B[0m: operands could not be broadcast together with shapes (768,) (1536,) "
     ]
    }
   ],
   "source": [
    "CompVisionBERT = ChatBot(\"Computer Vision\", 'assets/' + BERT_KNOWLEDGE_FILENAME)\n",
    "print(Query.ask_bert('When did universities begin teaching Computer Vision?', CompVisionBERT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96c7a743564c4afda124d34911ac58d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [20]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mQuery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mask_bart\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mWhen did Universities begin teaching Computer Vision?\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCompVisionBERT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_source\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.ask_bart\u001B[1;34m(cls, query_text, chatbot_instance, show_source, confidence_level)\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQuestion is too long, please try again with a shorter question.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    214\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(query_text, chatbot_instance)\n\u001B[1;32m--> 215\u001B[0m response_message \u001B[38;5;241m=\u001B[39m \u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_bart_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfidence_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfidence_level\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m show_source \u001B[38;5;129;01mand\u001B[39;00m response_message\u001B[38;5;241m!=\u001B[39mANSWER_NOT_FOUND_MSG: \u001B[38;5;66;03m# Display the sources used:\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     response_message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m query\u001B[38;5;241m.\u001B[39mshow_source_message()\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.get_bart_output\u001B[1;34m(self, encoding_model, bert_model, confidence_level)\u001B[0m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_bart_output\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    171\u001B[0m                     \u001B[38;5;66;03m# chatbot_instance: ChatBot,\u001B[39;00m\n\u001B[0;32m    172\u001B[0m                     \u001B[38;5;66;03m# embedding_model: str = BART_EMBEDDING_MODEL,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    175\u001B[0m                     confidence_level: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m,\n\u001B[0;32m    176\u001B[0m                     ):\n\u001B[1;32m--> 177\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mknowledge_ranked_by_similarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfidence_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfidence_level\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mknowledge_used)\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m ANSWER_NOT_FOUND_MSG\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.knowledge_ranked_by_similarity\u001B[1;34m(self, max_num_sections, confidence_level, embedding_model)\u001B[0m\n\u001B[0;32m     28\u001B[0m query_embedding_response \u001B[38;5;241m=\u001B[39m get_embedding(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontent, embedding_model\u001B[38;5;241m=\u001B[39mBERT_EMBEDDING_MODEL)\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m embedding_model \u001B[38;5;241m==\u001B[39m GPT_EMBEDDING_MODEL:\n\u001B[1;32m---> 30\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[43mquery_embedding_response\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# knowledge_with_similarities[\"similarity\"] = knowledge_with_similarities[\"Embedding\"].apply(lambda x: self.similarity(query_embedding, x))\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(query_embedding_response)\n",
      "\u001B[1;31mIndexError\u001B[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\n",
    "print(Query.ask_bart('When did Universities begin teaching Computer Vision?', CompVisionGPT, show_source=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[6.7410e+22, 2.6729e+23, 5.3689e-05],\n",
      "        [1.3542e-05, 5.2905e-08, 6.7942e-07]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def describe(x):\n",
    "    print(\"Type: {}\".format(x.type()))\n",
    "    print(\"Shape/size: {}\".format(x.shape))\n",
    "    print(\"Values: \\n{}\".format(x))\n",
    "\n",
    "describe(torch.Tensor(2, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tasks\n",
    "Clean the parsed text so we have pure english words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", '’', 'crazy', 'ones', ',', 'misfits', ',', 'rebels', ',', 'troublemakers', ',', 'round', 'pegs', 'square', 'holes', '.', 'ones', 'see', 'things', 'differently', '—', '’', 'fond', 'rules', '.', 'quote', ',', 'disagree', ',', 'glorify', 'vilify', ',', 'thing', '’', 'ignore', 'change', 'things', '.', 'push', 'human', 'race', 'forward', ',', 'may', 'see', 'crazy', 'ones', ',', 'see', 'genius', ',', 'ones', 'crazy', 'enough', 'think', 'change', 'world', ',', 'ones', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"It's Here’s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. The ones who see things differently — they’re not fond of rules. You can quote them, disagree with them, glorify or vilify them, but the only thing you can’t do is ignore them because they change things. They push the human race forward, and while some may see them as the crazy ones, we see genius, because the ones who are crazy enough to think that they can change the world, are the ones who do.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "['It',\n \"'s\",\n 'Here',\n '’',\n 's',\n 'to',\n 'the',\n 'crazy',\n 'ones',\n ',',\n 'the',\n 'misfits',\n ',',\n 'the',\n 'rebels',\n ',',\n 'the',\n 'troublemakers',\n ',',\n 'the',\n 'round',\n 'pegs',\n 'in',\n 'the',\n 'square',\n 'holes',\n '.',\n 'The',\n 'ones',\n 'who',\n 'see',\n 'things',\n 'differently',\n '—',\n 'they',\n '’',\n 're',\n 'not',\n 'fond',\n 'of',\n 'rules',\n '.',\n 'You',\n 'can',\n 'quote',\n 'them',\n ',',\n 'disagree',\n 'with',\n 'them',\n ',',\n 'glorify',\n 'or',\n 'vilify',\n 'them',\n ',',\n 'but',\n 'the',\n 'only',\n 'thing',\n 'you',\n 'can',\n '’',\n 't',\n 'do',\n 'is',\n 'ignore',\n 'them',\n 'because',\n 'they',\n 'change',\n 'things',\n '.',\n 'They',\n 'push',\n 'the',\n 'human',\n 'race',\n 'forward',\n ',',\n 'and',\n 'while',\n 'some',\n 'may',\n 'see',\n 'them',\n 'as',\n 'the',\n 'crazy',\n 'ones',\n ',',\n 'we',\n 'see',\n 'genius',\n ',',\n 'because',\n 'the',\n 'ones',\n 'who',\n 'are',\n 'crazy',\n 'enough',\n 'to',\n 'think',\n 'that',\n 'they',\n 'can',\n 'change',\n 'the',\n 'world',\n ',',\n 'are',\n 'the',\n 'ones',\n 'who',\n 'do',\n '.']"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "sentences = [\n",
    "\"Artificial intelligence (AI) is revolutionising industries and transforming the way we live and work.\",\n",
    "\"Machine learning, a subset of AI, enables computers to learn and make predictions or decisions without being explicitly programmed.\",\n",
    "\"AI algorithms can analyse vast amounts of data to uncover patterns, detect anomalies, and extract valuable insights.\",\n",
    "\"Natural language processing (NLP) allows machines to understand and process human language, facilitating communication between humans and computers.\",\n",
    "\"Computer vision enables machines to interpret and understand visual information, enabling applications such as image recognition and object detection.\",\n",
    "\"AI-powered virtual assistants like Siri, Alexa, and Google Assistant are becoming increasingly common, providing voice-based interactions and personalised experiences.\",\n",
    "\"AI is driving advancements in healthcare, including disease diagnosis, drug discovery, and personalised medicine, leading to improved patient outcomes.\",\n",
    "\"In the field of autonomous vehicles, AI plays a crucial role in enabling self-driving cars to perceive their surroundings and make informed decisions.\",\n",
    "\"AI is transforming the customer service industry by utilising chatbots and automated systems to provide faster and more efficient support to customers.\",\n",
    "\"Ethical considerations, such as transparency, fairness, and privacy, are essential in the development and deployment of AI systems to ensure responsible and accountable use.\",\n",
    "\"Cars have revolutionised transportation, providing a convenient and efficient means of travel for people around the world.\",\n",
    "\"Automobile manufacturing involves a complex process of designing, engineering, and assembling various components to create a functional vehicle.\",\n",
    "\"Safety features such as seat belts, airbags, and anti-lock braking systems have greatly improved the overall safety of cars.\",\n",
    "\"Electric vehicles (EVs) are gaining popularity as eco-friendly alternatives to traditional petrol-powered cars, reducing carbon emissions and dependence on fossil fuels.\",\n",
    "\"Advanced driver-assistance systems (ADAS) enhance car safety by incorporating technologies like adaptive cruise control and lane-keeping assist.\",\n",
    "\"Sports cars are known for their high-performance capabilities, offering speed, agility, and an exhilarating driving experience.\",\n",
    "\"Classic cars hold a special place in automotive history, with their timeless designs and nostalgic appeal capturing the hearts of car enthusiasts.\",\n",
    "\"Car customisation allows owners to personalise their vehicles, from unique paint jobs and body modifications to performance upgrades.\",\n",
    "\"Car-sharing services and ride-hailing apps have transformed the way people access transportation, providing convenient alternatives to car ownership.\",\n",
    "\"The future of cars is expected to bring autonomous vehicles, where cars can navigate and operate without human intervention, promising increased safety and efficiency.\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_text(text: str, remove_stopwords: bool) -> str:\n",
    "    \"\"\"Function that cleans the input text by going to:\n",
    "    - remove links\n",
    "    - remove special characters\n",
    "    - remove numbers\n",
    "    - remove stopwords\n",
    "    - convert to lowercase\n",
    "    - remove excessive white spaces\n",
    "    Arguments:\n",
    "        text (str): text to clean\n",
    "        remove_stopwords (bool): whether to remove stopwords\n",
    "    Returns:\n",
    "        str: cleaned text\n",
    "    \"\"\"\n",
    "    # remove links\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # remove numbers and special characters\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
    "    # remove stopwords\n",
    "    if remove_stopwords:\n",
    "        # 1. create tokens\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # 2. check if it's a stopword\n",
    "        tokens = [w.lower().strip() for w in tokens if not w.lower() in stopwords.words(\"english\")]\n",
    "        # return a list of cleaned tokens\n",
    "        return tokens\n",
    "\n",
    "tokenised_sentences = [preprocess_text(sentence, remove_stopwords=True) for sentence in sentences]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "[['artificial',\n  'intelligence',\n  'ai',\n  'revolutionising',\n  'industries',\n  'transforming',\n  'way',\n  'live',\n  'work'],\n ['machine',\n  'learning',\n  'subset',\n  'ai',\n  'enables',\n  'computers',\n  'learn',\n  'make',\n  'predictions',\n  'decisions',\n  'without',\n  'explicitly',\n  'programmed'],\n ['ai',\n  'algorithms',\n  'analyse',\n  'vast',\n  'amounts',\n  'data',\n  'uncover',\n  'patterns',\n  'detect',\n  'anomalies',\n  'extract',\n  'valuable',\n  'insights'],\n ['natural',\n  'language',\n  'processing',\n  'nlp',\n  'allows',\n  'machines',\n  'understand',\n  'process',\n  'human',\n  'language',\n  'facilitating',\n  'communication',\n  'humans',\n  'computers'],\n ['computer',\n  'vision',\n  'enables',\n  'machines',\n  'interpret',\n  'understand',\n  'visual',\n  'information',\n  'enabling',\n  'applications',\n  'image',\n  'recognition',\n  'object',\n  'detection'],\n ['ai',\n  'powered',\n  'virtual',\n  'assistants',\n  'like',\n  'siri',\n  'alexa',\n  'google',\n  'assistant',\n  'becoming',\n  'increasingly',\n  'common',\n  'providing',\n  'voice',\n  'based',\n  'interactions',\n  'personalised',\n  'experiences'],\n ['ai',\n  'driving',\n  'advancements',\n  'healthcare',\n  'including',\n  'disease',\n  'diagnosis',\n  'drug',\n  'discovery',\n  'personalised',\n  'medicine',\n  'leading',\n  'improved',\n  'patient',\n  'outcomes'],\n ['field',\n  'autonomous',\n  'vehicles',\n  'ai',\n  'plays',\n  'crucial',\n  'role',\n  'enabling',\n  'self',\n  'driving',\n  'cars',\n  'perceive',\n  'surroundings',\n  'make',\n  'informed',\n  'decisions'],\n ['ai',\n  'transforming',\n  'customer',\n  'service',\n  'industry',\n  'utilising',\n  'chatbots',\n  'automated',\n  'systems',\n  'provide',\n  'faster',\n  'efficient',\n  'support',\n  'customers'],\n ['ethical',\n  'considerations',\n  'transparency',\n  'fairness',\n  'privacy',\n  'essential',\n  'development',\n  'deployment',\n  'ai',\n  'systems',\n  'ensure',\n  'responsible',\n  'accountable',\n  'use'],\n ['cars',\n  'revolutionised',\n  'transportation',\n  'providing',\n  'convenient',\n  'efficient',\n  'means',\n  'travel',\n  'people',\n  'around',\n  'world'],\n ['automobile',\n  'manufacturing',\n  'involves',\n  'complex',\n  'process',\n  'designing',\n  'engineering',\n  'assembling',\n  'various',\n  'components',\n  'create',\n  'functional',\n  'vehicle'],\n ['safety',\n  'features',\n  'seat',\n  'belts',\n  'airbags',\n  'anti',\n  'lock',\n  'braking',\n  'systems',\n  'greatly',\n  'improved',\n  'overall',\n  'safety',\n  'cars'],\n ['electric',\n  'vehicles',\n  'evs',\n  'gaining',\n  'popularity',\n  'eco',\n  'friendly',\n  'alternatives',\n  'traditional',\n  'petrol',\n  'powered',\n  'cars',\n  'reducing',\n  'carbon',\n  'emissions',\n  'dependence',\n  'fossil',\n  'fuels'],\n ['advanced',\n  'driver',\n  'assistance',\n  'systems',\n  'adas',\n  'enhance',\n  'car',\n  'safety',\n  'incorporating',\n  'technologies',\n  'like',\n  'adaptive',\n  'cruise',\n  'control',\n  'lane',\n  'keeping',\n  'assist'],\n ['sports',\n  'cars',\n  'known',\n  'high',\n  'performance',\n  'capabilities',\n  'offering',\n  'speed',\n  'agility',\n  'exhilarating',\n  'driving',\n  'experience'],\n ['classic',\n  'cars',\n  'hold',\n  'special',\n  'place',\n  'automotive',\n  'history',\n  'timeless',\n  'designs',\n  'nostalgic',\n  'appeal',\n  'capturing',\n  'hearts',\n  'car',\n  'enthusiasts'],\n ['car',\n  'customisation',\n  'allows',\n  'owners',\n  'personalise',\n  'vehicles',\n  'unique',\n  'paint',\n  'jobs',\n  'body',\n  'modifications',\n  'performance',\n  'upgrades'],\n ['car',\n  'sharing',\n  'services',\n  'ride',\n  'hailing',\n  'apps',\n  'transformed',\n  'way',\n  'people',\n  'access',\n  'transportation',\n  'providing',\n  'convenient',\n  'alternatives',\n  'car',\n  'ownership'],\n ['future',\n  'cars',\n  'expected',\n  'bring',\n  'autonomous',\n  'vehicles',\n  'cars',\n  'navigate',\n  'operate',\n  'without',\n  'human',\n  'intervention',\n  'promising',\n  'increased',\n  'safety',\n  'efficiency']]"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised_sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "[['artificial',\n  'intelligence',\n  'ai',\n  'revolutionise',\n  'industry',\n  'transform',\n  'way',\n  'live',\n  'work'],\n ['machine',\n  'learn',\n  'subset',\n  'ai',\n  'enable',\n  'computer',\n  'learn',\n  'make',\n  'prediction',\n  'decision',\n  'without',\n  'explicitly',\n  'program'],\n ['ai',\n  'algorithm',\n  'analyse',\n  'vast',\n  'amount',\n  'data',\n  'uncover',\n  'pattern',\n  'detect',\n  'anomaly',\n  'extract',\n  'valuable',\n  'insight'],\n ['natural',\n  'language',\n  'processing',\n  'nlp',\n  'allow',\n  'machine',\n  'understand',\n  'process',\n  'human',\n  'language',\n  'facilitate',\n  'communication',\n  'human',\n  'computer'],\n ['computer',\n  'vision',\n  'enable',\n  'machine',\n  'interpret',\n  'understand',\n  'visual',\n  'information',\n  'enable',\n  'application',\n  'image',\n  'recognition',\n  'object',\n  'detection'],\n ['ai',\n  'power',\n  'virtual',\n  'assistant',\n  'like',\n  'siri',\n  'alexa',\n  'google',\n  'assistant',\n  'become',\n  'increasingly',\n  'common',\n  'provide',\n  'voice',\n  'base',\n  'interaction',\n  'personalise',\n  'experience'],\n ['ai',\n  'drive',\n  'advancement',\n  'healthcare',\n  'include',\n  'disease',\n  'diagnosis',\n  'drug',\n  'discovery',\n  'personalise',\n  'medicine',\n  'lead',\n  'improved',\n  'patient',\n  'outcome'],\n ['field',\n  'autonomous',\n  'vehicle',\n  'ai',\n  'play',\n  'crucial',\n  'role',\n  'enable',\n  'self',\n  'drive',\n  'car',\n  'perceive',\n  'surroundings',\n  'make',\n  'informed',\n  'decision'],\n ['ai',\n  'transform',\n  'customer',\n  'service',\n  'industry',\n  'utilise',\n  'chatbots',\n  'automate',\n  'system',\n  'provide',\n  'faster',\n  'efficient',\n  'support',\n  'customer'],\n ['ethical',\n  'consideration',\n  'transparency',\n  'fairness',\n  'privacy',\n  'essential',\n  'development',\n  'deployment',\n  'ai',\n  'system',\n  'ensure',\n  'responsible',\n  'accountable',\n  'use'],\n ['car',\n  'revolutionise',\n  'transportation',\n  'provide',\n  'convenient',\n  'efficient',\n  'mean',\n  'travel',\n  'people',\n  'around',\n  'world'],\n ['automobile',\n  'manufacture',\n  'involves',\n  'complex',\n  'process',\n  'design',\n  'engineering',\n  'assemble',\n  'various',\n  'component',\n  'create',\n  'functional',\n  'vehicle'],\n ['safety',\n  'feature',\n  'seat',\n  'belt',\n  'airbags',\n  'anti',\n  'lock',\n  'brake',\n  'system',\n  'greatly',\n  'improve',\n  'overall',\n  'safety',\n  'car'],\n ['electric',\n  'vehicle',\n  'evs',\n  'gain',\n  'popularity',\n  'eco',\n  'friendly',\n  'alternative',\n  'traditional',\n  'petrol',\n  'power',\n  'car',\n  'reduce',\n  'carbon',\n  'emission',\n  'dependence',\n  'fossil',\n  'fuel'],\n ['advanced',\n  'driver',\n  'assistance',\n  'system',\n  'adas',\n  'enhance',\n  'car',\n  'safety',\n  'incorporate',\n  'technology',\n  'like',\n  'adaptive',\n  'cruise',\n  'control',\n  'lane',\n  'keep',\n  'assist'],\n ['sport',\n  'car',\n  'know',\n  'high',\n  'performance',\n  'capability',\n  'offer',\n  'speed',\n  'agility',\n  'exhilarate',\n  'drive',\n  'experience'],\n ['classic',\n  'car',\n  'hold',\n  'special',\n  'place',\n  'automotive',\n  'history',\n  'timeless',\n  'design',\n  'nostalgic',\n  'appeal',\n  'capture',\n  'heart',\n  'car',\n  'enthusiast'],\n ['car',\n  'customisation',\n  'allow',\n  'owner',\n  'personalise',\n  'vehicle',\n  'unique',\n  'paint',\n  'job',\n  'body',\n  'modification',\n  'performance',\n  'upgrade'],\n ['car',\n  'share',\n  'service',\n  'ride',\n  'hail',\n  'apps',\n  'transform',\n  'way',\n  'people',\n  'access',\n  'transportation',\n  'provide',\n  'convenient',\n  'alternative',\n  'car',\n  'ownership'],\n ['future',\n  'car',\n  'expect',\n  'bring',\n  'autonomous',\n  'vehicle',\n  'car',\n  'navigate',\n  'operate',\n  'without',\n  'human',\n  'intervention',\n  'promising',\n  'increase',\n  'safety',\n  'efficiency']]"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN # Default to noun if the POS tag is unknown/ambiguous\n",
    "\n",
    "def lemmatize_sentence(tokenised_sentence):\n",
    "    tagged_tokens = nltk.pos_tag(tokenised_sentence)\n",
    "    lemmatized_words = []\n",
    "    for word, tag in tagged_tokens:\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(word, pos=pos))\n",
    "    return lemmatized_words\n",
    "\n",
    "# refined_sentences = [sentence.strip()]\n",
    "lemmatised_sentences = [lemmatize_sentence(s) for s in tokenised_sentences]\n",
    "lemmatised_sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "VECTOR_SIZE = 100\n",
    "MIN_COUNT = 1\n",
    "WINDOW = 3\n",
    "SG = 1\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=lemmatised_sentences,\n",
    "    vector_size=VECTOR_SIZE,\n",
    "    min_count=MIN_COUNT,\n",
    "    sg=SG\n",
    ")\n",
    "\n",
    "model = Word2Vec(sentences=lemmatised_sentences, min_count=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.052282296"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('ai', 'algorithm')\n",
    "\n",
    "# model.wv.most_similar(positive=['ai'], topn=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\point\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning:\n",
      "\n",
      "The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "\n",
      "C:\\Users\\point\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning:\n",
      "\n",
      "The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "markers",
         "text": [
          "cars",
          "ai",
          "car",
          "vehicles",
          "systems",
          "safety",
          "driving",
          "providing",
          "personalized",
          "like",
          "enabling",
          "alternatives",
          "people",
          "performance",
          "improved",
          "human",
          "process",
          "understand",
          "machines",
          "allows",
          "language",
          "powered",
          "autonomous",
          "way",
          "convenient",
          "enables",
          "efficient",
          "transforming",
          "transportation",
          "computers",
          "without",
          "make",
          "decisions",
          "advancements",
          "diagnosis",
          "provide",
          "disease",
          "including",
          "healthcare",
          "customer",
          "faster",
          "discovery",
          "experiences",
          "interactions",
          "based",
          "voice",
          "support",
          "customers",
          "drug",
          "chatbots",
          "medicine",
          "leading",
          "informed",
          "surroundings",
          "perceive",
          "self",
          "role",
          "crucial",
          "plays",
          "increasingly",
          "field",
          "outcomes",
          "industry",
          "utilizing",
          "patient",
          "service",
          "automated",
          "common",
          "efficiency",
          "becoming",
          "valuable",
          "anomalies",
          "detect",
          "patterns",
          "uncover",
          "data",
          "amounts",
          "vast",
          "analyze",
          "algorithms",
          "programmed",
          "explicitly",
          "predictions",
          "learn",
          "subset",
          "learning",
          "machine",
          "work",
          "live",
          "industries",
          "revolutionizing",
          "intelligence",
          "extract",
          "insights",
          "assistant",
          "natural",
          "google",
          "alexa",
          "siri",
          "assistants",
          "virtual",
          "considerations",
          "detection",
          "object",
          "recognition",
          "image",
          "applications",
          "information",
          "visual",
          "interpret",
          "vision",
          "computer",
          "humans",
          "communication",
          "facilitating",
          "nlp",
          "processing",
          "ethical",
          "essential",
          "transparency",
          "agility",
          "nostalgic",
          "designs",
          "timeless",
          "history",
          "automotive",
          "place",
          "special",
          "hold",
          "classic",
          "experience",
          "thrilling",
          "speed",
          "capturing",
          "offering",
          "capabilities",
          "high",
          "known",
          "sports",
          "assist",
          "keeping",
          "lane",
          "control",
          "cruise",
          "adaptive",
          "appeal",
          "hearts",
          "fairness",
          "hailing",
          "promising",
          "intervention",
          "operate",
          "navigate",
          "bring",
          "expected",
          "future",
          "ownership",
          "access",
          "transformed",
          "apps",
          "ride",
          "enthusiasts",
          "services",
          "sharing",
          "upgrades",
          "modifications",
          "body",
          "jobs",
          "paint",
          "unique",
          "personalize",
          "owners",
          "customization",
          "technologies",
          "incorporating",
          "enhance",
          "world",
          "functional",
          "create",
          "components",
          "various",
          "assembling",
          "engineering",
          "designing",
          "complex",
          "involves",
          "manufacturing",
          "automobile",
          "around",
          "adas",
          "travel",
          "means",
          "revolutionized",
          "use",
          "accountable",
          "responsible",
          "ensure",
          "deployment",
          "development",
          "increased",
          "privacy",
          "vehicle",
          "features",
          "seat",
          "belts",
          "assistance",
          "driver",
          "advanced",
          "fuels",
          "fossil",
          "dependence",
          "emissions",
          "carbon",
          "reducing",
          "gasoline",
          "traditional",
          "friendly",
          "eco",
          "popularity",
          "gaining",
          "evs",
          "electric",
          "overall",
          "greatly",
          "braking",
          "lock",
          "anti",
          "airbags",
          "artificial"
         ],
         "x": [
          -3.5569868087768555,
          -3.345669984817505,
          -1.1631700992584229,
          -4.823192596435547,
          1.4799708127975464,
          -2.5234670639038086,
          3.3336856365203857,
          4.033675193786621,
          -1.097055435180664,
          1.8911492824554443,
          -4.095557689666748,
          -6.807097434997559,
          3.94748592376709,
          1.8565915822982788,
          -6.009923458099365,
          0.7721247673034668,
          -2.0102412700653076,
          -7.494566917419434,
          -5.4318718910217285,
          4.244304180145264,
          1.411681890487671,
          5.025524139404297,
          -3.7039387226104736,
          -8.411649703979492,
          -0.7734807133674622,
          0.2362508475780487,
          2.539975643157959,
          -1.1920018196105957,
          7.879120826721191,
          -4.905570030212402,
          4.294675827026367,
          7.35902738571167,
          -6.065314292907715,
          5.191819190979004,
          -5.435687065124512,
          0.6472213864326477,
          8.186429023742676,
          3.4760055541992188,
          6.416111946105957,
          3.8564202785491943,
          0.012898512184619904,
          -2.008552312850952,
          -3.6230485439300537,
          -4.542641639709473,
          7.342559814453125,
          -0.9007460474967957,
          -5.753945827484131,
          -2.073754072189331,
          -1.8793163299560547,
          -1.26099693775177,
          -5.374608993530273,
          -6.9775896072387695,
          3.9699244499206543,
          1.5151407718658447,
          4.543593883514404,
          -4.996665954589844,
          -6.796648025512695,
          -0.796980082988739,
          -5.379490852355957,
          -0.549635648727417,
          -9.417557716369629,
          -2.632219076156616,
          1.7778784036636353,
          1.6346489191055298,
          -7.801143169403076,
          7.2114739418029785,
          -9.09225082397461,
          0.9202542901039124,
          -1.1646091938018799,
          2.6558477878570557,
          -5.547433853149414,
          1.9413443803787231,
          -3.9593071937561035,
          2.2311630249023438,
          1.97158944606781,
          6.333553791046143,
          -2.814406633377075,
          0.1673775613307953,
          -8.345366477966309,
          3.1074397563934326,
          -3.0089330673217773,
          -4.5951337814331055,
          -3.7368662357330322,
          -5.906049728393555,
          0.728748619556427,
          0.3020936846733093,
          -1.9193578958511353,
          -3.302121877670288,
          -2.283527135848999,
          1.0822992324829102,
          1.084127426147461,
          0.7212051153182983,
          -0.2904839813709259,
          0.0067567480728030205,
          -6.094676494598389,
          -0.402170330286026,
          0.21640515327453613,
          5.0021562576293945,
          -2.4156148433685303,
          4.747771739959717,
          3.1189804077148438,
          6.701613903045654,
          7.1602253913879395,
          6.287492275238037,
          0.8026138544082642,
          6.452852725982666,
          -2.5331742763519287,
          -4.900890827178955,
          2.9736883640289307,
          4.881409645080566,
          -1.4666893482208252,
          -4.851039409637451,
          2.059319257736206,
          2.250230073928833,
          3.969353437423706,
          3.364682912826538,
          -0.05929283797740936,
          -0.6684280037879944,
          5.829762935638428,
          -0.979670524597168,
          -3.566016912460327,
          -1.4442508220672607,
          -2.588890552520752,
          -3.3296337127685547,
          -2.7897987365722656,
          6.3759918212890625,
          1.5154504776000977,
          -8.110496520996094,
          0.17165987193584442,
          -6.858039855957031,
          -3.5445096492767334,
          2.5935659408569336,
          2.885282516479492,
          1.8959251642227173,
          -5.664699077606201,
          -3.4687421321868896,
          -2.4294581413269043,
          -7.189571857452393,
          -2.8525397777557373,
          5.089686870574951,
          0.6043485999107361,
          3.9126408100128174,
          -2.7547128200531006,
          -5.008999824523926,
          -3.657480001449585,
          -9.955078125,
          3.3863651752471924,
          -3.036243200302124,
          4.4467034339904785,
          -0.05079968646168709,
          -0.4907218813896179,
          -3.036567211151123,
          4.6397705078125,
          1.6668617725372314,
          -1.2682503461837769,
          1.3137140274047852,
          -6.264095783233643,
          4.439817905426025,
          -6.315651893615723,
          -5.666043281555176,
          -0.6006585955619812,
          5.461076259613037,
          -0.24642640352249146,
          -2.679663896560669,
          -3.422029495239258,
          -0.013748754747211933,
          4.076957702636719,
          6.95869255065918,
          2.3676674365997314,
          -8.981572151184082,
          -8.151212692260742,
          3.518017053604126,
          -0.759570300579071,
          -0.42632266879081726,
          7.180819034576416,
          1.6771918535232544,
          -3.5160317420959473,
          0.8269298076629639,
          1.8569263219833374,
          2.010329484939575,
          -3.7082324028015137,
          6.012582302093506,
          4.621157169342041,
          0.9195900559425354,
          -1.7041544914245605,
          3.4600207805633545,
          2.8068747520446777,
          -4.070095062255859,
          3.804140567779541,
          -3.074394702911377,
          -4.536877155303955,
          -1.7962968349456787,
          -2.7294981479644775,
          2.9486894607543945,
          -6.354874134063721,
          -8.988039016723633,
          8.563080787658691,
          6.317517280578613,
          -0.9279513359069824,
          -1.9361271858215332,
          -9.039132118225098,
          7.131534099578857,
          2.7934367656707764,
          -5.968392848968506,
          -0.06545160710811615,
          3.7102925777435303,
          2.258908987045288,
          4.861087799072266,
          -0.05108696222305298,
          -4.49846887588501,
          -4.576083660125732,
          -6.251190185546875,
          -3.3793859481811523,
          1.4788758754730225,
          4.066930294036865,
          -0.9180363416671753,
          5.740225315093994,
          -1.3265256881713867,
          5.262291431427002,
          0.7175686359405518,
          2.9360263347625732,
          -7.619258880615234,
          5.261261463165283,
          1.1680265665054321,
          -4.450006008148193,
          5.410119533538818,
          -3.2600202560424805,
          4.232682228088379,
          -6.257768154144287
         ],
         "y": [
          -2.366219997406006,
          1.4157464504241943,
          -8.381683349609375,
          -4.209931373596191,
          -5.979645252227783,
          -9.352097511291504,
          -1.8068146705627441,
          -7.144293785095215,
          6.021297931671143,
          6.609764575958252,
          -1.495269536972046,
          -0.639616072177887,
          -1.1818455457687378,
          -6.758322715759277,
          1.8673317432403564,
          1.815646767616272,
          0.16977642476558685,
          0.9171539545059204,
          -0.28289005160331726,
          -7.2281694412231445,
          0.7534630298614502,
          1.7798832654953003,
          2.778804302215576,
          3.790527105331421,
          -5.351175785064697,
          -3.560941457748413,
          4.08440637588501,
          5.833236217498779,
          -4.1663336753845215,
          0.3943403661251068,
          4.310480117797852,
          0.09831395000219345,
          0.24064360558986664,
          -1.9963525533676147,
          -1.881211519241333,
          4.180385589599609,
          -0.9039490222930908,
          -1.5471729040145874,
          0.5280646681785583,
          2.6742937564849854,
          4.0620808601379395,
          -6.513219356536865,
          -9.472671508789062,
          -1.1084809303283691,
          2.9618706703186035,
          4.339135646820068,
          -5.158017635345459,
          -5.029111862182617,
          4.206965923309326,
          2.9538791179656982,
          5.694890022277832,
          -0.4904268980026245,
          6.932773113250732,
          7.793333053588867,
          -4.360994338989258,
          4.613007545471191,
          2.7395997047424316,
          0.7967098355293274,
          3.114056348800659,
          -10.288593292236328,
          -1.2579964399337769,
          8.089855194091797,
          -2.3647656440734863,
          -3.7294106483459473,
          -2.29291033744812,
          2.8865668773651123,
          -3.3214125633239746,
          -7.906168460845947,
          -8.579545021057129,
          -7.941169261932373,
          -5.88237190246582,
          -2.719790458679199,
          -2.8582077026367188,
          -5.134355545043945,
          1.7824652194976807,
          6.741681098937988,
          5.792266845703125,
          -2.9503769874572754,
          -5.39525842666626,
          -5.694397926330566,
          5.918738842010498,
          -2.7563304901123047,
          -3.7066729068756104,
          -8.668044090270996,
          -5.506378650665283,
          -4.30855131149292,
          2.2824041843414307,
          0.5046464800834656,
          -5.011874675750732,
          -9.552413940429688,
          -7.896369457244873,
          0.7081783413887024,
          3.2146520614624023,
          -6.3050384521484375,
          -2.3505775928497314,
          -4.481626510620117,
          -1.7087410688400269,
          -4.868049621582031,
          -1.1532683372497559,
          0.6817308068275452,
          3.0158650875091553,
          1.4656099081039429,
          -7.470815658569336,
          -3.990213632583618,
          2.7786660194396973,
          -5.667232513427734,
          2.627997875213623,
          1.2886537313461304,
          4.768289566040039,
          -0.20373012125492096,
          -1.107700228691101,
          2.234194040298462,
          2.0606625080108643,
          -0.11353302001953125,
          -4.433785915374756,
          -0.016857877373695374,
          -6.968067646026611,
          -1.3603588342666626,
          -4.8725175857543945,
          -0.11705698072910309,
          -5.689452648162842,
          -3.5955538749694824,
          5.520202159881592,
          -1.0861376523971558,
          2.7821645736694336,
          -1.5905102491378784,
          -0.9805388450622559,
          -5.05893087387085,
          0.7785710096359253,
          4.528586387634277,
          -6.244285583496094,
          -7.922395706176758,
          1.2706235647201538,
          -4.036782741546631,
          -3.101402521133423,
          -8.064379692077637,
          -2.1249098777770996,
          -2.389819622039795,
          -3.1695380210876465,
          4.600878715515137,
          -1.2858613729476929,
          -9.436290740966797,
          -2.359637975692749,
          0.10975052416324615,
          3.9018781185150146,
          0.8384222984313965,
          -8.780622482299805,
          0.42526963353157043,
          1.231698751449585,
          -0.5188021063804626,
          -2.5711145401000977,
          -1.3426936864852905,
          -7.813931465148926,
          -3.1462206840515137,
          -2.4722371101379395,
          -1.0157849788665771,
          -4.328769207000732,
          6.737553596496582,
          -6.855597019195557,
          -2.030759572982788,
          -4.499425411224365,
          1.6545190811157227,
          6.392116546630859,
          -3.9923131465911865,
          3.8821256160736084,
          6.069917678833008,
          -0.25490161776542664,
          -0.4267646372318268,
          1.0929652452468872,
          3.285449504852295,
          1.4321503639221191,
          1.9575225114822388,
          8.271814346313477,
          1.9291417598724365,
          -1.3109941482543945,
          7.994966983795166,
          -5.386274814605713,
          4.301878452301025,
          2.89518141746521,
          6.410716533660889,
          6.38617467880249,
          -1.6168067455291748,
          6.841063976287842,
          0.05820506066083908,
          1.640087604522705,
          -2.8759775161743164,
          4.6040825843811035,
          -6.035190105438232,
          2.152789831161499,
          -3.5014779567718506,
          4.5739216804504395,
          -3.7116336822509766,
          0.06453826278448105,
          -5.643316268920898,
          -3.0912952423095703,
          -1.396384835243225,
          -2.5754761695861816,
          -3.2645411491394043,
          4.257584095001221,
          -6.593636989593506,
          -3.2896602153778076,
          -7.411569118499756,
          -1.3092409372329712,
          -6.6785125732421875,
          1.808240294456482,
          -0.3113737404346466,
          5.845041751861572,
          3.1287853717803955,
          -2.0365591049194336,
          -4.636128902435303,
          2.706270456314087,
          6.708128452301025,
          -9.077188491821289,
          -1.6567959785461426,
          -4.118268013000488,
          0.3064400553703308,
          -0.6685023903846741,
          0.6971976161003113,
          4.906248569488525,
          0.9191274642944336,
          -2.744675636291504,
          -4.471009731292725,
          4.209408283233643,
          3.0564539432525635,
          -7.672701358795166,
          -2.7060420513153076,
          -7.385237693786621,
          -2.326617479324341,
          -1.1584327220916748
         ],
         "type": "scatter"
        }
       ],
       "layout": {
        "template": {
         "data": {
          "histogram2dcontour": [
           {
            "type": "histogram2dcontour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "choropleth": [
           {
            "type": "choropleth",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "histogram2d": [
           {
            "type": "histogram2d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmap": [
           {
            "type": "heatmap",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmapgl": [
           {
            "type": "heatmapgl",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "contourcarpet": [
           {
            "type": "contourcarpet",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "contour": [
           {
            "type": "contour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "surface": [
           {
            "type": "surface",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "mesh3d": [
           {
            "type": "mesh3d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "parcoords": [
           {
            "type": "parcoords",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolargl": [
           {
            "type": "scatterpolargl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "scattergeo": [
           {
            "type": "scattergeo",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolar": [
           {
            "type": "scatterpolar",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "scattergl": [
           {
            "type": "scattergl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatter3d": [
           {
            "type": "scatter3d",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermapbox": [
           {
            "type": "scattermapbox",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterternary": [
           {
            "type": "scatterternary",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattercarpet": [
           {
            "type": "scattercarpet",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ]
         },
         "layout": {
          "autotypenumbers": "strict",
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "hovermode": "closest",
          "hoverlabel": {
           "align": "left"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "bgcolor": "#E5ECF6",
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "ternary": {
           "bgcolor": "#E5ECF6",
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "sequential": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ]
          },
          "xaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "automargin": true,
           "zerolinewidth": 2
          },
          "yaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "automargin": true,
           "zerolinewidth": 2
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "geo": {
           "bgcolor": "white",
           "landcolor": "#E5ECF6",
           "subunitcolor": "white",
           "showland": true,
           "showlakes": true,
           "lakecolor": "white"
          },
          "title": {
           "x": 0.05
          },
          "mapbox": {
           "style": "light"
          }
         }
        },
        "title": {
         "text": "Word2Vec - Visualising embeddings with TSNE"
        }
       },
       "config": {
        "plotlyServerURL": "https://plot.ly"
       }
      },
      "text/html": "<div>                            <div id=\"5c39eac1-352f-4e2a-af06-888c12c0a8bd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5c39eac1-352f-4e2a-af06-888c12c0a8bd\")) {                    Plotly.newPlot(                        \"5c39eac1-352f-4e2a-af06-888c12c0a8bd\",                        [{\"mode\":\"markers\",\"text\":[\"cars\",\"ai\",\"car\",\"vehicles\",\"systems\",\"safety\",\"driving\",\"providing\",\"personalized\",\"like\",\"enabling\",\"alternatives\",\"people\",\"performance\",\"improved\",\"human\",\"process\",\"understand\",\"machines\",\"allows\",\"language\",\"powered\",\"autonomous\",\"way\",\"convenient\",\"enables\",\"efficient\",\"transforming\",\"transportation\",\"computers\",\"without\",\"make\",\"decisions\",\"advancements\",\"diagnosis\",\"provide\",\"disease\",\"including\",\"healthcare\",\"customer\",\"faster\",\"discovery\",\"experiences\",\"interactions\",\"based\",\"voice\",\"support\",\"customers\",\"drug\",\"chatbots\",\"medicine\",\"leading\",\"informed\",\"surroundings\",\"perceive\",\"self\",\"role\",\"crucial\",\"plays\",\"increasingly\",\"field\",\"outcomes\",\"industry\",\"utilizing\",\"patient\",\"service\",\"automated\",\"common\",\"efficiency\",\"becoming\",\"valuable\",\"anomalies\",\"detect\",\"patterns\",\"uncover\",\"data\",\"amounts\",\"vast\",\"analyze\",\"algorithms\",\"programmed\",\"explicitly\",\"predictions\",\"learn\",\"subset\",\"learning\",\"machine\",\"work\",\"live\",\"industries\",\"revolutionizing\",\"intelligence\",\"extract\",\"insights\",\"assistant\",\"natural\",\"google\",\"alexa\",\"siri\",\"assistants\",\"virtual\",\"considerations\",\"detection\",\"object\",\"recognition\",\"image\",\"applications\",\"information\",\"visual\",\"interpret\",\"vision\",\"computer\",\"humans\",\"communication\",\"facilitating\",\"nlp\",\"processing\",\"ethical\",\"essential\",\"transparency\",\"agility\",\"nostalgic\",\"designs\",\"timeless\",\"history\",\"automotive\",\"place\",\"special\",\"hold\",\"classic\",\"experience\",\"thrilling\",\"speed\",\"capturing\",\"offering\",\"capabilities\",\"high\",\"known\",\"sports\",\"assist\",\"keeping\",\"lane\",\"control\",\"cruise\",\"adaptive\",\"appeal\",\"hearts\",\"fairness\",\"hailing\",\"promising\",\"intervention\",\"operate\",\"navigate\",\"bring\",\"expected\",\"future\",\"ownership\",\"access\",\"transformed\",\"apps\",\"ride\",\"enthusiasts\",\"services\",\"sharing\",\"upgrades\",\"modifications\",\"body\",\"jobs\",\"paint\",\"unique\",\"personalize\",\"owners\",\"customization\",\"technologies\",\"incorporating\",\"enhance\",\"world\",\"functional\",\"create\",\"components\",\"various\",\"assembling\",\"engineering\",\"designing\",\"complex\",\"involves\",\"manufacturing\",\"automobile\",\"around\",\"adas\",\"travel\",\"means\",\"revolutionized\",\"use\",\"accountable\",\"responsible\",\"ensure\",\"deployment\",\"development\",\"increased\",\"privacy\",\"vehicle\",\"features\",\"seat\",\"belts\",\"assistance\",\"driver\",\"advanced\",\"fuels\",\"fossil\",\"dependence\",\"emissions\",\"carbon\",\"reducing\",\"gasoline\",\"traditional\",\"friendly\",\"eco\",\"popularity\",\"gaining\",\"evs\",\"electric\",\"overall\",\"greatly\",\"braking\",\"lock\",\"anti\",\"airbags\",\"artificial\"],\"x\":[-3.5569868087768555,-3.345669984817505,-1.1631700992584229,-4.823192596435547,1.4799708127975464,-2.5234670639038086,3.3336856365203857,4.033675193786621,-1.097055435180664,1.8911492824554443,-4.095557689666748,-6.807097434997559,3.94748592376709,1.8565915822982788,-6.009923458099365,0.7721247673034668,-2.0102412700653076,-7.494566917419434,-5.4318718910217285,4.244304180145264,1.411681890487671,5.025524139404297,-3.7039387226104736,-8.411649703979492,-0.7734807133674622,0.2362508475780487,2.539975643157959,-1.1920018196105957,7.879120826721191,-4.905570030212402,4.294675827026367,7.35902738571167,-6.065314292907715,5.191819190979004,-5.435687065124512,0.6472213864326477,8.186429023742676,3.4760055541992188,6.416111946105957,3.8564202785491943,0.012898512184619904,-2.008552312850952,-3.6230485439300537,-4.542641639709473,7.342559814453125,-0.9007460474967957,-5.753945827484131,-2.073754072189331,-1.8793163299560547,-1.26099693775177,-5.374608993530273,-6.9775896072387695,3.9699244499206543,1.5151407718658447,4.543593883514404,-4.996665954589844,-6.796648025512695,-0.796980082988739,-5.379490852355957,-0.549635648727417,-9.417557716369629,-2.632219076156616,1.7778784036636353,1.6346489191055298,-7.801143169403076,7.2114739418029785,-9.09225082397461,0.9202542901039124,-1.1646091938018799,2.6558477878570557,-5.547433853149414,1.9413443803787231,-3.9593071937561035,2.2311630249023438,1.97158944606781,6.333553791046143,-2.814406633377075,0.1673775613307953,-8.345366477966309,3.1074397563934326,-3.0089330673217773,-4.5951337814331055,-3.7368662357330322,-5.906049728393555,0.728748619556427,0.3020936846733093,-1.9193578958511353,-3.302121877670288,-2.283527135848999,1.0822992324829102,1.084127426147461,0.7212051153182983,-0.2904839813709259,0.0067567480728030205,-6.094676494598389,-0.402170330286026,0.21640515327453613,5.0021562576293945,-2.4156148433685303,4.747771739959717,3.1189804077148438,6.701613903045654,7.1602253913879395,6.287492275238037,0.8026138544082642,6.452852725982666,-2.5331742763519287,-4.900890827178955,2.9736883640289307,4.881409645080566,-1.4666893482208252,-4.851039409637451,2.059319257736206,2.250230073928833,3.969353437423706,3.364682912826538,-0.05929283797740936,-0.6684280037879944,5.829762935638428,-0.979670524597168,-3.566016912460327,-1.4442508220672607,-2.588890552520752,-3.3296337127685547,-2.7897987365722656,6.3759918212890625,1.5154504776000977,-8.110496520996094,0.17165987193584442,-6.858039855957031,-3.5445096492767334,2.5935659408569336,2.885282516479492,1.8959251642227173,-5.664699077606201,-3.4687421321868896,-2.4294581413269043,-7.189571857452393,-2.8525397777557373,5.089686870574951,0.6043485999107361,3.9126408100128174,-2.7547128200531006,-5.008999824523926,-3.657480001449585,-9.955078125,3.3863651752471924,-3.036243200302124,4.4467034339904785,-0.05079968646168709,-0.4907218813896179,-3.036567211151123,4.6397705078125,1.6668617725372314,-1.2682503461837769,1.3137140274047852,-6.264095783233643,4.439817905426025,-6.315651893615723,-5.666043281555176,-0.6006585955619812,5.461076259613037,-0.24642640352249146,-2.679663896560669,-3.422029495239258,-0.013748754747211933,4.076957702636719,6.95869255065918,2.3676674365997314,-8.981572151184082,-8.151212692260742,3.518017053604126,-0.759570300579071,-0.42632266879081726,7.180819034576416,1.6771918535232544,-3.5160317420959473,0.8269298076629639,1.8569263219833374,2.010329484939575,-3.7082324028015137,6.012582302093506,4.621157169342041,0.9195900559425354,-1.7041544914245605,3.4600207805633545,2.8068747520446777,-4.070095062255859,3.804140567779541,-3.074394702911377,-4.536877155303955,-1.7962968349456787,-2.7294981479644775,2.9486894607543945,-6.354874134063721,-8.988039016723633,8.563080787658691,6.317517280578613,-0.9279513359069824,-1.9361271858215332,-9.039132118225098,7.131534099578857,2.7934367656707764,-5.968392848968506,-0.06545160710811615,3.7102925777435303,2.258908987045288,4.861087799072266,-0.05108696222305298,-4.49846887588501,-4.576083660125732,-6.251190185546875,-3.3793859481811523,1.4788758754730225,4.066930294036865,-0.9180363416671753,5.740225315093994,-1.3265256881713867,5.262291431427002,0.7175686359405518,2.9360263347625732,-7.619258880615234,5.261261463165283,1.1680265665054321,-4.450006008148193,5.410119533538818,-3.2600202560424805,4.232682228088379,-6.257768154144287],\"y\":[-2.366219997406006,1.4157464504241943,-8.381683349609375,-4.209931373596191,-5.979645252227783,-9.352097511291504,-1.8068146705627441,-7.144293785095215,6.021297931671143,6.609764575958252,-1.495269536972046,-0.639616072177887,-1.1818455457687378,-6.758322715759277,1.8673317432403564,1.815646767616272,0.16977642476558685,0.9171539545059204,-0.28289005160331726,-7.2281694412231445,0.7534630298614502,1.7798832654953003,2.778804302215576,3.790527105331421,-5.351175785064697,-3.560941457748413,4.08440637588501,5.833236217498779,-4.1663336753845215,0.3943403661251068,4.310480117797852,0.09831395000219345,0.24064360558986664,-1.9963525533676147,-1.881211519241333,4.180385589599609,-0.9039490222930908,-1.5471729040145874,0.5280646681785583,2.6742937564849854,4.0620808601379395,-6.513219356536865,-9.472671508789062,-1.1084809303283691,2.9618706703186035,4.339135646820068,-5.158017635345459,-5.029111862182617,4.206965923309326,2.9538791179656982,5.694890022277832,-0.4904268980026245,6.932773113250732,7.793333053588867,-4.360994338989258,4.613007545471191,2.7395997047424316,0.7967098355293274,3.114056348800659,-10.288593292236328,-1.2579964399337769,8.089855194091797,-2.3647656440734863,-3.7294106483459473,-2.29291033744812,2.8865668773651123,-3.3214125633239746,-7.906168460845947,-8.579545021057129,-7.941169261932373,-5.88237190246582,-2.719790458679199,-2.8582077026367188,-5.134355545043945,1.7824652194976807,6.741681098937988,5.792266845703125,-2.9503769874572754,-5.39525842666626,-5.694397926330566,5.918738842010498,-2.7563304901123047,-3.7066729068756104,-8.668044090270996,-5.506378650665283,-4.30855131149292,2.2824041843414307,0.5046464800834656,-5.011874675750732,-9.552413940429688,-7.896369457244873,0.7081783413887024,3.2146520614624023,-6.3050384521484375,-2.3505775928497314,-4.481626510620117,-1.7087410688400269,-4.868049621582031,-1.1532683372497559,0.6817308068275452,3.0158650875091553,1.4656099081039429,-7.470815658569336,-3.990213632583618,2.7786660194396973,-5.667232513427734,2.627997875213623,1.2886537313461304,4.768289566040039,-0.20373012125492096,-1.107700228691101,2.234194040298462,2.0606625080108643,-0.11353302001953125,-4.433785915374756,-0.016857877373695374,-6.968067646026611,-1.3603588342666626,-4.8725175857543945,-0.11705698072910309,-5.689452648162842,-3.5955538749694824,5.520202159881592,-1.0861376523971558,2.7821645736694336,-1.5905102491378784,-0.9805388450622559,-5.05893087387085,0.7785710096359253,4.528586387634277,-6.244285583496094,-7.922395706176758,1.2706235647201538,-4.036782741546631,-3.101402521133423,-8.064379692077637,-2.1249098777770996,-2.389819622039795,-3.1695380210876465,4.600878715515137,-1.2858613729476929,-9.436290740966797,-2.359637975692749,0.10975052416324615,3.9018781185150146,0.8384222984313965,-8.780622482299805,0.42526963353157043,1.231698751449585,-0.5188021063804626,-2.5711145401000977,-1.3426936864852905,-7.813931465148926,-3.1462206840515137,-2.4722371101379395,-1.0157849788665771,-4.328769207000732,6.737553596496582,-6.855597019195557,-2.030759572982788,-4.499425411224365,1.6545190811157227,6.392116546630859,-3.9923131465911865,3.8821256160736084,6.069917678833008,-0.25490161776542664,-0.4267646372318268,1.0929652452468872,3.285449504852295,1.4321503639221191,1.9575225114822388,8.271814346313477,1.9291417598724365,-1.3109941482543945,7.994966983795166,-5.386274814605713,4.301878452301025,2.89518141746521,6.410716533660889,6.38617467880249,-1.6168067455291748,6.841063976287842,0.05820506066083908,1.640087604522705,-2.8759775161743164,4.6040825843811035,-6.035190105438232,2.152789831161499,-3.5014779567718506,4.5739216804504395,-3.7116336822509766,0.06453826278448105,-5.643316268920898,-3.0912952423095703,-1.396384835243225,-2.5754761695861816,-3.2645411491394043,4.257584095001221,-6.593636989593506,-3.2896602153778076,-7.411569118499756,-1.3092409372329712,-6.6785125732421875,1.808240294456482,-0.3113737404346466,5.845041751861572,3.1287853717803955,-2.0365591049194336,-4.636128902435303,2.706270456314087,6.708128452301025,-9.077188491821289,-1.6567959785461426,-4.118268013000488,0.3064400553703308,-0.6685023903846741,0.6971976161003113,4.906248569488525,0.9191274642944336,-2.744675636291504,-4.471009731292725,4.209408283233643,3.0564539432525635,-7.672701358795166,-2.7060420513153076,-7.385237693786621,-2.326617479324341,-1.1584327220916748],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Word2Vec - Visualising embeddings with TSNE\"}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('5c39eac1-352f-4e2a-af06-888c12c0a8bd');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "def reduce_dimensions(model):\n",
    "    num_components = 2  # number of dimensions to keep after compression\n",
    "\n",
    "    # extract vocabulary from model and vectors in order to associate them in the graph\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)\n",
    "\n",
    "    # apply TSNE\n",
    "    tsne = TSNE(n_components=num_components, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "def plot_embeddings(x_vals, y_vals, labels):\n",
    "    import plotly.graph_objs as go\n",
    "    fig = go.Figure()\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='markers', text=labels)\n",
    "    fig.add_trace(trace)\n",
    "    fig.update_layout(title=\"Word2Vec - Visualising embeddings with TSNE\")\n",
    "    fig.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "\n",
    "plot = plot_embeddings(x_vals, y_vals, labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
