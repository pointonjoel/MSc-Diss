{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GPT-3.5-Turbo Model\n",
    "Creating a question answering chatbot using GPT-3.5. Adapted from: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_tokens() IS OFTEN USED IN KNOWLEDGE.PY (and other files??) WITHOUT SPECIFYING THE EMBEDDING MODEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/303 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "226c6e530a6745f4abf13224fcb31176"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\point\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\point\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33fb978cbe574121b142d7766ace2ef1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45ed9045f1de459ca5321a1378a34ffb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)in/added_tokens.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "778da2af8e494d5fbe37a97f73cb3810"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/127 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac64fc3449f04e83994bf4c5f132b78d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using a CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets, sentencepiece, transformers, accelerate, tiktoken, rouge_score, evaluate rouge_score bleu_score\n",
    "import sys\n",
    "sys.path.append(\"modules\")\n",
    "from modules.config import *\n",
    "from modules.knowledge import *\n",
    "from modules.chatbot import *\n",
    "from modules.embedding_functions import *\n",
    "from modules.data_extraction import *\n",
    "from modules.data_preprocessing import *\n",
    "from modules.gpt_ans_extraction import *\n",
    "from modules.query import *\n",
    "sys.path.remove(\"modules\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    log_and_print_message(f'Using GPU - details are as follows:')\n",
    "    log_and_print_message(f'__CUDNN VERSION: {torch.backends.cudnn.version()}')\n",
    "    log_and_print_message(f'__Number CUDA Devices: {torch.cuda.device_count()}')\n",
    "    log_and_print_message(f'__CUDA Device Name: {torch.cuda.get_device_name(0)}')\n",
    "    log_and_print_message(f'__CUDA Device Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9}')\n",
    "else:\n",
    "    log_and_print_message(f'No GPU available, using a CPU')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Knowledge Base"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "textbooks = ['Digital_Image_Processing_Textbook', 'Fundamentals_of_Digital_Image_Processing_Textbook']\n",
    "CompVisionKnowledge = Knowledge(CHATBOT_TOPIC) # Uses BERT embeddings to calculate cosine similarity\n",
    "print('here')\n",
    "for page in WIKI_PAGES:\n",
    "    CompVisionKnowledge.append_wikipedia_page(WIKI_PAGE)\n",
    "print('now here')\n",
    "for textbook in textbooks:\n",
    "    CompVisionKnowledge.append_pdf(f'assets/knowledge/{textbook}.pdf', textbook)\n",
    "CompVisionKnowledge.export_to_csv()\n",
    "CompVisionKnowledge.df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NQ Data Extraction and Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Data extraction\n",
    "training = AllData(cache_dir='/content/drive/MyDrive/Diss/Datasets', default='dev')\n",
    "training.export_simplified_dataset(path=\"/content/drive/MyDrive/Diss/Output/simplified_dataset_validation_new.csv\")\n",
    "\n",
    "### Data preprocessing\n",
    "training_data = TrainingData(save_dir=f'{OUTPUT_DIR}/all_data_{SHORT_MODEL_NAME}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'url', 'content', 'question', 'answer', 'start_token', 'end_token', 'length', 'input_ids', 'attention_mask', 'labels', 'num_tokens'],\n        num_rows: 11504\n    })\n    test: Dataset({\n        features: ['id', 'title', 'url', 'content', 'question', 'answer', 'start_token', 'end_token', 'length', 'input_ids', 'attention_mask', 'labels', 'num_tokens'],\n        num_rows: 2876\n    })\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset and extract only the answerable questions\n",
    "OUTPUT_DIR = 'G:\\My Drive\\Diss\\Output'\n",
    "short_model_name = \"bart-large-xsum\"\n",
    "all_data = load_from_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}').shuffle(seed=9)\n",
    "all_ans_data = all_data.filter(lambda row: (row[\"answer\"] != NO_ANS_TOKEN))\n",
    "all_ans_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 86/2876 [02:16<38:52,  1.20it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 506/2876 [15:25<36:59,  1.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 771/2876 [23:47<31:57,  1.10it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 25 Jul 2023 15:07:02 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7ec547d56c0306fd-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 2234/2876 [1:09:37<10:13,  1.05it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 25 Jul 2023 15:52:52 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7ec58af8dff5413b-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 2820/2876 [1:30:58<01:00,  1.08s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2876/2876 [1:32:34<00:00,  1.93s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/2876 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31ccaa46dc78498babfd6ef20cfb59ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 109/11504 [01:48<2:52:27,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 1314/11504 [36:18<2:13:34,  1.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1722/11504 [45:01<2:36:38,  1.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1839/11504 [47:56<2:16:00,  1.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2765/11504 [1:07:07<3:04:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 3371/11504 [1:19:37<2:30:37,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 3688/11504 [1:26:28<2:27:04,  1.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 25 Jul 2023 17:42:28 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7ec62b52c9312508-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 11475/11504 [4:10:47<00:40,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 25 Jul 2023 20:26:39 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7ec71c07ff762408-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11504/11504 [4:16:32<00:00,  1.34s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/11504 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9209fc1c2d84c9ba0891ac71cceddee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Iterate through the dataset and query GPT to extract a natural-language answer\n",
    "splits = ['test', 'train']\n",
    "for s in splits:\n",
    "    all_timestamps = []\n",
    "    responses = []\n",
    "    # Obtain GPT answers/responses\n",
    "    for i in tqdm(range(len(all_ans_data[s]))): # Due to the RPM\n",
    "        all_timestamps = pause_if_needed(all_timestamps)\n",
    "\n",
    "        formatted_text = format_request(all_ans_data[s][i])\n",
    "        inputs = [\n",
    "                {\"role\": \"system\", \"content\": f\"You answer questions by only using a provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": formatted_text},\n",
    "            ]\n",
    "        response, all_timestamps = query_gpt(inputs, all_timestamps)\n",
    "        responses.append(response.choices[0].message.content)\n",
    "\n",
    "    # Export to txt file\n",
    "    file_name = f\"{s}_all.txt\"\n",
    "    # Open the file in write mode and save the list to the file\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        for item in responses:\n",
    "            file.write(item + \"\\n\")\n",
    "\n",
    "    # Export as HF dataset\n",
    "    df_pandas = all_ans_data[s].to_pandas()\n",
    "    df_pandas['gpt_ans'] = responses\n",
    "    new_dataset = Dataset.from_pandas(df_pandas)\n",
    "    new_dataset.save_to_disk(f\"{OUTPUT_DIR}/{short_model_name}_{s}_split\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Now the new (GPT) answers can be merged with the original dataset\n",
    "# Load the original data and GPT data generated above\n",
    "train_dataset_pandas = all_data['train'].to_pandas()\n",
    "test_dataset_pandas = all_data['test'].to_pandas()\n",
    "# updated_train_dataset_pandas = load_from_disk(f\"assets/{short_model_name}_train_split\").to_pandas()\n",
    "# updated_test_dataset_pandas = load_from_disk(f\"assets/{short_model_name}_test_split\").to_pandas()\n",
    "updated_train_dataset_pandas = load_from_disk(f\"{OUTPUT_DIR}/{short_model_name}_train_split\").to_pandas()\n",
    "updated_test_dataset_pandas = load_from_disk(f\"{OUTPUT_DIR}/{short_model_name}_test_split\").to_pandas()\n",
    "\n",
    "# Merge on id and keep all rows\n",
    "train_merged = pd.merge(train_dataset_pandas, updated_train_dataset_pandas[['id', 'gpt_ans']], on='id', how='outer')\n",
    "test_merged = pd.merge(test_dataset_pandas, updated_test_dataset_pandas[['id', 'gpt_ans']], on='id', how='outer')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/13744 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "976cb31e7ae9472f876abc21ac064ead"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/3475 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31f12ed7eb734225843f55d6e8991547"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/13744 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a3ad9cbbd4f4da4b83e4176e48426b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/3475 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "951dce8ee25f4a65880ff51755a30a9b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0.797264784095226"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove any examples where it apparently had an answer but GPT couldn't extract oen\n",
    "train_merged = train_merged[train_merged['gpt_ans'] != NO_ANS_TOKEN]\n",
    "test_merged = test_merged[test_merged['gpt_ans'] !=  NO_ANS_TOKEN]\n",
    "\n",
    "# Update the answers to match the GPT ones\n",
    "train_merged.loc[train_merged['answer'] != NO_ANS_TOKEN, 'answer'] = train_merged.loc[train_merged['answer'] != NO_ANS_TOKEN, 'gpt_ans']\n",
    "test_merged.loc[test_merged['answer'] != NO_ANS_TOKEN, 'answer'] = test_merged.loc[test_merged['answer'] != NO_ANS_TOKEN, 'gpt_ans']\n",
    "\n",
    "merged_dataset = DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(train_merged),\n",
    "        \"test\": Dataset.from_pandas(test_merged),\n",
    "    })\n",
    "\n",
    "# Maintain the answerable/non-answerable balance\n",
    "def ensure_ans_non_ans_balance(dataset, dataset_splits=('training', 'validation'), proportion=0.3, seed=SEED):\n",
    "    # Extracting the unanswerable examples\n",
    "    no_ans = dataset.filter(lambda row: (row[\"answer\"] == NO_ANS_TOKEN))\n",
    "    good_ans = dataset.filter(lambda row: (row[\"answer\"] != NO_ANS_TOKEN))\n",
    "\n",
    "    # Discarding some unanswerable examples so the answer-no_ans ratio is favourable in each split\n",
    "    processed_datasets_dict = {}\n",
    "    for split in dataset_splits:\n",
    "      num_no_ans = proportion*len(good_ans[split])/(1-proportion)\n",
    "      no_ans_keep = no_ans[split].train_test_split(train_size=num_no_ans/len(no_ans[split]), seed=seed)['train']\n",
    "      processed_datasets_dict[split] = concatenate_datasets([no_ans_keep, good_ans[split]])\n",
    "\n",
    "    processed_dataset = DatasetDict({\n",
    "            dataset_splits[0]: processed_datasets_dict[dataset_splits[0]],\n",
    "            dataset_splits[1]: processed_datasets_dict[dataset_splits[1]],\n",
    "        })\n",
    "    shuffled_dataset = processed_dataset.shuffle(seed=seed)\n",
    "    return shuffled_dataset\n",
    "\n",
    "final_dataset = ensure_ans_non_ans_balance(merged_dataset, dataset_splits=['train', 'test'])\n",
    "\n",
    "# As can be seen, the 80/20 train-test split has been maintained\n",
    "len(final_dataset['train'])/(len(final_dataset['train'])+len(final_dataset['test']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/12592 [00:01<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ef1c868be8e48f6a59fab883c03e16b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\point\\anaconda3\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/3202 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4b8aaeed8da467e90840cf28fc66971"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/12592 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1236f3043a0a47648a69ef82e436d61f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/3202 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c29bba6c5bf74893bbe6dd45706a3b56"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finally, the questions/contexts and answers need to be tokenised to override previous values\n",
    "def tokenise(data):\n",
    "    # tokenize the inputs (questions and contexts)\n",
    "\n",
    "    # FIX THIS AND CHECK WHERE MODEL_NAME IS BEING DEFINED AS GPT2 AS IT SHOULDN'T BE\n",
    "    tokeniser = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokeniser.pad_token is None:\n",
    "        print('Adding PAD token')\n",
    "        tokeniser.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokeniser.add_special_tokens({'additional_special_tokens': [NO_ANS_TOKEN]})\n",
    "\n",
    "    additional_cols = tokeniser(data['content'], data['question'], truncation=False)\n",
    "\n",
    "    # tokenize the answers\n",
    "    targets = tokeniser(text_target=data['answer'], truncation=False)\n",
    "\n",
    "    #set labels\n",
    "    additional_cols['labels'] = targets['input_ids']\n",
    "    additional_cols['num_tokens'] = [len(row) for row in additional_cols[\"input_ids\"]]\n",
    "    return additional_cols\n",
    "\n",
    "final_dataset = final_dataset.map(tokenise, batched = True)\n",
    "final_dataset.save_to_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}_gpt_updated')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NQ Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = \"google/mt5-small\"  # \"gpt2\", \"facebook/bart-large-xsum\"\n",
    "short_model_name = model_name.split('/')[1]\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_name) # NEED TO RENAME THIS\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\",\n",
    "                                              torch_dtype=\"auto\")\n",
    "tokeniser = add_special_tokens(tokeniser)\n",
    "model.resize_token_embeddings(len(tokeniser))\n",
    "\n",
    "# Load data\n",
    "all_data = load_from_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}_gpt_updated') # add or remove the suffix a required\n",
    "\n",
    "# Connect to HuggingFace\n",
    "HfFolder.save_token(\"ADD_TOKEN_HERE\")\n",
    "!git config --global user.email \"pointon.joel@gmail.com\"\n",
    "!git config --global user.name \"Joel Pointon\"\n",
    "\n",
    "# Training arguments/config\n",
    "batch_size = 8 # 64\n",
    "num_train_epochs = 8\n",
    "logging_steps = len(all_data[\"training\"]) // batch_size # Show the training loss with every epoch\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_name}_updated\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    push_to_hub=True,\n",
    "    seed=9,\n",
    "    # optim=\"adafactor\"\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokeniser.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokeniser.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokeniser.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores\n",
    "    # result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokeniser, model=model)\n",
    "\n",
    "def model_init():\n",
    "  return AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=all_data[\"train\"],\n",
    "    eval_dataset=all_data[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokeniser,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "path = f\"/content/drive/MyDrive/Diss/Output/{short_model_name}-finetuned-natural-questions\"\n",
    "trainer.train()\n",
    "trainer.push_to_hub()\n",
    "trainer.save_model(path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# NEED TO FIX THIS\n",
    "filename = 'assets/knowledge/Fundamentals_of_Digital_Image_Processing_Textbook.pdf'\n",
    "doc = fitz.open(filename)\n",
    "page_limit = None\n",
    "all_text = ''\n",
    "# Iterate through the content\n",
    "for page in doc:\n",
    "    page_limit = doc.page_count if not page_limit else page_limit\n",
    "    if page.number <= page_limit:\n",
    "        block_content = page.get_text(\"blocks\") #.encode(\"utf8\") # \"blocks\"\n",
    "        for block in block_content:\n",
    "            if block[6] == 0:  # I.e. only extract text\n",
    "                plain_text = unidecode(block[4])  # .decode('latin1') #.decode('utf-8')\n",
    "                all_text += plain_text\n",
    "    else:\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MLM Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extracting training content\n",
    "for textbook in textbooks:\n",
    "    all_text = ''.join(CompVisionKnowledge.df.loc[CompVisionKnowledge.df['Source']==textbook, 'Content'].tolist())\n",
    "    with open(f'assets/knowledge/{textbook}.txt', \"w\") as f:\n",
    "        f.write(all_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read txt files\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "# For formatting a PDF as a HF datasets\n",
    "def get_text_dataset(path):\n",
    "  dataset_obj = TextDataset(\n",
    "        tokenizer = tokeniser,\n",
    "        file_path = path,\n",
    "        block_size = 512,\n",
    "    )\n",
    "  return dataset_obj\n",
    "\n",
    "# Read documents from the directory\n",
    "training_file = 'assets/knowledge/Digital_Image_Processing_Textbook.txt'\n",
    "validation_file = 'assets/knowledge/Fundamentals_of_Digital_Image_Processing_Textbook.txt'\n",
    "train_data = read_txt(training_file)\n",
    "validation_data = read_txt(validation_file)\n",
    "train_data = re.sub(r'\\n+', '\\n', train_data).strip()  # Remove excess newline characters\n",
    "validation_data = re.sub(r'\\n+', '\\n', validation_data).strip()  # Remove excess newline characters\n",
    "\n",
    "with open(f'{training_file}_formatted', \"w\") as f:\n",
    "    f.write(train_data)\n",
    "with open(f'{validation_file}_formatted', \"w\") as f:\n",
    "    f.write(validation_data)\n",
    "\n",
    "model_name = 'gpt2'\n",
    "output_dir = 'models/custom_q_and_a'\n",
    "batch_size = 8\n",
    "num_train_epochs = 8\n",
    "\n",
    "# Train\n",
    "tokeniser = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "if tokeniser.pad_token is None:\n",
    "    tokeniser.add_special_tokens({'pad_token': '[PAD]', 'mask_token': '[MASK]'})\n",
    "    model.resize_token_embeddings(len(tokeniser))\n",
    "\n",
    "\n",
    "train_dataset = get_text_dataset(f'{training_file}_formatted')\n",
    "validation_dataset = get_text_dataset(f'{validation_file}_formatted')\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokeniser,\n",
    "        mlm=True,\n",
    "    )\n",
    "\n",
    "HF_REFERENCE='mlm'\n",
    "logging_steps = len(train_dataset) // batch_size // 4\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=HF_REFERENCE,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=logging_steps,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    push_to_hub=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    ")\n",
    "trainer.train()\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "trainer.push_to_hub()\n",
    "tokeniser.push_to_hub(f'psxjp5/{HF_REFERENCE}')\n",
    "print('Finished training and pushed to hub!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Todo:\n",
    "# Adapt for the MLM and QA models\n",
    "# Check num_tokens usage and specifying the encoding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CompVisionChatbot = ChatBot(CHATBOT_TOPIC)\n",
    "print(Query.ask_gpt('When did Universities begin teaching Computer Vision?', CompVisionChatbot, show_source=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(Query.ask_finetuned('When did Universities begin teaching Computer Vision?', CompVisionChatbot, hf_reference=MLM_HF_REFERENCE, show_source=True))  # What if the GPT knowledge sections are longer than 1024 tokens?? Need to account for this!\n",
    "\n",
    "# 'What is PCA?'\n",
    "\n",
    "question_answering = pipeline(model=f'psxjp5/mlm')\n",
    "print(question_answering(text)[0]['generated_text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
