{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GPT-3.5-Turbo Model\n",
    "Creating a question answering chatbot using GPT-3.5. Adapted from: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_tokens() IS OFTEN USED IN KNOWLEDGE.PY (and other files??) WITHOUT SPECIFYING THE EMBEDDING MODEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using a CPU\n",
      "No GPU available, using a CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets, sentencepiece, transformers, accelerate, tiktoken, rouge_score, evaluate rouge_score bleu_score\n",
    "import sys\n",
    "sys.path.append(\"modules\")\n",
    "from modules.config import *\n",
    "from modules.knowledge import *\n",
    "from modules.chatbot import *\n",
    "from modules.embedding_functions import *\n",
    "from modules.data_extraction import *\n",
    "from modules.data_preprocessing import *\n",
    "from modules.gpt_ans_extraction import *\n",
    "from modules.query import *\n",
    "sys.path.remove(\"modules\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Knowledge Base"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following PDF has been successfully added to the knowledge database: Digital_Image_Processing_Textbook (assets/knowledge/Digital_Image_Processing_Textbook.pdf)\n",
      "The following PDF has been successfully added to the knowledge database: Fundamentals_of_Digital_Image_Processing_Textbook (assets/knowledge/Fundamentals_of_Digital_Image_Processing_Textbook.pdf)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                Source Heading Subheading  \\\n0                    Digital_Image_Processing_Textbook                      \n1                    Digital_Image_Processing_Textbook                      \n2                    Digital_Image_Processing_Textbook                      \n3                    Digital_Image_Processing_Textbook                      \n4                    Digital_Image_Processing_Textbook                      \n..                                                 ...     ...        ...   \n224  Fundamentals_of_Digital_Image_Processing_Textbook                      \n225  Fundamentals_of_Digital_Image_Processing_Textbook                      \n226  Fundamentals_of_Digital_Image_Processing_Textbook                      \n227  Fundamentals_of_Digital_Image_Processing_Textbook                      \n228  Fundamentals_of_Digital_Image_Processing_Textbook                      \n\n                            Page  \\\n0                        0/1/2/3   \n1                  3/4/5/6/7/8/9   \n2                        9/10/11   \n3                          11/12   \n4                          12/13   \n..                           ...   \n224                      341/342   \n225                  342/343/344   \n226                      344/345   \n227  345/346/347/348/349/350/351   \n228              351/352/353/354   \n\n                                               Content  \\\n0    \\nGLOBAL \\nEDITION\\nDigital Image Processing\\n...   \n1    \\nISBN 10: 1-292-22304-9\\nISBN 13: 978-1-292-2...   \n2    \\nBackground  904\\nPatterns and Pattern Classe...   \n3    \\nChapter 5: Revisions to this chapter were li...   \n4    \\nChapter 12: This chapter underwent a major r...   \n..                                                 ...   \n224  \\nkey elements, 21\\nmathematics of, 22 37\\ncon...   \n225  \\nMagnetic resonance imaging (MRI), 49\\nMahala...   \n226  \\nPeriodic square wave synthesis, 117\\nPerspec...   \n227  \\nSalt and pepper noise, 46, 90, 91, 93\\nSampl...   \n228  \\nbottom, we have: (a) the original image with...   \n\n                                               Section  Tokens  \\\n0    Digital_Image_Processing_Textbook->Page(s)0/1/2/3   791.0   \n1    Digital_Image_Processing_Textbook->Page(s)3/4/...   793.0   \n2    Digital_Image_Processing_Textbook->Page(s)9/10/11   777.0   \n3      Digital_Image_Processing_Textbook->Page(s)11/12   413.0   \n4      Digital_Image_Processing_Textbook->Page(s)12/13   563.0   \n..                                                 ...     ...   \n224  Fundamentals_of_Digital_Image_Processing_Textb...   771.0   \n225  Fundamentals_of_Digital_Image_Processing_Textb...   640.0   \n226  Fundamentals_of_Digital_Image_Processing_Textb...   591.0   \n227  Fundamentals_of_Digital_Image_Processing_Textb...   780.0   \n228  Fundamentals_of_Digital_Image_Processing_Textb...   476.0   \n\n                                             Embedding  \n0    [-0.021132638677954674, 0.025948533788323402, ...  \n1    [0.005928434897214174, 0.01934611052274704, -0...  \n2    [-0.011725608259439468, 0.021519940346479416, ...  \n3    [0.013898221775889397, 0.013536868616938591, -...  \n4    [-0.005244759377092123, 0.018554115667939186, ...  \n..                                                 ...  \n224  [0.0032916401978582144, 0.0268571637570858, -0...  \n225  [-0.009114357642829418, 0.00879641529172659, 0...  \n226  [-0.011132902465760708, 0.001304946606978774, ...  \n227  [0.015710284933447838, 0.001202668296173215, 0...  \n228  [-0.0019758823327720165, 0.0056046489626169205...  \n\n[1059 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Source</th>\n      <th>Heading</th>\n      <th>Subheading</th>\n      <th>Page</th>\n      <th>Content</th>\n      <th>Section</th>\n      <th>Tokens</th>\n      <th>Embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>0/1/2/3</td>\n      <td>\\nGLOBAL \\nEDITION\\nDigital Image Processing\\n...</td>\n      <td>Digital_Image_Processing_Textbook-&gt;Page(s)0/1/2/3</td>\n      <td>791.0</td>\n      <td>[-0.021132638677954674, 0.025948533788323402, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>3/4/5/6/7/8/9</td>\n      <td>\\nISBN 10: 1-292-22304-9\\nISBN 13: 978-1-292-2...</td>\n      <td>Digital_Image_Processing_Textbook-&gt;Page(s)3/4/...</td>\n      <td>793.0</td>\n      <td>[0.005928434897214174, 0.01934611052274704, -0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>9/10/11</td>\n      <td>\\nBackground  904\\nPatterns and Pattern Classe...</td>\n      <td>Digital_Image_Processing_Textbook-&gt;Page(s)9/10/11</td>\n      <td>777.0</td>\n      <td>[-0.011725608259439468, 0.021519940346479416, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>11/12</td>\n      <td>\\nChapter 5: Revisions to this chapter were li...</td>\n      <td>Digital_Image_Processing_Textbook-&gt;Page(s)11/12</td>\n      <td>413.0</td>\n      <td>[0.013898221775889397, 0.013536868616938591, -...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>12/13</td>\n      <td>\\nChapter 12: This chapter underwent a major r...</td>\n      <td>Digital_Image_Processing_Textbook-&gt;Page(s)12/13</td>\n      <td>563.0</td>\n      <td>[-0.005244759377092123, 0.018554115667939186, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>224</th>\n      <td>Fundamentals_of_Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>341/342</td>\n      <td>\\nkey elements, 21\\nmathematics of, 22 37\\ncon...</td>\n      <td>Fundamentals_of_Digital_Image_Processing_Textb...</td>\n      <td>771.0</td>\n      <td>[0.0032916401978582144, 0.0268571637570858, -0...</td>\n    </tr>\n    <tr>\n      <th>225</th>\n      <td>Fundamentals_of_Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>342/343/344</td>\n      <td>\\nMagnetic resonance imaging (MRI), 49\\nMahala...</td>\n      <td>Fundamentals_of_Digital_Image_Processing_Textb...</td>\n      <td>640.0</td>\n      <td>[-0.009114357642829418, 0.00879641529172659, 0...</td>\n    </tr>\n    <tr>\n      <th>226</th>\n      <td>Fundamentals_of_Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>344/345</td>\n      <td>\\nPeriodic square wave synthesis, 117\\nPerspec...</td>\n      <td>Fundamentals_of_Digital_Image_Processing_Textb...</td>\n      <td>591.0</td>\n      <td>[-0.011132902465760708, 0.001304946606978774, ...</td>\n    </tr>\n    <tr>\n      <th>227</th>\n      <td>Fundamentals_of_Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>345/346/347/348/349/350/351</td>\n      <td>\\nSalt and pepper noise, 46, 90, 91, 93\\nSampl...</td>\n      <td>Fundamentals_of_Digital_Image_Processing_Textb...</td>\n      <td>780.0</td>\n      <td>[0.015710284933447838, 0.001202668296173215, 0...</td>\n    </tr>\n    <tr>\n      <th>228</th>\n      <td>Fundamentals_of_Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>351/352/353/354</td>\n      <td>\\nbottom, we have: (a) the original image with...</td>\n      <td>Fundamentals_of_Digital_Image_Processing_Textb...</td>\n      <td>476.0</td>\n      <td>[-0.0019758823327720165, 0.0056046489626169205...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1059 rows Ã— 8 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textbooks = ['Digital_Image_Processing_Textbook', 'Fundamentals_of_Digital_Image_Processing_Textbook']\n",
    "CompVisionKnowledge = Knowledge(CHATBOT_TOPIC, gpt=True)\n",
    "# for page in WIKI_PAGES:\n",
    "#     CompVisionKnowledge.append_wikipedia_page(WIKI_PAGE)\n",
    "for textbook in textbooks:\n",
    "    CompVisionKnowledge.append_pdf(f'assets/knowledge/{textbook}.pdf', textbook)\n",
    "CompVisionKnowledge.export_to_csv()\n",
    "CompVisionKnowledge.df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NQ Data Extraction and Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Data extraction\n",
    "training = AllData(cache_dir='/content/drive/MyDrive/Diss/Datasets', default='dev')\n",
    "training.export_simplified_dataset(path=\"/content/drive/MyDrive/Diss/Output/simplified_dataset_validation_new.csv\")\n",
    "\n",
    "### Data preprocessing\n",
    "model_name = \"google/mt5-small\" # This will dictate the model used for all the training\n",
    "# model_name = \"facebook/bart-large-xsum\"\n",
    "short_model_name = model_name.split('/')[1]\n",
    "model_output_name = f\"{short_model_name}_epochs_new_new\"\n",
    "HF_reference = f\"psxjp5/{model_output_name}\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\",\n",
    "                                              torch_dtype=\"auto\")\n",
    "model.config.hidden_dropout_prob = 0.1 # For regularisation\n",
    "tokeniser = add_special_tokens(tokeniser)\n",
    "model.resize_token_embeddings(len(tokeniser))\n",
    "\n",
    "training_data = TrainingData(tokeniser=tokeniser, save_dir=f'{OUTPUT_DIR}/all_data_{short_model_name}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the dataset and extract only the answerable questions\n",
    "all_data = load_from_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}').shuffle(seed=9)\n",
    "all_ans_data = all_data.filter(lambda row: (row[\"answer\"] != NO_ANS_TOKEN))\n",
    "all_ans_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate through the dataset and query GPT to extract a natural-language answer\n",
    "splits = ['test', 'train']\n",
    "for s in splits:\n",
    "    all_timestamps = []\n",
    "    responses = []\n",
    "    # Obtain GPT answers/responses\n",
    "    for i in tqdm(range(len(all_ans_data[s]))): # Due to the RPM\n",
    "        all_timestamps = pause_if_needed(all_timestamps)\n",
    "\n",
    "        formatted_text = format_request(all_ans_data[s][i])\n",
    "        inputs = [\n",
    "                {\"role\": \"system\", \"content\": f\"You answer questions by only using a provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": formatted_text},\n",
    "            ]\n",
    "        response, all_timestamps = query_gpt(inputs, all_timestamps)\n",
    "        responses.append(response.choices[0].message.content)\n",
    "\n",
    "    # Export as HF dataset\n",
    "    df_pandas = all_ans_data[s].to_pandas()\n",
    "    df_pandas['gpt_ans'] = responses\n",
    "    new_dataset = Dataset.from_pandas(df_pandas)\n",
    "    new_dataset.save_to_disk(f\"{OUTPUT_DIR}/{short_model_name}_{s}_split\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now the new (GPT) answers can be merged with the original dataset\n",
    "# Load the original data and GPT data generated above\n",
    "train_dataset_pandas = all_data['train'].to_pandas()\n",
    "test_dataset_pandas = all_data['test'].to_pandas()\n",
    "# updated_train_dataset_pandas = load_from_disk(f\"assets/{short_model_name}_train_split\").to_pandas()\n",
    "# updated_test_dataset_pandas = load_from_disk(f\"assets/{short_model_name}_test_split\").to_pandas()\n",
    "updated_train_dataset_pandas = load_from_disk(f\"{OUTPUT_DIR}/{short_model_name}_train_split\").to_pandas()\n",
    "updated_test_dataset_pandas = load_from_disk(f\"{OUTPUT_DIR}/{short_model_name}_test_split\").to_pandas()\n",
    "\n",
    "# Merge on id and keep all rows\n",
    "train_merged = pd.merge(train_dataset_pandas, updated_train_dataset_pandas[['id', 'gpt_ans']], on='id', how='outer')\n",
    "test_merged = pd.merge(test_dataset_pandas, updated_test_dataset_pandas[['id', 'gpt_ans']], on='id', how='outer')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove any examples where it apparently had an answer but GPT couldn't extract oen\n",
    "train_merged = train_merged[train_merged['gpt_ans'] != NO_ANS_TOKEN]\n",
    "test_merged = test_merged[test_merged['gpt_ans'] !=  NO_ANS_TOKEN]\n",
    "\n",
    "# Update the answers to match the GPT ones\n",
    "train_merged.loc[train_merged['answer'] != NO_ANS_TOKEN, 'answer'] = train_merged.loc[train_merged['answer'] != NO_ANS_TOKEN, 'gpt_ans']\n",
    "test_merged.loc[test_merged['answer'] != NO_ANS_TOKEN, 'answer'] = test_merged.loc[test_merged['answer'] != NO_ANS_TOKEN, 'gpt_ans']\n",
    "\n",
    "merged_dataset = DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(train_merged),\n",
    "        \"test\": Dataset.from_pandas(test_merged),\n",
    "    })\n",
    "\n",
    "# Maintain the answerable/non-answerable balance\n",
    "def ensure_ans_non_ans_balance(dataset, dataset_splits=('training', 'validation'), proportion=0.3, seed=SEED):\n",
    "    # Extracting the unanswerable examples\n",
    "    no_ans = dataset.filter(lambda row: (row[\"answer\"] == NO_ANS_TOKEN))\n",
    "    good_ans = dataset.filter(lambda row: (row[\"answer\"] != NO_ANS_TOKEN))\n",
    "\n",
    "    # Discarding some unanswerable examples so the answer-no_ans ratio is favourable in each split\n",
    "    processed_datasets_dict = {}\n",
    "    for split in dataset_splits:\n",
    "      num_no_ans = proportion*len(good_ans[split])/(1-proportion)\n",
    "      no_ans_keep = no_ans[split].train_test_split(train_size=num_no_ans/len(no_ans[split]), seed=seed)['train']\n",
    "      processed_datasets_dict[split] = concatenate_datasets([no_ans_keep, good_ans[split]])\n",
    "\n",
    "    processed_dataset = DatasetDict({\n",
    "            dataset_splits[0]: processed_datasets_dict[dataset_splits[0]],\n",
    "            dataset_splits[1]: processed_datasets_dict[dataset_splits[1]],\n",
    "        })\n",
    "    shuffled_dataset = processed_dataset.shuffle(seed=seed)\n",
    "    return shuffled_dataset\n",
    "\n",
    "final_dataset = ensure_ans_non_ans_balance(merged_dataset, dataset_splits=['train', 'test'])\n",
    "\n",
    "# As can be seen, the 80/20 train-test split has been maintained and there's no error responses\n",
    "percentage = len(final_dataset['train'])/(len(final_dataset['train'])+len(final_dataset['test']))\n",
    "print(f'Train/test split: {percentage}')\n",
    "all_data.filter(lambda row: (row[\"answer\"] == 'error'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finally, the questions/contexts and answers need to be tokenised to override previous values\n",
    "def tokenise(data):\n",
    "    # tokenize the inputs (questions and contexts)\n",
    "    additional_cols = tokeniser(data['content'], data['question'], truncation=False)\n",
    "\n",
    "    # tokenize the answers\n",
    "    targets = tokeniser(text_target=data['answer'], truncation=False)\n",
    "\n",
    "    #set labels\n",
    "    additional_cols['labels'] = targets['input_ids']\n",
    "    additional_cols['num_tokens'] = [len(row) for row in additional_cols[\"input_ids\"]]\n",
    "    return additional_cols\n",
    "\n",
    "final_dataset = final_dataset.map(tokenise, batched = True)\n",
    "final_dataset.save_to_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}_gpt_updated')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NQ Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the GPT updated data\n",
    "all_data = load_from_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}_gpt_updated') # add or remove the suffix a required\n",
    "\n",
    "# Training arguments/config\n",
    "batch_size = 8\n",
    "num_train_epochs = 20\n",
    "logging_steps = len(all_data[\"train\"]) // batch_size // 4 # Show the training loss with every epoch\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_output_name,\n",
    "    logging_steps=logging_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=logging_steps,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    metric_for_best_model = 'bleu',\n",
    "    load_best_model_at_end = True,\n",
    "    push_to_hub=True,\n",
    "    seed=9\n",
    ")\n",
    "\n",
    "# For model evaluation/metric computation\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_rouge(decoded_preds, decoded_labels):\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "\n",
    "    # Extract the median scores\n",
    "    results = {key: value * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in results.items()}\n",
    "\n",
    "def compute_bleu(decoded_preds, decoded_labels, preds):\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokeniser.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "def compute_meteor(decoded_preds, decoded_labels):\n",
    "    result = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "def compute_perc_noans_acc(decoded_preds, decoded_labels):\n",
    "    total = len(decoded_labels)\n",
    "    correct = sum(1 for pred, label in zip(decoded_preds, decoded_labels) if (pred == label and label==''))\n",
    "\n",
    "    return {'No Ans Accuracy': round(correct/total, 4)*100}\n",
    "\n",
    "def compute_cosine_similarity(decoded_preds, decoded_labels):\n",
    "    cosine_sims = []\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "      if label != '':  # i.e. Is answerable\n",
    "        encoded_pred = GENERAL_EMBEDDING_MODEL.encode(pred)\n",
    "        encoded_label = GENERAL_EMBEDDING_MODEL.encode(label)\n",
    "        similarity = 1 - spatial.distance.cosine(encoded_pred, encoded_label)\n",
    "        cosine_sims.append(similarity)\n",
    "\n",
    "    mean = sum(cosine_sims)/len(cosine_sims)\n",
    "\n",
    "    return {'Av Cosine Sim': round(mean, 4)}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    # Decode generated answers\n",
    "    decoded_preds = tokeniser.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokeniser.pad_token_id)\n",
    "    # Decode target answers\n",
    "    decoded_labels = tokeniser.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute metrics\n",
    "    rouge_results = compute_rouge(decoded_preds, decoded_labels)\n",
    "    bleu_results = compute_bleu(decoded_preds, decoded_labels, preds)\n",
    "    meteor_results = compute_meteor(decoded_preds, decoded_labels)\n",
    "    noans_acc = compute_perc_noans_acc(decoded_preds, decoded_labels)\n",
    "    cosine_sim = compute_cosine_similarity(decoded_preds, decoded_labels)\n",
    "\n",
    "    all_results = {**rouge_results, **bleu_results, **meteor_results, **noans_acc, **cosine_sim}\n",
    "\n",
    "    return all_results\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokeniser, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=all_data[\"train\"],\n",
    "    eval_dataset=all_data[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokeniser,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.1)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.push_to_hub()\n",
    "trainer.save_model(model_output_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MLM Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extracting training content\n",
    "for textbook in textbooks:\n",
    "    all_text = ''.join(CompVisionKnowledge.df.loc[CompVisionKnowledge.df['Source']==textbook, 'Content'].tolist())\n",
    "    with open(f'assets/knowledge/{textbook}.txt', \"w\") as f:\n",
    "        f.write(all_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read txt files\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def get_text_dataset(path):\n",
    "  dataset_obj = TextDataset(\n",
    "        tokenizer = tokeniser,\n",
    "        file_path = path,\n",
    "        block_size = 512,\n",
    "    )\n",
    "  return dataset_obj\n",
    "\n",
    "# Read documents from the directory\n",
    "training_file = '/content/drive/MyDrive/Diss/Datasets/Digital_Image_Processing_Textbook.txt'\n",
    "validation_file = '/content/drive/MyDrive/Diss/Datasets/Fundamentals_of_Digital_Image_Processing_Textbook.txt'\n",
    "train_data = read_txt(training_file)\n",
    "validation_data = read_txt(validation_file)\n",
    "train_data = re.sub(r'\\n+', '\\n', train_data).strip()  # Remove excess newline characters\n",
    "validation_data = re.sub(r'\\n+', '\\n', validation_data).strip()  # Remove excess newline characters\n",
    "\n",
    "with open(f'{training_file}_formatted', \"w\") as f:\n",
    "    f.write(train_data)\n",
    "with open(f'{validation_file}_formatted', \"w\") as f:\n",
    "    f.write(validation_data)\n",
    "\n",
    "batch_size = 8\n",
    "num_train_epochs = 50\n",
    "\n",
    "# Train\n",
    "gpt_tokeniser = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "if gpt_tokeniser.pad_token is None:\n",
    "    gpt_tokeniser.add_special_tokens({'pad_token': '[PAD]', 'mask_token': '[MASK]'})\n",
    "    gpt_model.resize_token_embeddings(len(gpt_tokeniser))\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=gpt_tokeniser,\n",
    "        mlm=True,\n",
    "    )\n",
    "\n",
    "train_dataset = get_text_dataset(f'{training_file}_formatted')\n",
    "validation_dataset = get_text_dataset(f'{validation_file}_formatted')\n",
    "\n",
    "HF_REFERENCE='mlm_new'\n",
    "logging_steps = len(train_dataset) // batch_size // 4\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=HF_REFERENCE,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=logging_steps,\n",
    "    save_steps=logging_steps,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    metric_for_best_model = 'loss',\n",
    "    load_best_model_at_end = True,\n",
    "    save_total_limit=3,\n",
    "    push_to_hub=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=gpt_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=0.05)]\n",
    ")\n",
    "trainer.train()\n",
    "import math\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "trainer.push_to_hub()\n",
    "tokeniser.push_to_hub(f'psxjp5/{HF_REFERENCE}')\n",
    "print('Finished training and pushed to hub!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Todo:\n",
    "# Check num_tokens usage and specifying the encoding model\n",
    "CompVisionChatbot = ChatBot(CHATBOT_TOPIC, hf_reference=T5_QA_GPT_HF_REFERENCE, embedding='gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\point\\OneDrive\\Documents\\Standard Files\\University\\Masters\\Diss\\MSc-Diss\\code\\modules\\query.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Index'] = np.arange(len(self.knowledge_used)) + 1\n",
      "C:\\Users\\point\\OneDrive\\Documents\\Standard Files\\University\\Masters\\Diss\\MSc-Diss\\code\\modules\\query.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Tokens'] = self.knowledge_used[\"Content\"].apply(\n",
      "C:\\Users\\point\\OneDrive\\Documents\\Standard Files\\University\\Masters\\Diss\\MSc-Diss\\code\\modules\\query.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Cumulative_tokens'] = self.knowledge_used['Tokens'].cumsum()\n",
      "C:\\Users\\point\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PCA is used to approximate the face.\n",
      "\n",
      "To construct this answer, I used the following documents : \n",
      "\n",
      "1. Fundamentals_of_Digital_Image_Processing_Textbook->Page(s)278/279/280/281:\n",
      "When we carry out a PCA on this ensemble of faces, we find that the eigenvalue spectrum\n",
      "dies off qu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\point\\OneDrive\\Documents\\Standard Files\\University\\Masters\\Diss\\MSc-Diss\\code\\modules\\query.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Output'] = '\\n\\n' + self.knowledge_used['Index'].astype(str) + '. ' + self.knowledge_used[\n"
     ]
    }
   ],
   "source": [
    "# print(Query.ask_gpt('When did Universities begin teaching Computer Vision?', CompVisionChatbot, show_source=True))\n",
    "\n",
    "print(Query.ask_finetuned('What is PCA?', CompVisionChatbot, show_source=True))  # What if the GPT knowledge sections are longer than 1024 tokens?? Need to account for this!\n",
    "\n",
    "# 'What is PCA?'"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
