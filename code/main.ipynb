{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GPT-3.5-Turbo Model\n",
    "Creating a question answering chatbot using GPT-3.5. Adapted from: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_tokens() IS OFTEN USED IN KNOWLEDGE.PY (and other files??) WITHOUT SPECIFYING THE EMBEDDING MODEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m sys\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodules\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodules\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodules\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mknowledge\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodules\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchatbot\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodules\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01membedding_functions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\Standard Files\\University\\Masters\\Diss\\MSc-Diss\\code\\modules\\knowledge.py:4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01membedding_functions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mKnowledge\u001B[39;00m:\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, chatbot_topic: \u001B[38;5;28mstr\u001B[39m, model_family: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mT5\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m      6\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchatbot_topic: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m chatbot_topic  \u001B[38;5;66;03m# The chatbot domain, used to export the knowledge as a csv file\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\Standard Files\\University\\Masters\\Diss\\MSc-Diss\\code\\modules\\knowledge.py:44\u001B[0m, in \u001B[0;36mKnowledge\u001B[1;34m()\u001B[0m\n\u001B[0;32m     37\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ModelNotSupportedError(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe EMBEDDING model type isn\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124mt currently supported. Please select from GPT, T5 and \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     38\u001B[0m                                      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBART.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# def get_embedding_model(self):\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;66;03m#     return GPT_EMBEDDING_MODEL if self.model_family == 'GPT' else GENERAL_EMBEDDING_MODEL\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m---> 44\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_blank_knowledge_df\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mDataFrame:\n\u001B[0;32m     45\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;124;03m    Creates a blank dataframe to contain the sections of knowledge.\u001B[39;00m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mDataFrame(columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSource\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHeading\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSubheading\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPage\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mContent\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# !pip install datasets, sentencepiece, transformers, accelerate, tiktoken, rouge_score, evaluate rouge_score bleu_score\n",
    "import sys\n",
    "sys.path.append(\"modules\")\n",
    "from modules.config import *\n",
    "from modules.knowledge import *\n",
    "from modules.chatbot import *\n",
    "from modules.embedding_functions import *\n",
    "from modules.data_extraction import *\n",
    "from modules.data_preprocessing import *\n",
    "from modules.gpt_ans_extraction import *\n",
    "from modules.query import *\n",
    "sys.path.remove(\"modules\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    log_and_print_message(f'Using GPU - details are as follows:')\n",
    "    log_and_print_message(f'__CUDNN VERSION: {torch.backends.cudnn.version()}')\n",
    "    log_and_print_message(f'__Number CUDA Devices: {torch.cuda.device_count()}')\n",
    "    log_and_print_message(f'__CUDA Device Name: {torch.cuda.get_device_name(0)}')\n",
    "    log_and_print_message(f'__CUDA Device Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9}')\n",
    "else:\n",
    "    log_and_print_message(f'No GPU available, using a CPU')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Knowledge Base"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "textbooks = ['Digital_Image_Processing_Textbook', 'Fundamentals_of_Digital_Image_Processing_Textbook']\n",
    "CompVisionKnowledge = Knowledge(CHATBOT_TOPIC) # Uses BERT embeddings to calculate cosine similarity\n",
    "print('here')\n",
    "for page in WIKI_PAGES:\n",
    "    CompVisionKnowledge.append_wikipedia_page(WIKI_PAGE)\n",
    "print('now here')\n",
    "for textbook in textbooks:\n",
    "    CompVisionKnowledge.append_pdf(f'assets/knowledge/{textbook}.pdf', textbook)\n",
    "CompVisionKnowledge.export_to_csv()\n",
    "CompVisionKnowledge.df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NQ Data Extraction and Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Data extraction\n",
    "training = AllData(cache_dir='/content/drive/MyDrive/Diss/Datasets', default='dev')\n",
    "training.export_simplified_dataset(path=\"/content/drive/MyDrive/Diss/Output/simplified_dataset_validation_new.csv\")\n",
    "\n",
    "### Data preprocessing\n",
    "training_data = TrainingData(save_dir=f'{OUTPUT_DIR}/all_data_{SHORT_MODEL_NAME}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the dataset and extract only the answerable questions\n",
    "OUTPUT_DIR = 'G:\\My Drive\\Diss\\Output'\n",
    "short_model_name = \"bart-large-xsum\"\n",
    "all_data = load_from_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}').shuffle(seed=9)\n",
    "all_ans_data = all_data.filter(lambda row: (row[\"answer\"] != NO_ANS_TOKEN))\n",
    "all_ans_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate through the dataset and query GPT to extract a natural-language answer\n",
    "splits = ['test', 'train']\n",
    "for s in splits:\n",
    "    all_timestamps = []\n",
    "    responses = []\n",
    "    # Obtain GPT answers/responses\n",
    "    for i in tqdm(range(len(all_ans_data[s]))): # Due to the RPM\n",
    "        all_timestamps = pause_if_needed(all_timestamps)\n",
    "\n",
    "        formatted_text = format_request(all_ans_data[s][i])\n",
    "        inputs = [\n",
    "                {\"role\": \"system\", \"content\": f\"You answer questions by only using a provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": formatted_text},\n",
    "            ]\n",
    "        response, all_timestamps = query_gpt(inputs, all_timestamps)\n",
    "        responses.append(response.choices[0].message.content)\n",
    "\n",
    "    # Export to txt file\n",
    "    file_name = f\"{s}_all.txt\"\n",
    "    # Open the file in write mode and save the list to the file\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        for item in responses:\n",
    "            file.write(item + \"\\n\")\n",
    "\n",
    "    # Export as HF dataset\n",
    "    df_pandas = all_ans_data[s].to_pandas()\n",
    "    df_pandas['gpt_ans'] = responses\n",
    "    new_dataset = Dataset.from_pandas(df_pandas)\n",
    "    new_dataset.save_to_disk(f\"{OUTPUT_DIR}/{short_model_name}_{s}_split\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now the new (GPT) answers can be merged with the original dataset\n",
    "# Load the original data and GPT data generated above\n",
    "train_dataset_pandas = all_data['train'].to_pandas()\n",
    "test_dataset_pandas = all_data['test'].to_pandas()\n",
    "# updated_train_dataset_pandas = load_from_disk(f\"assets/{short_model_name}_train_split\").to_pandas()\n",
    "# updated_test_dataset_pandas = load_from_disk(f\"assets/{short_model_name}_test_split\").to_pandas()\n",
    "updated_train_dataset_pandas = load_from_disk(f\"{OUTPUT_DIR}/{short_model_name}_train_split\").to_pandas()\n",
    "updated_test_dataset_pandas = load_from_disk(f\"{OUTPUT_DIR}/{short_model_name}_test_split\").to_pandas()\n",
    "\n",
    "# Merge on id and keep all rows\n",
    "train_merged = pd.merge(train_dataset_pandas, updated_train_dataset_pandas[['id', 'gpt_ans']], on='id', how='outer')\n",
    "test_merged = pd.merge(test_dataset_pandas, updated_test_dataset_pandas[['id', 'gpt_ans']], on='id', how='outer')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove any examples where it apparently had an answer but GPT couldn't extract oen\n",
    "train_merged = train_merged[train_merged['gpt_ans'] != NO_ANS_TOKEN]\n",
    "test_merged = test_merged[test_merged['gpt_ans'] !=  NO_ANS_TOKEN]\n",
    "\n",
    "# Update the answers to match the GPT ones\n",
    "train_merged.loc[train_merged['answer'] != NO_ANS_TOKEN, 'answer'] = train_merged.loc[train_merged['answer'] != NO_ANS_TOKEN, 'gpt_ans']\n",
    "test_merged.loc[test_merged['answer'] != NO_ANS_TOKEN, 'answer'] = test_merged.loc[test_merged['answer'] != NO_ANS_TOKEN, 'gpt_ans']\n",
    "\n",
    "merged_dataset = DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(train_merged),\n",
    "        \"test\": Dataset.from_pandas(test_merged),\n",
    "    })\n",
    "\n",
    "# Maintain the answerable/non-answerable balance\n",
    "def ensure_ans_non_ans_balance(dataset, dataset_splits=('training', 'validation'), proportion=0.3, seed=SEED):\n",
    "    # Extracting the unanswerable examples\n",
    "    no_ans = dataset.filter(lambda row: (row[\"answer\"] == NO_ANS_TOKEN))\n",
    "    good_ans = dataset.filter(lambda row: (row[\"answer\"] != NO_ANS_TOKEN))\n",
    "\n",
    "    # Discarding some unanswerable examples so the answer-no_ans ratio is favourable in each split\n",
    "    processed_datasets_dict = {}\n",
    "    for split in dataset_splits:\n",
    "      num_no_ans = proportion*len(good_ans[split])/(1-proportion)\n",
    "      no_ans_keep = no_ans[split].train_test_split(train_size=num_no_ans/len(no_ans[split]), seed=seed)['train']\n",
    "      processed_datasets_dict[split] = concatenate_datasets([no_ans_keep, good_ans[split]])\n",
    "\n",
    "    processed_dataset = DatasetDict({\n",
    "            dataset_splits[0]: processed_datasets_dict[dataset_splits[0]],\n",
    "            dataset_splits[1]: processed_datasets_dict[dataset_splits[1]],\n",
    "        })\n",
    "    shuffled_dataset = processed_dataset.shuffle(seed=seed)\n",
    "    return shuffled_dataset\n",
    "\n",
    "final_dataset = ensure_ans_non_ans_balance(merged_dataset, dataset_splits=['train', 'test'])\n",
    "\n",
    "# As can be seen, the 80/20 train-test split has been maintained and there's no error responses\n",
    "percentage = len(final_dataset['train'])/(len(final_dataset['train'])+len(final_dataset['test']))\n",
    "print(f'Train/test split: {percentage}')\n",
    "all_data.filter(lambda row: (row[\"answer\"] == 'error'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finally, the questions/contexts and answers need to be tokenised to override previous values\n",
    "def tokenise(data):\n",
    "    # tokenize the inputs (questions and contexts)\n",
    "\n",
    "    # FIX THIS AND CHECK WHERE MODEL_NAME IS BEING DEFINED AS GPT2 AS IT SHOULDN'T BE\n",
    "    tokeniser = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokeniser.pad_token is None:\n",
    "        print('Adding PAD token')\n",
    "        tokeniser.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokeniser.add_special_tokens({'additional_special_tokens': [NO_ANS_TOKEN]})\n",
    "\n",
    "    additional_cols = tokeniser(data['content'], data['question'], truncation=False)\n",
    "\n",
    "    # tokenize the answers\n",
    "    targets = tokeniser(text_target=data['answer'], truncation=False)\n",
    "\n",
    "    #set labels\n",
    "    additional_cols['labels'] = targets['input_ids']\n",
    "    additional_cols['num_tokens'] = [len(row) for row in additional_cols[\"input_ids\"]]\n",
    "    return additional_cols\n",
    "\n",
    "final_dataset = final_dataset.map(tokenise, batched = True)\n",
    "final_dataset.save_to_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}_gpt_updated')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NQ Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = \"google/mt5-small\"  # \"gpt2\", \"facebook/bart-large-xsum\"\n",
    "short_model_name = model_name.split('/')[1]\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_name) # NEED TO RENAME THIS\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\",\n",
    "                                              torch_dtype=\"auto\")\n",
    "tokeniser = add_special_tokens(tokeniser)\n",
    "model.resize_token_embeddings(len(tokeniser))\n",
    "\n",
    "# Load data\n",
    "all_data = load_from_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}_gpt_updated') # add or remove the suffix a required\n",
    "\n",
    "# Connect to HuggingFace\n",
    "HfFolder.save_token(\"ADD_TOKEN_HERE\")\n",
    "!git config --global user.email \"pointon.joel@gmail.com\"\n",
    "!git config --global user.name \"Joel Pointon\"\n",
    "\n",
    "# Training arguments/config\n",
    "batch_size = 8 # 64\n",
    "num_train_epochs = 8\n",
    "logging_steps = len(all_data[\"training\"]) // batch_size # Show the training loss with every epoch\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_name}_updated\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    push_to_hub=True,\n",
    "    seed=9,\n",
    "    # optim=\"adafactor\"\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokeniser.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokeniser.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokeniser.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores\n",
    "    # result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokeniser, model=model)\n",
    "\n",
    "def model_init():\n",
    "  return AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=all_data[\"train\"],\n",
    "    eval_dataset=all_data[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokeniser,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "path = f\"/content/drive/MyDrive/Diss/Output/{short_model_name}-finetuned-natural-questions\"\n",
    "trainer.train()\n",
    "trainer.push_to_hub()\n",
    "trainer.save_model(path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# NEED TO FIX THIS\n",
    "filename = 'assets/knowledge/Fundamentals_of_Digital_Image_Processing_Textbook.pdf'\n",
    "doc = fitz.open(filename)\n",
    "page_limit = None\n",
    "all_text = ''\n",
    "# Iterate through the content\n",
    "for page in doc:\n",
    "    page_limit = doc.page_count if not page_limit else page_limit\n",
    "    if page.number <= page_limit:\n",
    "        block_content = page.get_text(\"blocks\") #.encode(\"utf8\") # \"blocks\"\n",
    "        for block in block_content:\n",
    "            if block[6] == 0:  # I.e. only extract text\n",
    "                plain_text = unidecode(block[4])  # .decode('latin1') #.decode('utf-8')\n",
    "                all_text += plain_text\n",
    "    else:\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MLM Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extracting training content\n",
    "for textbook in textbooks:\n",
    "    all_text = ''.join(CompVisionKnowledge.df.loc[CompVisionKnowledge.df['Source']==textbook, 'Content'].tolist())\n",
    "    with open(f'assets/knowledge/{textbook}.txt', \"w\") as f:\n",
    "        f.write(all_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read txt files\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "# For formatting a PDF as a HF datasets\n",
    "def get_text_dataset(path):\n",
    "  dataset_obj = TextDataset(\n",
    "        tokenizer = tokeniser,\n",
    "        file_path = path,\n",
    "        block_size = 512,\n",
    "    )\n",
    "  return dataset_obj\n",
    "\n",
    "# Read documents from the directory\n",
    "training_file = 'assets/knowledge/Digital_Image_Processing_Textbook.txt'\n",
    "validation_file = 'assets/knowledge/Fundamentals_of_Digital_Image_Processing_Textbook.txt'\n",
    "train_data = read_txt(training_file)\n",
    "validation_data = read_txt(validation_file)\n",
    "train_data = re.sub(r'\\n+', '\\n', train_data).strip()  # Remove excess newline characters\n",
    "validation_data = re.sub(r'\\n+', '\\n', validation_data).strip()  # Remove excess newline characters\n",
    "\n",
    "with open(f'{training_file}_formatted', \"w\") as f:\n",
    "    f.write(train_data)\n",
    "with open(f'{validation_file}_formatted', \"w\") as f:\n",
    "    f.write(validation_data)\n",
    "\n",
    "model_name = 'gpt2'\n",
    "output_dir = 'models/custom_q_and_a'\n",
    "batch_size = 8\n",
    "num_train_epochs = 8\n",
    "\n",
    "# Train\n",
    "tokeniser = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "if tokeniser.pad_token is None:\n",
    "    tokeniser.add_special_tokens({'pad_token': '[PAD]', 'mask_token': '[MASK]'})\n",
    "    model.resize_token_embeddings(len(tokeniser))\n",
    "\n",
    "\n",
    "train_dataset = get_text_dataset(f'{training_file}_formatted')\n",
    "validation_dataset = get_text_dataset(f'{validation_file}_formatted')\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokeniser,\n",
    "        mlm=True,\n",
    "    )\n",
    "\n",
    "HF_REFERENCE='mlm'\n",
    "logging_steps = len(train_dataset) // batch_size // 4\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=HF_REFERENCE,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=logging_steps,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    push_to_hub=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    ")\n",
    "trainer.train()\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "trainer.push_to_hub()\n",
    "tokeniser.push_to_hub(f'psxjp5/{HF_REFERENCE}')\n",
    "print('Finished training and pushed to hub!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Todo:\n",
    "# Adapt for the MLM and QA models\n",
    "# Check num_tokens usage and specifying the encoding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CompVisionChatbot = ChatBot(CHATBOT_TOPIC)\n",
    "print(Query.ask_gpt('When did Universities begin teaching Computer Vision?', CompVisionChatbot, show_source=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(Query.ask_finetuned('When did Universities begin teaching Computer Vision?', CompVisionChatbot, hf_reference=MLM_HF_REFERENCE, show_source=True))  # What if the GPT knowledge sections are longer than 1024 tokens?? Need to account for this!\n",
    "\n",
    "# 'What is PCA?'\n",
    "\n",
    "question_answering = pipeline(model=f'psxjp5/mlm')\n",
    "print(question_answering(text)[0]['generated_text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
