{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "R6HtkSCAFby9"
      },
      "source": [
        "# GPT-3.5-Turbo Model\n",
        "Creating a question answering chatbot using GPT-3.5. Adapted from: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw5KQLS8Fd6P",
        "outputId": "d605cc45-6e42-4175-c367-77da7992513b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "try:\n",
        "  os.chdir('drive/MyDrive/Diss/Repo/MSc-Diss/code')\n",
        "except:\n",
        "  !pwd\n",
        "else:\n",
        "  !pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1Fe0HRGFdZp",
        "outputId": "f20c8360-3718-4074-c694-458fd2a01bc6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Diss/Repo/MSc-Diss/code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets\n",
        "!pip install PyPDF2 openai tiktoken wikipedia mwparserfromhell frontend pymupdf\n",
        "!pip install unidecode datasets==2.13.1 sentencepiece transformers rouge_score\n",
        "!pip install sacrebleu meteor evaluate sentence_transformers accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqVGfSBSGJ8U",
        "outputId": "e788beb8-c1a4-49c3-9a1a-16a47ff4aab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mwparserfromhell\n",
            "  Downloading mwparserfromhell-0.6.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frontend\n",
            "  Downloading frontend-0.0.3-py3-none-any.whl (32 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.22.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Collecting starlette>=0.12.0 (from frontend)\n",
            "  Downloading starlette-0.31.0-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.8/69.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn>=0.7.1 (from frontend)\n",
            "  Downloading uvicorn-0.23.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from frontend) (2.1.2)\n",
            "Collecting aiofiles (from frontend)\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette>=0.12.0->frontend) (3.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (8.1.6)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.7.1->frontend)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (4.7.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (1.1.2)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=c2245585824ac2d71d04b088a94cc85d4470c3b01e2b59977b25e649ebcee0b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: PyPDF2, pymupdf, mwparserfromhell, h11, aiofiles, wikipedia, uvicorn, tiktoken, starlette, openai, frontend\n",
            "Successfully installed PyPDF2-3.0.1 aiofiles-23.1.0 frontend-0.0.3 h11-0.14.0 mwparserfromhell-0.6.4 openai-0.27.8 pymupdf-1.22.5 starlette-0.31.0 tiktoken-0.4.0 uvicorn-0.23.1 wikipedia-1.4.0\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==2.13.1\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets==2.13.1)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (4.65.0)\n",
            "Collecting xxhash (from datasets==2.13.1)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets==2.13.1)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets==2.13.1)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.13.1) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.13.1) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.13.1) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.13.1) (3.4)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.13.1)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.13.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.13.1) (2022.7.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=b468a49ac997d5a8f4cd2eb3eccd8740b0815e55a569460c64674d10edddea26\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, xxhash, unidecode, dill, rouge_score, multiprocess, huggingface-hub, transformers, datasets\n",
            "Successfully installed datasets-2.13.1 dill-0.3.6 huggingface-hub-0.16.4 multiprocess-0.70.14 rouge_score-0.1.2 safetensors-0.3.1 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.31.0 unidecode-1.3.6 xxhash-3.2.0\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting meteor\n",
            "  Downloading meteor-0.1.0-py3-none-any.whl (4.2 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2022.10.31)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.22.4)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.13.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.31.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=346b81399866833b13f72bfbc1cec8247406c54ce3572001b335f860bbe244b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: portalocker, meteor, colorama, sacrebleu, responses, evaluate, sentence_transformers, accelerate\n",
            "Successfully installed accelerate-0.21.0 colorama-0.4.6 evaluate-0.4.0 meteor-0.1.0 portalocker-2.7.0 responses-0.18.0 sacrebleu-2.3.1 sentence_transformers-2.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"modules\")\n",
        "from modules.config import *\n",
        "from modules.knowledge import *\n",
        "from modules.chatbot import *\n",
        "from modules.embedding_functions import *\n",
        "from modules.data_extraction import *\n",
        "from modules.data_preprocessing import *\n",
        "from modules.gpt_ans_extraction import *\n",
        "from modules.query import *\n",
        "sys.path.remove(\"modules\")\n",
        "openai.api_key = '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'"
      ],
      "metadata": {
        "id": "war9mPMFFbzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Knowledge Base"
      ],
      "metadata": {
        "collapsed": false,
        "id": "WLIPkQZ7FbzE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following PDF has been successfully added to the knowledge database: Digital_Image_Processing_Textbook (assets/knowledge/Digital_Image_Processing_Textbook.pdf)\n",
            "The following PDF has been successfully added to the knowledge database: Fundamentals_of_Digital_Image_Processing_Textbook (assets/knowledge/Fundamentals_of_Digital_Image_Processing_Textbook.pdf)\n"
          ]
        },
        {
          "data": {
            "text/plain": "                                                Source Heading Subheading  \\\n0                    Digital_Image_Processing_Textbook                      \n1                    Digital_Image_Processing_Textbook                      \n2                    Digital_Image_Processing_Textbook                      \n3                    Digital_Image_Processing_Textbook                      \n4                    Digital_Image_Processing_Textbook                      \n..                                                 ...     ...        ...   \n224  Fundamentals_of_Digital_Image_Processing_Textbook                      \n225  Fundamentals_of_Digital_Image_Processing_Textbook                      \n226  Fundamentals_of_Digital_Image_Processing_Textbook                      \n227  Fundamentals_of_Digital_Image_Processing_Textbook                      \n228  Fundamentals_of_Digital_Image_Processing_Textbook                      \n\n                            Page  \\\n0                        0/1/2/3   \n1                  3/4/5/6/7/8/9   \n2                        9/10/11   \n3                          11/12   \n4                          12/13   \n..                           ...   \n224                      341/342   \n225                  342/343/344   \n226                      344/345   \n227  345/346/347/348/349/350/351   \n228              351/352/353/354   \n\n                                               Content  \\\n0    \\nGLOBAL \\nEDITION\\nDigital Image Processing\\n...   \n1    \\nISBN 10: 1-292-22304-9\\nISBN 13: 978-1-292-2...   \n2    \\nBackground  904\\nPatterns and Pattern Classe...   \n3    \\nChapter 5: Revisions to this chapter were li...   \n4    \\nChapter 12: This chapter underwent a major r...   \n..                                                 ...   \n224  \\nkey elements, 21\\nmathematics of, 22 37\\ncon...   \n225  \\nMagnetic resonance imaging (MRI), 49\\nMahala...   \n226  \\nPeriodic square wave synthesis, 117\\nPerspec...   \n227  \\nSalt and pepper noise, 46, 90, 91, 93\\nSampl...   \n228  \\nbottom, we have: (a) the original image with...   \n\n                                               Section  Tokens  \\\n0    Digital_Image_Processing_Textbook->Page(s)0/1/2/3   791.0   \n1    Digital_Image_Processing_Textbook->Page(s)3/4/...   793.0   \n2    Digital_Image_Processing_Textbook->Page(s)9/10/11   777.0   \n3      Digital_Image_Processing_Textbook->Page(s)11/12   413.0   \n4      Digital_Image_Processing_Textbook->Page(s)12/13   563.0   \n..                                                 ...     ...   \n224  Fundamentals_of_Digital_Image_Processing_Textb...   771.0   \n225  Fundamentals_of_Digital_Image_Processing_Textb...   640.0   \n226  Fundamentals_of_Digital_Image_Processing_Textb...   591.0   \n227  Fundamentals_of_Digital_Image_Processing_Textb...   780.0   \n228  Fundamentals_of_Digital_Image_Processing_Textb...   476.0   \n\n                                             Embedding  \n0    [-0.021132638677954674, 0.025948533788323402, ...  \n1    [0.005928434897214174, 0.01934611052274704, -0...  \n2    [-0.011725608259439468, 0.021519940346479416, ...  \n3    [0.013898221775889397, 0.013536868616938591, -...  \n4    [-0.005244759377092123, 0.018554115667939186, ...  \n..                                                 ...  \n224  [0.0032916401978582144, 0.0268571637570858, -0...  \n225  [-0.009114357642829418, 0.00879641529172659, 0...  \n226  [-0.011132902465760708, 0.001304946606978774, ...  \n227  [0.015710284933447838, 0.001202668296173215, 0...  \n228  [-0.0019758823327720165, 0.0056046489626169205...  \n\n[1059 rows x 8 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Source</th>\n      <th>Heading</th>\n      <th>Subheading</th>\n      <th>Page</th>\n      <th>Content</th>\n      <th>Section</th>\n      <th>Tokens</th>\n      <th>Embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>0/1/2/3</td>\n      <td>\\nGLOBAL \\nEDITION\\nDigital Image Processing\\n...</td>\n      <td>Digital_Image_Processing_Textbook-&gt;Page(s)0/1/2/3</td>\n      <td>791.0</td>\n      <td>[-0.021132638677954674, 0.025948533788323402, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>3/4/5/6/7/8/9</td>\n      <td>\\nISBN 10: 1-292-22304-9\\nISBN 13: 978-1-292-2...</td>\n      <td>Digital_Image_Processing_Textbook-&gt;Page(s)3/4/...</td>\n      <td>793.0</td>\n      <td>[0.005928434897214174, 0.01934611052274704, -0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>9/10/11</td>\n      <td>\\nBackground  904\\nPatterns and Pattern Classe...</td>\n      <td>Digital_Image_Processing_Textbook-&gt;Page(s)9/10/11</td>\n      <td>777.0</td>\n      <td>[-0.011725608259439468, 0.021519940346479416, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>11/12</td>\n      <td>\\nChapter 5: Revisions to this chapter were li...</td>\n      <td>Digital_Image_Processing_Textbook-&gt;Page(s)11/12</td>\n      <td>413.0</td>\n      <td>[0.013898221775889397, 0.013536868616938591, -...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>12/13</td>\n      <td>\\nChapter 12: This chapter underwent a major r...</td>\n      <td>Digital_Image_Processing_Textbook-&gt;Page(s)12/13</td>\n      <td>563.0</td>\n      <td>[-0.005244759377092123, 0.018554115667939186, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>224</th>\n      <td>Fundamentals_of_Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>341/342</td>\n      <td>\\nkey elements, 21\\nmathematics of, 22 37\\ncon...</td>\n      <td>Fundamentals_of_Digital_Image_Processing_Textb...</td>\n      <td>771.0</td>\n      <td>[0.0032916401978582144, 0.0268571637570858, -0...</td>\n    </tr>\n    <tr>\n      <th>225</th>\n      <td>Fundamentals_of_Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>342/343/344</td>\n      <td>\\nMagnetic resonance imaging (MRI), 49\\nMahala...</td>\n      <td>Fundamentals_of_Digital_Image_Processing_Textb...</td>\n      <td>640.0</td>\n      <td>[-0.009114357642829418, 0.00879641529172659, 0...</td>\n    </tr>\n    <tr>\n      <th>226</th>\n      <td>Fundamentals_of_Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>344/345</td>\n      <td>\\nPeriodic square wave synthesis, 117\\nPerspec...</td>\n      <td>Fundamentals_of_Digital_Image_Processing_Textb...</td>\n      <td>591.0</td>\n      <td>[-0.011132902465760708, 0.001304946606978774, ...</td>\n    </tr>\n    <tr>\n      <th>227</th>\n      <td>Fundamentals_of_Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>345/346/347/348/349/350/351</td>\n      <td>\\nSalt and pepper noise, 46, 90, 91, 93\\nSampl...</td>\n      <td>Fundamentals_of_Digital_Image_Processing_Textb...</td>\n      <td>780.0</td>\n      <td>[0.015710284933447838, 0.001202668296173215, 0...</td>\n    </tr>\n    <tr>\n      <th>228</th>\n      <td>Fundamentals_of_Digital_Image_Processing_Textbook</td>\n      <td></td>\n      <td></td>\n      <td>351/352/353/354</td>\n      <td>\\nbottom, we have: (a) the original image with...</td>\n      <td>Fundamentals_of_Digital_Image_Processing_Textb...</td>\n      <td>476.0</td>\n      <td>[-0.0019758823327720165, 0.0056046489626169205...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1059 rows × 8 columns</p>\n</div>"
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "textbooks = ['Digital_Image_Processing_Textbook', 'Fundamentals_of_Digital_Image_Processing_Textbook']\n",
        "CompVisionKnowledge = Knowledge(CHATBOT_TOPIC, gpt=True)\n",
        "# for page in WIKI_PAGES:\n",
        "#     CompVisionKnowledge.append_wikipedia_page(WIKI_PAGE)\n",
        "for textbook in textbooks:\n",
        "    CompVisionKnowledge.append_pdf(f'assets/knowledge/{textbook}.pdf', textbook)\n",
        "CompVisionKnowledge.export_to_csv()\n",
        "CompVisionKnowledge.df"
      ],
      "metadata": {
        "id": "pjpyDHzoFbzF",
        "outputId": "6368ef58-6b58-4725-e8ab-92869dce61fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NQ Data Extraction and Preprocessing"
      ],
      "metadata": {
        "collapsed": false,
        "id": "iNPvMytFFbzF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "### Data extraction\n",
        "training = AllData(cache_dir='/content/drive/MyDrive/Diss/Datasets', default='dev')\n",
        "training.export_simplified_dataset(path=\"/content/drive/MyDrive/Diss/Output/simplified_dataset_validation_new.csv\")\n",
        "\n",
        "### Data preprocessing\n",
        "model_name = \"google/mt5-small\" # This will dictate the model used for all the training\n",
        "# model_name = \"facebook/bart-large-xsum\"\n",
        "short_model_name = model_name.split('/')[1]\n",
        "model_output_name = f\"{short_model_name}_epochs_new_new\"\n",
        "HF_reference = f\"psxjp5/{model_output_name}\"\n",
        "tokeniser = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\",\n",
        "                                              torch_dtype=\"auto\")\n",
        "model.config.hidden_dropout_prob = 0.1 # For regularisation\n",
        "tokeniser = add_special_tokens(tokeniser)\n",
        "model.resize_token_embeddings(len(tokeniser))\n",
        "\n",
        "training_data = TrainingData(tokeniser=tokeniser, save_dir=f'{OUTPUT_DIR}/all_data_{short_model_name}')"
      ],
      "metadata": {
        "id": "zj-2Pc1CFbzG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load the dataset and extract only the answerable questions\n",
        "all_data = load_from_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}').shuffle(seed=9)\n",
        "all_ans_data = all_data.filter(lambda row: (row[\"answer\"] != NO_ANS_TOKEN))\n",
        "all_ans_data"
      ],
      "metadata": {
        "id": "PkZzWzlBFbzG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Iterate through the dataset and query GPT to extract a natural-language answer\n",
        "splits = ['test', 'train']\n",
        "for s in splits:\n",
        "    all_timestamps = []\n",
        "    responses = []\n",
        "    # Obtain GPT answers/responses\n",
        "    for i in tqdm(range(len(all_ans_data[s]))): # Due to the RPM\n",
        "        all_timestamps = pause_if_needed(all_timestamps)\n",
        "\n",
        "        formatted_text = format_request(all_ans_data[s][i])\n",
        "        inputs = [\n",
        "                {\"role\": \"system\", \"content\": f\"You answer questions by only using a provided context.\"},\n",
        "                {\"role\": \"user\", \"content\": formatted_text},\n",
        "            ]\n",
        "        response, all_timestamps = query_gpt(inputs, all_timestamps)\n",
        "        responses.append(response.choices[0].message.content)\n",
        "\n",
        "    # Export as HF dataset\n",
        "    df_pandas = all_ans_data[s].to_pandas()\n",
        "    df_pandas['gpt_ans'] = responses\n",
        "    new_dataset = Dataset.from_pandas(df_pandas)\n",
        "    new_dataset.save_to_disk(f\"{OUTPUT_DIR}/{short_model_name}_{s}_split\")"
      ],
      "metadata": {
        "id": "QSDccGFHFbzH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Now the new (GPT) answers can be merged with the original dataset\n",
        "# Load the original data and GPT data generated above\n",
        "train_dataset_pandas = all_data['train'].to_pandas()\n",
        "test_dataset_pandas = all_data['test'].to_pandas()\n",
        "# updated_train_dataset_pandas = load_from_disk(f\"assets/{short_model_name}_train_split\").to_pandas()\n",
        "# updated_test_dataset_pandas = load_from_disk(f\"assets/{short_model_name}_test_split\").to_pandas()\n",
        "updated_train_dataset_pandas = load_from_disk(f\"{OUTPUT_DIR}/{short_model_name}_train_split\").to_pandas()\n",
        "updated_test_dataset_pandas = load_from_disk(f\"{OUTPUT_DIR}/{short_model_name}_test_split\").to_pandas()\n",
        "\n",
        "# Merge on id and keep all rows\n",
        "train_merged = pd.merge(train_dataset_pandas, updated_train_dataset_pandas[['id', 'gpt_ans']], on='id', how='outer')\n",
        "test_merged = pd.merge(test_dataset_pandas, updated_test_dataset_pandas[['id', 'gpt_ans']], on='id', how='outer')"
      ],
      "metadata": {
        "id": "AEJyhITzFbzH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Remove any examples where it apparently had an answer but GPT couldn't extract oen\n",
        "train_merged = train_merged[train_merged['gpt_ans'] != NO_ANS_TOKEN]\n",
        "test_merged = test_merged[test_merged['gpt_ans'] !=  NO_ANS_TOKEN]\n",
        "\n",
        "# Update the answers to match the GPT ones\n",
        "train_merged.loc[train_merged['answer'] != NO_ANS_TOKEN, 'answer'] = train_merged.loc[train_merged['answer'] != NO_ANS_TOKEN, 'gpt_ans']\n",
        "test_merged.loc[test_merged['answer'] != NO_ANS_TOKEN, 'answer'] = test_merged.loc[test_merged['answer'] != NO_ANS_TOKEN, 'gpt_ans']\n",
        "\n",
        "merged_dataset = DatasetDict({\n",
        "        \"train\": Dataset.from_pandas(train_merged),\n",
        "        \"test\": Dataset.from_pandas(test_merged),\n",
        "    })\n",
        "\n",
        "# Maintain the answerable/non-answerable balance\n",
        "def ensure_ans_non_ans_balance(dataset, dataset_splits=('training', 'validation'), proportion=0.3, seed=SEED):\n",
        "    # Extracting the unanswerable examples\n",
        "    no_ans = dataset.filter(lambda row: (row[\"answer\"] == NO_ANS_TOKEN))\n",
        "    good_ans = dataset.filter(lambda row: (row[\"answer\"] != NO_ANS_TOKEN))\n",
        "\n",
        "    # Discarding some unanswerable examples so the answer-no_ans ratio is favourable in each split\n",
        "    processed_datasets_dict = {}\n",
        "    for split in dataset_splits:\n",
        "      num_no_ans = proportion*len(good_ans[split])/(1-proportion)\n",
        "      no_ans_keep = no_ans[split].train_test_split(train_size=num_no_ans/len(no_ans[split]), seed=seed)['train']\n",
        "      processed_datasets_dict[split] = concatenate_datasets([no_ans_keep, good_ans[split]])\n",
        "\n",
        "    processed_dataset = DatasetDict({\n",
        "            dataset_splits[0]: processed_datasets_dict[dataset_splits[0]],\n",
        "            dataset_splits[1]: processed_datasets_dict[dataset_splits[1]],\n",
        "        })\n",
        "    shuffled_dataset = processed_dataset.shuffle(seed=seed)\n",
        "    return shuffled_dataset\n",
        "\n",
        "final_dataset = ensure_ans_non_ans_balance(merged_dataset, dataset_splits=['train', 'test'])\n",
        "\n",
        "# As can be seen, the 80/20 train-test split has been maintained and there's no error responses\n",
        "percentage = len(final_dataset['train'])/(len(final_dataset['train'])+len(final_dataset['test']))\n",
        "print(f'Train/test split: {percentage}')\n",
        "all_data.filter(lambda row: (row[\"answer\"] == 'error'))"
      ],
      "metadata": {
        "id": "yzEaDliWFbzI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Finally, the questions/contexts and answers need to be tokenised to override previous values\n",
        "def tokenise(data):\n",
        "    # tokenize the inputs (questions and contexts)\n",
        "    additional_cols = tokeniser(data['content'], data['question'], truncation=False)\n",
        "\n",
        "    # tokenize the answers\n",
        "    targets = tokeniser(text_target=data['answer'], truncation=False)\n",
        "\n",
        "    #set labels\n",
        "    additional_cols['labels'] = targets['input_ids']\n",
        "    additional_cols['num_tokens'] = [len(row) for row in additional_cols[\"input_ids\"]]\n",
        "    return additional_cols\n",
        "\n",
        "final_dataset = final_dataset.map(tokenise, batched = True)\n",
        "final_dataset.save_to_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}_gpt_updated')"
      ],
      "metadata": {
        "id": "StYjI9YoFbzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NQ Model Training"
      ],
      "metadata": {
        "collapsed": false,
        "id": "ZNyPp28MFbzI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load the GPT updated data\n",
        "all_data = load_from_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}_gpt_updated') # add or remove the suffix a required\n",
        "\n",
        "# Training arguments/config\n",
        "batch_size = 8\n",
        "num_train_epochs = 20\n",
        "logging_steps = len(all_data[\"train\"]) // batch_size // 4 # Show the training loss with every epoch\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=model_output_name,\n",
        "    logging_steps=logging_steps,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_steps=logging_steps,\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    predict_with_generate=True,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    metric_for_best_model = 'bleu',\n",
        "    load_best_model_at_end = True,\n",
        "    push_to_hub=True,\n",
        "    seed=9\n",
        ")\n",
        "\n",
        "# For model evaluation/metric computation\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "def compute_rouge(decoded_preds, decoded_labels):\n",
        "    # ROUGE expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    result = rouge_score.compute(\n",
        "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
        "    )\n",
        "\n",
        "    # Extract the median scores\n",
        "    results = {key: value * 100 for key, value in result.items()}\n",
        "    return {k: round(v, 4) for k, v in results.items()}\n",
        "\n",
        "def compute_bleu(decoded_preds, decoded_labels, preds):\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "    result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokeniser.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    return {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "def compute_meteor(decoded_preds, decoded_labels):\n",
        "    result = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "def compute_perc_noans_acc(decoded_preds, decoded_labels):\n",
        "    total = len(decoded_labels)\n",
        "    correct = sum(1 for pred, label in zip(decoded_preds, decoded_labels) if (pred == label and label==''))\n",
        "\n",
        "    return {'No Ans Accuracy': round(correct/total, 4)*100}\n",
        "\n",
        "def compute_cosine_similarity(decoded_preds, decoded_labels):\n",
        "    cosine_sims = []\n",
        "    for pred, label in zip(decoded_preds, decoded_labels):\n",
        "      if label != '':  # i.e. Is answerable\n",
        "        encoded_pred = GENERAL_EMBEDDING_MODEL.encode(pred)\n",
        "        encoded_label = GENERAL_EMBEDDING_MODEL.encode(label)\n",
        "        similarity = 1 - spatial.distance.cosine(encoded_pred, encoded_label)\n",
        "        cosine_sims.append(similarity)\n",
        "\n",
        "    mean = sum(cosine_sims)/len(cosine_sims)\n",
        "\n",
        "    return {'Av Cosine Sim': round(mean, 4)}\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    # Decode generated answers\n",
        "    decoded_preds = tokeniser.batch_decode(preds, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, tokeniser.pad_token_id)\n",
        "    # Decode target answers\n",
        "    decoded_labels = tokeniser.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Compute metrics\n",
        "    rouge_results = compute_rouge(decoded_preds, decoded_labels)\n",
        "    bleu_results = compute_bleu(decoded_preds, decoded_labels, preds)\n",
        "    meteor_results = compute_meteor(decoded_preds, decoded_labels)\n",
        "    noans_acc = compute_perc_noans_acc(decoded_preds, decoded_labels)\n",
        "    cosine_sim = compute_cosine_similarity(decoded_preds, decoded_labels)\n",
        "\n",
        "    all_results = {**rouge_results, **bleu_results, **meteor_results, **noans_acc, **cosine_sim}\n",
        "\n",
        "    return all_results\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokeniser, model=model)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=all_data[\"train\"],\n",
        "    eval_dataset=all_data[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokeniser,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.1)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.push_to_hub()\n",
        "trainer.save_model(model_output_name)"
      ],
      "metadata": {
        "id": "WA07MQT_FbzJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLM Training"
      ],
      "metadata": {
        "collapsed": false,
        "id": "aEAvODlWFbzJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Extracting training content\n",
        "for textbook in textbooks:\n",
        "    all_text = ''.join(CompVisionKnowledge.df.loc[CompVisionKnowledge.df['Source']==textbook, 'Content'].tolist())\n",
        "    with open(f'assets/knowledge/{textbook}.txt', \"w\") as f:\n",
        "        f.write(all_text)"
      ],
      "metadata": {
        "id": "2m4d2Ye_FbzJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Read txt files\n",
        "def read_txt(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def get_text_dataset(path):\n",
        "  dataset_obj = TextDataset(\n",
        "        tokenizer = tokeniser,\n",
        "        file_path = path,\n",
        "        block_size = 512,\n",
        "    )\n",
        "  return dataset_obj\n",
        "\n",
        "# Read documents from the directory\n",
        "training_file = '/content/drive/MyDrive/Diss/Datasets/Digital_Image_Processing_Textbook.txt'\n",
        "validation_file = '/content/drive/MyDrive/Diss/Datasets/Fundamentals_of_Digital_Image_Processing_Textbook.txt'\n",
        "train_data = read_txt(training_file)\n",
        "validation_data = read_txt(validation_file)\n",
        "train_data = re.sub(r'\\n+', '\\n', train_data).strip()  # Remove excess newline characters\n",
        "validation_data = re.sub(r'\\n+', '\\n', validation_data).strip()  # Remove excess newline characters\n",
        "\n",
        "with open(f'{training_file}_formatted', \"w\") as f:\n",
        "    f.write(train_data)\n",
        "with open(f'{validation_file}_formatted', \"w\") as f:\n",
        "    f.write(validation_data)\n",
        "\n",
        "batch_size = 8\n",
        "num_train_epochs = 50\n",
        "\n",
        "# Train\n",
        "gpt_tokeniser = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "if gpt_tokeniser.pad_token is None:\n",
        "    gpt_tokeniser.add_special_tokens({'pad_token': '[PAD]', 'mask_token': '[MASK]'})\n",
        "    gpt_model.resize_token_embeddings(len(gpt_tokeniser))\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=gpt_tokeniser,\n",
        "        mlm=True,\n",
        "    )\n",
        "\n",
        "train_dataset = get_text_dataset(f'{training_file}_formatted')\n",
        "validation_dataset = get_text_dataset(f'{validation_file}_formatted')\n",
        "\n",
        "HF_REFERENCE='mlm_new'\n",
        "logging_steps = len(train_dataset) // batch_size // 4\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=HF_REFERENCE,\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_steps=logging_steps,\n",
        "    save_steps=logging_steps,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    metric_for_best_model = 'loss',\n",
        "    load_best_model_at_end = True,\n",
        "    save_total_limit=3,\n",
        "    push_to_hub=True,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=gpt_model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=0.05)]\n",
        ")\n",
        "trainer.train()\n",
        "import math\n",
        "# eval_results = trainer.evaluate()\n",
        "# print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
        "trainer.push_to_hub()\n",
        "tokeniser.push_to_hub(f'psxjp5/{HF_REFERENCE}')\n",
        "print('Finished training and pushed to hub!')"
      ],
      "metadata": {
        "id": "eOb0cUvfFbzJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Te9T2G4mFbzK"
      },
      "source": [
        "# Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGlSBQ5oFbzK"
      },
      "outputs": [],
      "source": [
        "# Todo:\n",
        "# Check num_tokens usage and specifying the encoding model\n",
        "CompVisionChatbot = ChatBot(CHATBOT_TOPIC, hf_reference=T5_QA_GPT_HF_REFERENCE, embedding='gpt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "error_msg = 'Please select either GPT or T5.'\n",
        "endings = ['goodbye', 'bye', 'exit', 'quit']\n",
        "model_chosen = False\n",
        "while not model_chosen:\n",
        "  model = input(\"Please select GPT or T5: \")\n",
        "  if model.lower() in ['gpt', 't5']:\n",
        "    print(\"Model loaded, ready for questions.\")\n",
        "    model_chosen = True\n",
        "  else:\n",
        "    print(error_msg)\n",
        "while True:\n",
        "  question = input(\"User: \")\n",
        "  if question in endings:\n",
        "    break\n",
        "  formatted_question = question if question[0]=='?' else f'{question}?'\n",
        "  if model.lower().strip() == 'gpt':\n",
        "    response = Query.ask_gpt(formatted_question, CompVisionChatbot, show_source=True)\n",
        "  elif model.lower().strip() == 't5':\n",
        "    response = Query.ask_finetuned(formatted_question, CompVisionChatbot, show_source=True)\n",
        "  else:\n",
        "    print(error_msg)\n",
        "    break\n",
        "  dedented_text = textwrap.dedent(response)\n",
        "  paragraphs = response.strip().split('\\n\\n')  # Split into paragraphs based on double line breaks\n",
        "  for paragraph in paragraphs:\n",
        "      dedented_text = textwrap.dedent(paragraph)\n",
        "      lines = textwrap.fill(dedented_text, width=100).splitlines()\n",
        "      for line in lines:\n",
        "          for char in line:\n",
        "              print(char, end='', flush=True)\n",
        "              time.sleep(0.015)\n",
        "          print()  # Print a newline after each line\n",
        "      print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfEQEqmcKkuW",
        "outputId": "6dbef37d-44b5-4322-8ef1-4351c3263085"
      },
      "execution_count": 36,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please select GPT or T5: gpt\n",
            "Model loaded, ready for questions.\n",
            "User: what is pca\n",
            "PCA stands for Principal Component Analysis. It is a statistical technique used for dimensionality\n",
            "reduction and data compression. It involves finding the principal components of a dataset, which are\n",
            "new variables that are linear combinations of the original variables. These principal components are\n",
            "chosen in such a way that they capture the maximum amount of variance in the data. PCA is commonly\n",
            "used in computer vision for tasks such as face recognition and image compression.\n",
            "\n",
            "To construct this answer, I used the following documents :\n",
            "\n",
            "1. Fundamentals_of_Digital_Image_Processing_Textbook->Page(s)278/279/280/281:  When we carry out a\n",
            "PCA on this ensemble of faces, we find that the eigenvalue spectrum  dies off ...\n",
            "\n",
            "2. Fundamentals_of_Digital_Image_Processing_Textbook->Page(s)265/266/267:  To grasp the essence of\n",
            "PCA, suppose we have a large sample of M children and that we wish  to ass...\n",
            "\n",
            "3. Fundamentals_of_Digital_Image_Processing_Textbook->Page(s)275/276/277:  In the context of pattern\n",
            "recognition and classification applications, we speak of training  data ...\n",
            "\n",
            "4. Fundamentals_of_Digital_Image_Processing_Textbook->Page(s)274/275:  Depending on the precise\n",
            "context, there may be several closely related reasons for doing  PCA, but...\n",
            "\n",
            "User: bye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8G8z3GXcH1-v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}