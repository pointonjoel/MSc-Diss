{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GPT-3.5-Turbo Model\n",
    "Creating a question answering chatbot using GPT-3.5. Adapted from: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_tokens() IS OFTEN USED IN KNOWLEDGE.PY (and other files??) WITHOUT SPECIFYING THE EMBEDDING MODEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using a CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\point\\anaconda3\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using a CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets, sentencepiece, transformers, accelerate, tiktoken, rouge_score, evaluate rouge_score bleu_score\n",
    "import sys\n",
    "\n",
    "import openai.error\n",
    "\n",
    "sys.path.append(\"modules\")\n",
    "from modules.config import *\n",
    "from modules.knowledge import *\n",
    "from modules.chatbot import *\n",
    "from modules.embedding_functions import *\n",
    "from modules.data_extraction import *\n",
    "from modules.data_preprocessing import *\n",
    "from modules.gpt_ans_extraction import *\n",
    "from modules.query import *\n",
    "sys.path.remove(\"modules\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Knowledge Base"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "textbooks = ['Digital_Image_Processing_Textbook', 'Fundamentals_of_Digital_Image_Processing_Textbook']\n",
    "CompVisionKnowledge = Knowledge('CompVisionPDF', 'BERT') # Use BERT embeddings to calculate cosine similarity\n",
    "for page in WIKI_PAGES:\n",
    "    CompVisionKnowledge.append_wikipedia_page(WIKI_PAGE)\n",
    "for textbook in textbooks:\n",
    "    CompVisionKnowledge.append_pdf(f'assets/knowledge/{textbook}.pdf', textbook)\n",
    "CompVisionKnowledge.export_to_csv(GPT_KNOWLEDGE_FILENAME)\n",
    "CompVisionKnowledge.df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NQ Data Extraction and Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Data extraction\n",
    "training = AllData(cache_dir='/content/drive/MyDrive/Diss/Datasets', default='dev') # NEED TO CORRECT THIS (train/test separation for efficiency)\n",
    "training.export_simplified_dataset(path=\"/content/drive/MyDrive/Diss/Output/simplified_dataset_validation_new.csv\")\n",
    "\n",
    "### Data preprocessing\n",
    "training_data = TrainingData(save_dir=f'{OUTPUT_DIR}/all_data_{short_model_name}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'url', 'content', 'question', 'answer', 'start_token', 'end_token', 'length', 'input_ids', 'attention_mask', 'labels', 'num_tokens'],\n        num_rows: 11504\n    })\n    test: Dataset({\n        features: ['id', 'title', 'url', 'content', 'question', 'answer', 'start_token', 'end_token', 'length', 'input_ids', 'attention_mask', 'labels', 'num_tokens'],\n        num_rows: 2876\n    })\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset and extract only the answerable questions\n",
    "OUTPUT_DIR = 'G:\\My Drive\\Diss\\Output'\n",
    "short_model_name = \"bart-large-xsum\"\n",
    "all_data = load_from_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}').shuffle(seed=9)\n",
    "all_ans_data = all_data.filter(lambda row: (row[\"answer\"] != NO_ANS_TOKEN))\n",
    "all_ans_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 86/2876 [02:16<38:52,  1.20it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 506/2876 [15:25<36:59,  1.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 771/2876 [23:47<31:57,  1.10it/s]   "
     ]
    }
   ],
   "source": [
    "# Iterate through the dataset and query GPT to extract a natural-language answer\n",
    "splits = ['test', 'train']\n",
    "for s in splits:\n",
    "    all_timestamps = []\n",
    "    responses = []\n",
    "    # Obtain GPT answers/responses\n",
    "    for i in tqdm(range(len(all_ans_data[s]))): # Due to the RPM\n",
    "        all_timestamps = pause_if_needed(all_timestamps)\n",
    "\n",
    "        formatted_text = format_request(all_ans_data[s][i])\n",
    "        inputs = [\n",
    "                {\"role\": \"system\", \"content\": f\"You answer questions by only using a provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": formatted_text},\n",
    "            ]\n",
    "        response, all_timestamps = query_gpt(inputs, all_timestamps)\n",
    "        responses.append(response.choices[0].message.content)\n",
    "\n",
    "    # Export to txt file\n",
    "    file_name = f\"{s}_all.txt\"\n",
    "    # Open the file in write mode and save the list to the file\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        for item in responses:\n",
    "            file.write(item + \"\\n\")\n",
    "\n",
    "    # Export as HF dataset\n",
    "    df_pandas = all_ans_data[s].to_pandas()\n",
    "    df_pandas['gpt_ans'] = responses\n",
    "    new_dataset = Dataset.from_pandas(df_pandas)\n",
    "    new_dataset.save_to_disk(f\"{OUTPUT_DIR}/{short_model_name}_{s}_split\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now the new (GPT) answers can be merged with the original dataset\n",
    "# Load the original data and GPT data generated above\n",
    "train_dataset_pandas = training_data.training_data['train'].to_pandas()\n",
    "test_dataset_pandas = training_data.training_data['test'].to_pandas()\n",
    "# updated_train_dataset_pandas = load_from_disk(f\"assets/{short_model_name}_train_split\").to_pandas()\n",
    "# updated_test_dataset_pandas = load_from_disk(f\"assets/{short_model_name}_test_split\").to_pandas()\n",
    "updated_train_dataset_pandas = load_from_disk(f\"{OUTPUT_DIR}/{short_model_name}_train_split\").to_pandas()\n",
    "updated_test_dataset_pandas = load_from_disk(f\"{OUTPUT_DIR}/{short_model_name}_test_split\").to_pandas()\n",
    "\n",
    "# Merge on id and keep all rows\n",
    "train_merged = pd.merge(train_dataset_pandas, updated_train_dataset_pandas[['id', 'gpt_ans']], on='id', how='outer')\n",
    "test_merged = pd.merge(test_dataset_pandas, updated_test_dataset_pandas[['id', 'gpt_ans']], on='id', how='outer')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove any examples where it apparently had an answer but GPT couldn't extract oen\n",
    "train_merged = train_merged[train_merged['gpt_ans'] != NO_ANS_TOKEN]\n",
    "test_merged = test_merged[test_merged['gpt_ans'] !=  NO_ANS_TOKEN]\n",
    "\n",
    "# Update the answers to match the GPT ones\n",
    "train_merged.loc[train_merged['answer'] != NO_ANS_TOKEN, 'answer'] = train_merged.loc[train_merged['answer'] != NO_ANS_TOKEN, 'gpt_ans']\n",
    "test_merged.loc[test_merged['answer'] != NO_ANS_TOKEN, 'answer'] = test_merged.loc[test_merged['answer'] != NO_ANS_TOKEN, 'gpt_ans']\n",
    "\n",
    "merged_dataset = DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(train_merged),\n",
    "        \"test\": Dataset.from_pandas(test_merged),\n",
    "    })\n",
    "\n",
    "# Maintain the answerable/non-answerable balance\n",
    "def ensure_ans_non_ans_balance(dataset, dataset_splits=('training', 'validation'), proportion=0.3, seed=SEED):\n",
    "    # Extracting the unanswerable examples\n",
    "    no_ans = dataset.filter(lambda row: (row[\"answer\"] == NO_ANS_TOKEN))\n",
    "    good_ans = dataset.filter(lambda row: (row[\"answer\"] != NO_ANS_TOKEN))\n",
    "\n",
    "    # Discarding some unanswerable examples so the answer-no_ans ratio is favourable in each split\n",
    "    processed_datasets_dict = {}\n",
    "    for split in dataset_splits:\n",
    "      num_no_ans = proportion*len(good_ans[split])/(1-proportion)\n",
    "      no_ans_keep = no_ans[split].train_test_split(train_size=num_no_ans/len(no_ans[split]), seed=seed)['train']\n",
    "      processed_datasets_dict[split] = concatenate_datasets([no_ans_keep, good_ans[split]])\n",
    "\n",
    "    processed_dataset = DatasetDict({\n",
    "            dataset_splits[0]: processed_datasets_dict[dataset_splits[0]],\n",
    "            dataset_splits[1]: processed_datasets_dict[dataset_splits[1]],\n",
    "        })\n",
    "    shuffled_dataset = processed_dataset.shuffle(seed=seed)\n",
    "    return shuffled_dataset\n",
    "\n",
    "final_dataset = ensure_ans_non_ans_balance(merged_dataset, dataset_splits=['train', 'test'])\n",
    "\n",
    "# As can be seen, the 80/20 train-test split has been maintained\n",
    "len(final_dataset['train'])/(len(final_dataset['train'])+len(final_dataset['test']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finally, the questions/contexts and answers need to be tokenised to override previous values\n",
    "def tokenise(data):\n",
    "    # tokenize the inputs (questions and contexts)\n",
    "    additional_cols = tokeniser(data['content'], data['question'], truncation=False)\n",
    "\n",
    "    # tokenize the answers\n",
    "    targets = tokeniser(text_target=data['answer'], truncation=False)\n",
    "\n",
    "    #set labels\n",
    "    additional_cols['labels'] = targets['input_ids']\n",
    "    additional_cols['num_tokens'] = [len(row) for row in additional_cols[\"input_ids\"]]\n",
    "    return additional_cols\n",
    "\n",
    "final_dataset = final_dataset.map(tokenise, batched = True)\n",
    "final_dataset.save_to_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}_gpt_updated')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NQ Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "all_data = load_from_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}_gpt_updated') # add or remove the suffix a required\n",
    "\n",
    "# Connect to HuggingFace\n",
    "HfFolder.save_token(\"ADD_TOKEN_HERE\")\n",
    "!git config --global user.email \"pointon.joel@gmail.com\"\n",
    "!git config --global user.name \"Joel Pointon\"\n",
    "\n",
    "# Training arguments/config\n",
    "batch_size = 8 # 64\n",
    "num_train_epochs = 8\n",
    "logging_steps = len(all_data[\"training\"]) // batch_size # Show the training loss with every epoch\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_name}_updated\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    push_to_hub=True,\n",
    "    seed=9,\n",
    "    # optim=\"adafactor\"\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokeniser.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokeniser.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokeniser.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores\n",
    "    # result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokeniser, model=model)\n",
    "\n",
    "def model_init():\n",
    "  return AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=all_data[\"train\"],\n",
    "    eval_dataset=all_data[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokeniser,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "path = f\"/content/drive/MyDrive/Diss/Output/{short_model_name}-finetuned-natural-questions\"\n",
    "trainer.train()\n",
    "trainer.push_to_hub()\n",
    "trainer.save_model(path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# NEED TO FIX THIS\n",
    "filename = 'assets/knowledge/Fundamentals_of_Digital_Image_Processing_Textbook.pdf'\n",
    "doc = fitz.open(filename)\n",
    "page_limit = None\n",
    "all_text = ''\n",
    "# Iterate through the content\n",
    "for page in doc:\n",
    "    page_limit = doc.page_count if not page_limit else page_limit\n",
    "    if page.number <= page_limit:\n",
    "        block_content = page.get_text(\"blocks\") #.encode(\"utf8\") # \"blocks\"\n",
    "        for block in block_content:\n",
    "            if block[6] == 0:  # I.e. only extract text\n",
    "                plain_text = unidecode(block[4])  # .decode('latin1') #.decode('utf-8')\n",
    "                all_text += plain_text\n",
    "    else:\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MLM Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extracting training content\n",
    "for textbook in textbooks:\n",
    "    all_text = ''.join(CompVisionKnowledge.df.loc[CompVisionKnowledge.df['Source']==textbook, 'Content'].tolist())\n",
    "    with open(f'assets/knowledge/{textbook}.txt', \"w\") as f:\n",
    "        f.write(all_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read txt files\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "# For formatting a PDF as a HF datasets\n",
    "def get_text_dataset(path):\n",
    "  dataset_obj = TextDataset(\n",
    "        tokenizer = tokeniser,\n",
    "        file_path = path,\n",
    "        block_size = 512,\n",
    "    )\n",
    "  return dataset_obj\n",
    "\n",
    "# Read documents from the directory\n",
    "training_file = 'assets/knowledge/Digital_Image_Processing_Textbook.txt'\n",
    "validation_file = 'assets/knowledge/Fundamentals_of_Digital_Image_Processing_Textbook.txt'\n",
    "train_data = read_txt(training_file)\n",
    "validation_data = read_txt(validation_file)\n",
    "train_data = re.sub(r'\\n+', '\\n', train_data).strip()  # Remove excess newline characters\n",
    "validation_data = re.sub(r'\\n+', '\\n', validation_data).strip()  # Remove excess newline characters\n",
    "\n",
    "with open(f'{training_file}_formatted', \"w\") as f:\n",
    "    f.write(train_data)\n",
    "with open(f'{validation_file}_formatted', \"w\") as f:\n",
    "    f.write(validation_data)\n",
    "\n",
    "model_name = 'gpt2'\n",
    "output_dir = 'models/custom_q_and_a'\n",
    "batch_size = 8\n",
    "num_train_epochs = 8\n",
    "\n",
    "# Train\n",
    "tokeniser = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "if tokeniser.pad_token is None:\n",
    "    tokeniser.add_special_tokens({'pad_token': '[PAD]', 'mask_token': '[MASK]'})\n",
    "    model.resize_token_embeddings(len(tokeniser))\n",
    "\n",
    "\n",
    "train_dataset = get_text_dataset(f'{training_file}_formatted')\n",
    "validation_dataset = get_text_dataset(f'{validation_file}_formatted')\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokeniser,\n",
    "        mlm=True,\n",
    "    )\n",
    "\n",
    "HF_REFERENCE='mlm'\n",
    "logging_steps = len(train_dataset) // batch_size // 4\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=HF_REFERENCE,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=logging_steps,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    push_to_hub=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    ")\n",
    "trainer.train()\n",
    "import math\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "trainer.push_to_hub()\n",
    "tokeniser.push_to_hub(f'psxjp5/{HF_REFERENCE}')\n",
    "print('Finished training and pushed to hub!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Todo:\n",
    "# I need to make it more efficient on the number of tokens\n",
    "# Check num_tokens usage and specifying the encoding model\n",
    "# Fix the potential issue of GPT sections being longer than 1024 tokens when using BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MLM model\n",
    "formatted_text = 'What is PCA?'\n",
    "question_answering = pipeline(model=f'psxjp5/mlm')\n",
    "print(question_answering(formatted_text)[0]['generated_text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\n",
    "print(Query.ask('What is PCA?', CompVisionGPT, show_source=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CompVisionBERT = ChatBot(\"Computer Vision\", 'assets/' + BERT_KNOWLEDGE_FILENAME)\n",
    "print(Query.ask_bert('When did universities begin teaching Computer Vision?', CompVisionBERT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\n",
    "print(Query.ask_bart('When did Universities begin teaching Computer Vision?', CompVisionGPT, show_source=True))  # What if the GPT knowledge sections are longer than 1024 tokens?? Need to account for this!"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
