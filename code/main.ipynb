{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GPT-3.5-Turbo Model\n",
    "Creating a question answering chatbot using GPT-3.5. Adapted from: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_tokens() IS OFTEN USED IN KNOWLEDGE.PY (and other files??) WITHOUT SPECIFYING THE EMBEDDING MODEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"modules\")\n",
    "from modules.config import *\n",
    "from modules.knowledge import *\n",
    "from modules.chatbot import *\n",
    "from modules.embedding_functions import *\n",
    "from modules.query import *\n",
    "sys.path.remove(\"modules\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "filename = 'assets/knowledge/Fundamentals_of_Digital_Image_Processing_Textbook.pdf'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following Wikipedia page has been successfully added to the knowledge database: Computer vision\n"
     ]
    }
   ],
   "source": [
    "# get_embedding(\"Joel\", embedding_model=self.embedding_model)\n",
    "CompVisionKnowledge = Knowledge('CompVisionPDF', 'GPT')\n",
    "# for page in WIKI_PAGES:\n",
    "CompVisionKnowledge.append_wikipedia_page(WIKI_PAGE) # page"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                         Source         Heading                   Subheading  \\\n0   Wikipedia (Computer vision)                                                \n1   Wikipedia (Computer vision)      Definition                                \n2   Wikipedia (Computer vision)         History                                \n4   Wikipedia (Computer vision)  Related fields          Solid-state physics   \n5   Wikipedia (Computer vision)  Related fields                 Neurobiology   \n6   Wikipedia (Computer vision)  Related fields            Signal processing   \n7   Wikipedia (Computer vision)  Related fields           Robotic navigation   \n8   Wikipedia (Computer vision)  Related fields                 Other fields   \n9   Wikipedia (Computer vision)  Related fields                 Distinctions   \n10  Wikipedia (Computer vision)    Applications                                \n11  Wikipedia (Computer vision)    Applications                     Medicine   \n12  Wikipedia (Computer vision)    Applications               Machine vision   \n13  Wikipedia (Computer vision)    Applications                     Military   \n14  Wikipedia (Computer vision)    Applications          Autonomous vehicles   \n15  Wikipedia (Computer vision)    Applications             Tactile feedback   \n16  Wikipedia (Computer vision)   Typical tasks                                \n17  Wikipedia (Computer vision)   Typical tasks                  Recognition   \n18  Wikipedia (Computer vision)   Typical tasks              Motion analysis   \n19  Wikipedia (Computer vision)   Typical tasks         Scene reconstruction   \n20  Wikipedia (Computer vision)   Typical tasks            Image restoration   \n21  Wikipedia (Computer vision)  System methods                                \n22  Wikipedia (Computer vision)  System methods  Image-understanding systems   \n23  Wikipedia (Computer vision)        Hardware                                \n\n   Page                                            Content  \\\n0        \\nComputer vision tasks include methods for ac...   \n1        \\nComputer vision is an interdisciplinary fiel...   \n2        \\nIn the late 1960s, computer vision began at ...   \n4        \\nSolid-state physics is another field that is...   \n5        \\nNeurobiology has greatly influenced the deve...   \n6        \\nYet another field related to computer vision...   \n7        \\nRobot navigation sometimes deals with autono...   \n8        \\nBesides the above-mentioned views on compute...   \n9        \\nThe fields most closely related to computer ...   \n10       \\nApplications range from tasks such as indust...   \n11       \\nOne of the most prominent application fields...   \n12       \\nA second application area in computer vision...   \n13       \\nMilitary applications are probably one of th...   \n14       \\nOne of the newer application areas is autono...   \n15       \\nMaterials such as rubber and silicon are bei...   \n16       \\nEach of the application areas described abov...   \n17       \\nThe classical problem in computer vision, im...   \n18       \\nSeveral tasks relate to motion estimation wh...   \n19       \\nGiven one or (typically) more images of a sc...   \n20       \\nImage restoration comes into picture when th...   \n21       \\nThe organization of a computer vision system...   \n22       \\nImage-understanding systems (IUS) include th...   \n23       \\nThere are many kinds of computer vision syst...   \n\n                                              Section  Tokens  \\\n0                         Wikipedia (Computer vision)   301.0   \n1             Wikipedia (Computer vision)->Definition   158.0   \n2                Wikipedia (Computer vision)->History   507.0   \n4   Wikipedia (Computer vision)->Related fields->S...   120.0   \n5   Wikipedia (Computer vision)->Related fields->N...   293.0   \n6   Wikipedia (Computer vision)->Related fields->S...   103.0   \n7   Wikipedia (Computer vision)->Related fields->R...    64.0   \n8   Wikipedia (Computer vision)->Related fields->O...   119.0   \n9   Wikipedia (Computer vision)->Related fields->D...   639.0   \n10          Wikipedia (Computer vision)->Applications   272.0   \n11  Wikipedia (Computer vision)->Applications->Med...   135.0   \n12  Wikipedia (Computer vision)->Applications->Mac...   141.0   \n13  Wikipedia (Computer vision)->Applications->Mil...   129.0   \n14  Wikipedia (Computer vision)->Applications->Aut...   226.0   \n15  Wikipedia (Computer vision)->Applications->Tac...   270.0   \n16         Wikipedia (Computer vision)->Typical tasks   161.0   \n17  Wikipedia (Computer vision)->Typical tasks->Re...   735.0   \n18  Wikipedia (Computer vision)->Typical tasks->Mo...   193.0   \n19  Wikipedia (Computer vision)->Typical tasks->Sc...   125.0   \n20  Wikipedia (Computer vision)->Typical tasks->Im...   197.0   \n21        Wikipedia (Computer vision)->System methods   672.0   \n22  Wikipedia (Computer vision)->System methods->I...   192.0   \n23              Wikipedia (Computer vision)->Hardware   392.0   \n\n                                            Embedding  \n0   [-0.017756473273038864, 0.0029741437174379826,...  \n1   [-0.0211649127304554, 0.004849217366427183, 0....  \n2   [-0.011456056497991085, -0.004036989063024521,...  \n4   [0.001918427529744804, 0.011286016553640366, -...  \n5   [-0.009113207459449768, 0.0011424196418374777,...  \n6   [-0.027323242276906967, 0.007451199926435947, ...  \n7   [0.0034060052130371332, -0.014033270068466663,...  \n8   [0.0024171562399715185, -0.003781835548579693,...  \n9   [-0.017146369442343712, 0.005803829059004784, ...  \n10  [-0.022551782429218292, 0.005745057016611099, ...  \n11  [-0.016160931438207626, 0.022575857117772102, ...  \n12  [-0.016608819365501404, 0.002659277291968465, ...  \n13  [-0.026233529672026634, 0.001755360746756196, ...  \n14  [0.00027228015824221075, -0.001870156731456518...  \n15  [-0.015142167918384075, 0.023844724521040916, ...  \n16  [-0.018180591985583305, 0.007207258604466915, ...  \n17  [-0.018381645902991295, 0.019364971667528152, ...  \n18  [-0.02081947773694992, 0.0022223966661840677, ...  \n19  [-0.025001976639032364, 0.01593908853828907, 0...  \n20  [0.000656316289678216, 0.030923858284950256, 0...  \n21  [-0.0013866721419617534, 0.026621488854289055,...  \n22  [0.006777079775929451, -0.0048777274787425995,...  \n23  [-0.006047586910426617, 0.016233345493674278, ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Source</th>\n      <th>Heading</th>\n      <th>Subheading</th>\n      <th>Page</th>\n      <th>Content</th>\n      <th>Section</th>\n      <th>Tokens</th>\n      <th>Embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>\\nComputer vision tasks include methods for ac...</td>\n      <td>Wikipedia (Computer vision)</td>\n      <td>301.0</td>\n      <td>[-0.017756473273038864, 0.0029741437174379826,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Definition</td>\n      <td></td>\n      <td></td>\n      <td>\\nComputer vision is an interdisciplinary fiel...</td>\n      <td>Wikipedia (Computer vision)-&gt;Definition</td>\n      <td>158.0</td>\n      <td>[-0.0211649127304554, 0.004849217366427183, 0....</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>History</td>\n      <td></td>\n      <td></td>\n      <td>\\nIn the late 1960s, computer vision began at ...</td>\n      <td>Wikipedia (Computer vision)-&gt;History</td>\n      <td>507.0</td>\n      <td>[-0.011456056497991085, -0.004036989063024521,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Related fields</td>\n      <td>Solid-state physics</td>\n      <td></td>\n      <td>\\nSolid-state physics is another field that is...</td>\n      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;S...</td>\n      <td>120.0</td>\n      <td>[0.001918427529744804, 0.011286016553640366, -...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Related fields</td>\n      <td>Neurobiology</td>\n      <td></td>\n      <td>\\nNeurobiology has greatly influenced the deve...</td>\n      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;N...</td>\n      <td>293.0</td>\n      <td>[-0.009113207459449768, 0.0011424196418374777,...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Related fields</td>\n      <td>Signal processing</td>\n      <td></td>\n      <td>\\nYet another field related to computer vision...</td>\n      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;S...</td>\n      <td>103.0</td>\n      <td>[-0.027323242276906967, 0.007451199926435947, ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Related fields</td>\n      <td>Robotic navigation</td>\n      <td></td>\n      <td>\\nRobot navigation sometimes deals with autono...</td>\n      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;R...</td>\n      <td>64.0</td>\n      <td>[0.0034060052130371332, -0.014033270068466663,...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Related fields</td>\n      <td>Other fields</td>\n      <td></td>\n      <td>\\nBesides the above-mentioned views on compute...</td>\n      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;O...</td>\n      <td>119.0</td>\n      <td>[0.0024171562399715185, -0.003781835548579693,...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Related fields</td>\n      <td>Distinctions</td>\n      <td></td>\n      <td>\\nThe fields most closely related to computer ...</td>\n      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;D...</td>\n      <td>639.0</td>\n      <td>[-0.017146369442343712, 0.005803829059004784, ...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Applications</td>\n      <td></td>\n      <td></td>\n      <td>\\nApplications range from tasks such as indust...</td>\n      <td>Wikipedia (Computer vision)-&gt;Applications</td>\n      <td>272.0</td>\n      <td>[-0.022551782429218292, 0.005745057016611099, ...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Applications</td>\n      <td>Medicine</td>\n      <td></td>\n      <td>\\nOne of the most prominent application fields...</td>\n      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Med...</td>\n      <td>135.0</td>\n      <td>[-0.016160931438207626, 0.022575857117772102, ...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Applications</td>\n      <td>Machine vision</td>\n      <td></td>\n      <td>\\nA second application area in computer vision...</td>\n      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Mac...</td>\n      <td>141.0</td>\n      <td>[-0.016608819365501404, 0.002659277291968465, ...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Applications</td>\n      <td>Military</td>\n      <td></td>\n      <td>\\nMilitary applications are probably one of th...</td>\n      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Mil...</td>\n      <td>129.0</td>\n      <td>[-0.026233529672026634, 0.001755360746756196, ...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Applications</td>\n      <td>Autonomous vehicles</td>\n      <td></td>\n      <td>\\nOne of the newer application areas is autono...</td>\n      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Aut...</td>\n      <td>226.0</td>\n      <td>[0.00027228015824221075, -0.001870156731456518...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Applications</td>\n      <td>Tactile feedback</td>\n      <td></td>\n      <td>\\nMaterials such as rubber and silicon are bei...</td>\n      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Tac...</td>\n      <td>270.0</td>\n      <td>[-0.015142167918384075, 0.023844724521040916, ...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Typical tasks</td>\n      <td></td>\n      <td></td>\n      <td>\\nEach of the application areas described abov...</td>\n      <td>Wikipedia (Computer vision)-&gt;Typical tasks</td>\n      <td>161.0</td>\n      <td>[-0.018180591985583305, 0.007207258604466915, ...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Typical tasks</td>\n      <td>Recognition</td>\n      <td></td>\n      <td>\\nThe classical problem in computer vision, im...</td>\n      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Re...</td>\n      <td>735.0</td>\n      <td>[-0.018381645902991295, 0.019364971667528152, ...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Typical tasks</td>\n      <td>Motion analysis</td>\n      <td></td>\n      <td>\\nSeveral tasks relate to motion estimation wh...</td>\n      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Mo...</td>\n      <td>193.0</td>\n      <td>[-0.02081947773694992, 0.0022223966661840677, ...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Typical tasks</td>\n      <td>Scene reconstruction</td>\n      <td></td>\n      <td>\\nGiven one or (typically) more images of a sc...</td>\n      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Sc...</td>\n      <td>125.0</td>\n      <td>[-0.025001976639032364, 0.01593908853828907, 0...</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Typical tasks</td>\n      <td>Image restoration</td>\n      <td></td>\n      <td>\\nImage restoration comes into picture when th...</td>\n      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Im...</td>\n      <td>197.0</td>\n      <td>[0.000656316289678216, 0.030923858284950256, 0...</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>System methods</td>\n      <td></td>\n      <td></td>\n      <td>\\nThe organization of a computer vision system...</td>\n      <td>Wikipedia (Computer vision)-&gt;System methods</td>\n      <td>672.0</td>\n      <td>[-0.0013866721419617534, 0.026621488854289055,...</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>System methods</td>\n      <td>Image-understanding systems</td>\n      <td></td>\n      <td>\\nImage-understanding systems (IUS) include th...</td>\n      <td>Wikipedia (Computer vision)-&gt;System methods-&gt;I...</td>\n      <td>192.0</td>\n      <td>[0.006777079775929451, -0.0048777274787425995,...</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Hardware</td>\n      <td></td>\n      <td></td>\n      <td>\\nThere are many kinds of computer vision syst...</td>\n      <td>Wikipedia (Computer vision)-&gt;Hardware</td>\n      <td>392.0</td>\n      <td>[-0.006047586910426617, 0.016233345493674278, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CompVisionKnowledge.df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following PDF has been successfully added to the knowledge database: CompVisionPDF (assets/knowledge/Fundamentals_of_Digital_Image_Processing_Textbook.pdf)\n"
     ]
    }
   ],
   "source": [
    "# CompVisionKnowledge = Knowledge('CompVisionPDF', 'GPT')\n",
    "CompVisionKnowledge.append_pdf(filename, 'CompVisionPDF')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "                          Source         Heading           Subheading  \\\n0    Wikipedia (Computer vision)                                        \n1    Wikipedia (Computer vision)      Definition                        \n2    Wikipedia (Computer vision)         History                        \n4    Wikipedia (Computer vision)  Related fields  Solid-state physics   \n5    Wikipedia (Computer vision)  Related fields         Neurobiology   \n..                           ...             ...                  ...   \n110                CompVisionPDF                                        \n111                CompVisionPDF                                        \n112                CompVisionPDF                                        \n113                CompVisionPDF                                        \n114                CompVisionPDF                                        \n\n                                        Page  \\\n0                                              \n1                                              \n2                                              \n4                                              \n5                                              \n..                                       ...   \n110                                  339/340   \n111                              340/341/342   \n112                                  343/344   \n113  345/346/347/348/349/350/351/352/353/354   \n114                                      354   \n\n                                               Content  \\\n0    \\nComputer vision tasks include methods for ac...   \n1    \\nComputer vision is an interdisciplinary fiel...   \n2    \\nIn the late 1960s, computer vision began at ...   \n4    \\nSolid-state physics is another field that is...   \n5    \\nNeurobiology has greatly influenced the deve...   \n..                                                 ...   \n110  \\nEdge detection\\nchallenges, 270\\nfiltering f...   \n111  \\nImage\\ncolour, 2 3\\ncolour spaces, 9 14\\nper...   \n112  \\nMatlab, 1, 3, 14 17, 19, 35, 52, 56, 59, 61,...   \n113  \\nResolution, 3 5\\nbit resolution, 4\\nspatial ...   \n114  \\nmatrix and prior distributions. The class-co...   \n\n                                               Section  Tokens  \\\n0                          Wikipedia (Computer vision)   301.0   \n1              Wikipedia (Computer vision)->Definition   158.0   \n2                 Wikipedia (Computer vision)->History   507.0   \n4    Wikipedia (Computer vision)->Related fields->S...   120.0   \n5    Wikipedia (Computer vision)->Related fields->N...   293.0   \n..                                                 ...     ...   \n110                          CompVisionPDF-> p.339/340  1581.0   \n111                      CompVisionPDF-> p.340/341/342  1556.0   \n112                          CompVisionPDF-> p.343/344  1557.0   \n113  CompVisionPDF-> p.345/346/347/348/349/350/351/...  1418.0   \n114                              CompVisionPDF-> p.354   196.0   \n\n                                             Embedding  \n0    [-0.017756473273038864, 0.0029741437174379826,...  \n1    [-0.0211649127304554, 0.004849217366427183, 0....  \n2    [-0.011456056497991085, -0.004036989063024521,...  \n4    [0.001918427529744804, 0.011286016553640366, -...  \n5    [-0.009113207459449768, 0.0011424196418374777,...  \n..                                                 ...  \n110  [-0.002906471025198698, 0.026034852489829063, ...  \n111  [0.0033323888201266527, 0.030085023492574692, ...  \n112  [-0.0015604019863530993, 0.013584878295660019,...  \n113  [0.02063027210533619, 0.016986483708024025, 0....  \n114  [0.01059232372790575, 0.000738857954274863, -0...  \n\n[138 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Source</th>\n      <th>Heading</th>\n      <th>Subheading</th>\n      <th>Page</th>\n      <th>Content</th>\n      <th>Section</th>\n      <th>Tokens</th>\n      <th>Embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>\\nComputer vision tasks include methods for ac...</td>\n      <td>Wikipedia (Computer vision)</td>\n      <td>301.0</td>\n      <td>[-0.017756473273038864, 0.0029741437174379826,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Definition</td>\n      <td></td>\n      <td></td>\n      <td>\\nComputer vision is an interdisciplinary fiel...</td>\n      <td>Wikipedia (Computer vision)-&gt;Definition</td>\n      <td>158.0</td>\n      <td>[-0.0211649127304554, 0.004849217366427183, 0....</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>History</td>\n      <td></td>\n      <td></td>\n      <td>\\nIn the late 1960s, computer vision began at ...</td>\n      <td>Wikipedia (Computer vision)-&gt;History</td>\n      <td>507.0</td>\n      <td>[-0.011456056497991085, -0.004036989063024521,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Related fields</td>\n      <td>Solid-state physics</td>\n      <td></td>\n      <td>\\nSolid-state physics is another field that is...</td>\n      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;S...</td>\n      <td>120.0</td>\n      <td>[0.001918427529744804, 0.011286016553640366, -...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Wikipedia (Computer vision)</td>\n      <td>Related fields</td>\n      <td>Neurobiology</td>\n      <td></td>\n      <td>\\nNeurobiology has greatly influenced the deve...</td>\n      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;N...</td>\n      <td>293.0</td>\n      <td>[-0.009113207459449768, 0.0011424196418374777,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>110</th>\n      <td>CompVisionPDF</td>\n      <td></td>\n      <td></td>\n      <td>339/340</td>\n      <td>\\nEdge detection\\nchallenges, 270\\nfiltering f...</td>\n      <td>CompVisionPDF-&gt; p.339/340</td>\n      <td>1581.0</td>\n      <td>[-0.002906471025198698, 0.026034852489829063, ...</td>\n    </tr>\n    <tr>\n      <th>111</th>\n      <td>CompVisionPDF</td>\n      <td></td>\n      <td></td>\n      <td>340/341/342</td>\n      <td>\\nImage\\ncolour, 2 3\\ncolour spaces, 9 14\\nper...</td>\n      <td>CompVisionPDF-&gt; p.340/341/342</td>\n      <td>1556.0</td>\n      <td>[0.0033323888201266527, 0.030085023492574692, ...</td>\n    </tr>\n    <tr>\n      <th>112</th>\n      <td>CompVisionPDF</td>\n      <td></td>\n      <td></td>\n      <td>343/344</td>\n      <td>\\nMatlab, 1, 3, 14 17, 19, 35, 52, 56, 59, 61,...</td>\n      <td>CompVisionPDF-&gt; p.343/344</td>\n      <td>1557.0</td>\n      <td>[-0.0015604019863530993, 0.013584878295660019,...</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>CompVisionPDF</td>\n      <td></td>\n      <td></td>\n      <td>345/346/347/348/349/350/351/352/353/354</td>\n      <td>\\nResolution, 3 5\\nbit resolution, 4\\nspatial ...</td>\n      <td>CompVisionPDF-&gt; p.345/346/347/348/349/350/351/...</td>\n      <td>1418.0</td>\n      <td>[0.02063027210533619, 0.016986483708024025, 0....</td>\n    </tr>\n    <tr>\n      <th>114</th>\n      <td>CompVisionPDF</td>\n      <td></td>\n      <td></td>\n      <td>354</td>\n      <td>\\nmatrix and prior distributions. The class-co...</td>\n      <td>CompVisionPDF-&gt; p.354</td>\n      <td>196.0</td>\n      <td>[0.01059232372790575, 0.000738857954274863, -0...</td>\n    </tr>\n  </tbody>\n</table>\n<p>138 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CompVisionKnowledge.export_to_csv(GPT_KNOWLEDGE_FILENAME)\n",
    "CompVisionKnowledge.df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "               Source Heading Subheading  \\\n0   Comp Vision Stuff     {0}        NaN   \n0   Comp Vision Stuff     {0}        NaN   \n0   Comp Vision Stuff     {0}        NaN   \n0   Comp Vision Stuff     {0}        NaN   \n0   Comp Vision Stuff     {0}        NaN   \n..                ...     ...        ...   \n0   Comp Vision Stuff  {1021}        NaN   \n0   Comp Vision Stuff  {1021}        NaN   \n0   Comp Vision Stuff  {1021}        NaN   \n0   Comp Vision Stuff  {1021}        NaN   \n0   Comp Vision Stuff  {1021}        NaN   \n\n                                              Content  \n0                                \\nGLOBAL \\nEDITION\\n  \n0                        \\nDigital Image Processing\\n  \n0                                 \\n FOURTH EDITION\\n  \n0          \\n Rafael C. Gonzalez * Richard E. Woods\\n  \n0                              \\nwww.EBooksWorld.ir\\n  \n..                                                ...  \n0                   \\n18    Chapter 1  Introduction\\n  \n0         \\n1.1 WHAT IS DIGITAL IMAGE PROCESSING?  \\n  \n0   \\nAn image may be defined as a two-dimensional...  \n0                                             \\n1.1\\n  \n0                              \\nwww.EBooksWorld.ir\\n  \n\n[5168 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Source</th>\n      <th>Heading</th>\n      <th>Subheading</th>\n      <th>Content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{0}</td>\n      <td>NaN</td>\n      <td>\\nGLOBAL \\nEDITION\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{0}</td>\n      <td>NaN</td>\n      <td>\\nDigital Image Processing\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{0}</td>\n      <td>NaN</td>\n      <td>\\n FOURTH EDITION\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{0}</td>\n      <td>NaN</td>\n      <td>\\n Rafael C. Gonzalez * Richard E. Woods\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{0}</td>\n      <td>NaN</td>\n      <td>\\nwww.EBooksWorld.ir\\n</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{1021}</td>\n      <td>NaN</td>\n      <td>\\n18    Chapter 1  Introduction\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{1021}</td>\n      <td>NaN</td>\n      <td>\\n1.1 WHAT IS DIGITAL IMAGE PROCESSING?  \\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{1021}</td>\n      <td>NaN</td>\n      <td>\\nAn image may be defined as a two-dimensional...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{1021}</td>\n      <td>NaN</td>\n      <td>\\n1.1\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{1021}</td>\n      <td>NaN</td>\n      <td>\\nwww.EBooksWorld.ir\\n</td>\n    </tr>\n  </tbody>\n</table>\n<p>5168 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lec1QAPairs = [\n",
    "    [\"Q: What is this module's name?\", \"A: Introduction to Image Processing\"],\n",
    "    [\"Q: What is this module's code?\", \"A: COMP 2032\"],\n",
    "    [\"Q: When and where are the lectures held?\", \"A: F3A04 (Block F3, Level A, Room 04) on Wednesdays (09:00 to 11:00)\"],\n",
    "    [\"Q: When and where are the labs held?\", \"A: BB80 (Block B, Level B, Room 80) on Tuesdays (14:00 to 16:00)\"],\n",
    "    [\"Q: What sources can I use for wider reading?\", \"A: Digital Image Processing by R.C. Gonzalez and R.E. Woods. (2018). . (Fourth Edition), Fundamentals of Digital Image Processing: A Practical Approach with Examples in Matlab by Chris Solomon and Toby Breckon (2010), Hypermedia Image Processing Reference (HIPR) at the University of Edinburgh\"],\n",
    "    [\"Q: Who is the lecturer for this module?\", \"A: Dr Tissa Chandesa (Tissa.Chandesa@nottingham.edu.my)\"],\n",
    "    [\"Q: Who are the teaching assistants for this module?\". \"A: Mr Mahmood Haithami (hcxmh1@nottingham.edu.my) and Mr Muhammad Waqas (hcxmw1@nottingham.edu.my)\"],\n",
    "    [\"Q: What are the assessments for this module?\", \"A: Coursework (40% weighting) which comprises of Programming and 2000 Word Report and a 1-hour Written Examination (60%), where you must answer all 3 questions\"],\n",
    "    [\"Q: What is image analysis?\", \"A: Image analysis is concerned with making quantitative measurements on images, using image measurements as a proxy for real-world values\"],\n",
    "    [\"Q: What are the limitations of the techniques covered in this module?\", \"A: Not all solutions can be reached using the covered techniques.\"],\n",
    "    [\"Q: What can image analysis be used for?\", \"A: medicine, science, manufacturing, food, textiles\"],\n",
    "    [\"Q: Is image processing the same as computer graphics?\", \"A: No\"],\n",
    "    [\"Q: What are some actions that can be performed on digital images using image processing?\", \"A: Acquire an image, store, manipulate, model, analyze, interpret, and display an image.\"],\n",
    "    [\"Q: What file types will we use for image processing?\", \"A: jpeg or png\"],\n",
    "    [\"Q: Can image processing be applied to videos?\", \"A: Yes\"],\n",
    "    [\"Q: What's the primary aim of this module?\", \"A: Introduce students to the fundamentals of digital image processing theory and practice by gaining practical experience in writing programs to manipulate digital images. It lays down the foundation for studying advanced topic in related fields\"],\n",
    "    ['Q: What are pictures represented by?', \"A: Pixels\"],\n",
    "    ['Q: What colours are used in the greyscale plane?', \"A: It is a range from white to black\"],\n",
    "    ['Q: What three colours are used to construct an image?', \"A: Red, green and blue\"],\n",
    "    [\"Q: What are the three components of image processing?\", \"A: Image formation, acquisition, colour representation\"],\n",
    "    ['Q: What is image compression used for?', \"A: To efficiently represent image data for storage and communication, to minimise disk space and network bandwidth\"],\n",
    "    [\"Q: What is the effect of image compression\", \"A: It reduces the image size but makes the quality poorer and more blurry\"],\n",
    "    [\"Q: What is image manipulation used for?\", \"A: Image manipulation is used to remove noise, sharpen, sharpen and enhance or change the contrast and general appearance of an image.\"],\n",
    "    [\"Q: What are some examples of image compression formats?\", \"A: Examples include GIF, JPEG, and PNG.\"],\n",
    "    [\"Q: What are superpixels?\", \"A: A grouping of similar pixels used as an intermediate image representation to reduce the number of pixels\"],\n",
    "    [\"Q: What technique can be used to find geometric objects?\" \"A: The Hough Transform\"],\n",
    "    [\"Q: What are spatial domain methods?\", \"A: Methods which operate directly on the image, such as point operations and area operations.\"],\n",
    "    [\"Q: What is the purpose of image segmentation?\", \"A: Image segmentation is used to extract specific objects or regions from an image.\"],\n",
    "    [\"Q: What is the frequency domain method in image processing?\", \"A: Frequency domain methods involve computing and modifying the power spectrum of an image.\"],\n",
    "    [\"Q: What is the importance of geometric operations in image processing?\", \"A: Geometric operations allow for changes in the image's array structure, and include the manipulation of the orientation, rotation, and scaling.\"],\n",
    "    [\"Q: What is content-based image retrieval?\", \"A: Content-based image retrieval is a technique for searching large image databases based on their visual content.\"],\n",
    "    [\"Q: What is painterly rendering in image processing?\", \"A: Painterly rendering involves processing an image to give it the appearance of a painting, based on the work of a particular artist or movement (e.g. impressionism).\"],\n",
    "    [\"Q: What are interactive tools and compositing in image processing?\", \"A: Interactive tools and compositing involve overlaying and combining multiple images into a single output image.\"],\n",
    "    [\"Q: What are some programming languages commonly used for image processing?\", \"A: Commonly used languages include MATLAB, Python (with libraries like PIL and OpenCV), and Java.\"],\n",
    "]\n",
    "\n",
    "Lec2QAPairs = [\n",
    "    [\"Q: What are the two important processes in digital image formation?\", \"A: Sampling and quantization.\"],\n",
    "    [\"Q: What is sampling in digital image formation?\", \"A: Sampling is the process of digitizing the spatial coordinates of an image.\"],\n",
    "    [\"Q: What is quantization in digital image formation?\", \"A: Quantization is the process of digitizing the light intensity function of an image.\"],\n",
    "    [\"Q: What is aliasing and what causes it?\", \"A: Aliasing is an artifact that occurs when the sampling rate is insufficient, causing the image to become unrecognizable. It is caused by undersampling or sampling at a rate below the Nyquist rate.\"],\n",
    "    [\"Q: How many pixels are in an image?\", \"A: The number of pixels in an image depends on its resolution and size. It can vary from image to image.\"],\n",
    "    [\"Q: How many samples should you take from an image?\", \"A: Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed\"],\n",
    "    [\"Q: What is the Nyquist Rate?\", \"A: The minimum sampling rate required to accurately reconstruct an image from its sampled version\"],\n",
    "    [\"A: What is under-sampling?\", \"A: Sampling at a rate which is too course, i.e. below the Nyquist Rate.\"],\n",
    "    [\"Q: What causes Aliasing?\", \"A: Under-sampling.\"],\n",
    "    ['Q: What is unsampling', \"A: The process of reconstructing an image by interpolating pixel values from the sample values\"],\n",
    "    [\"Q: What is super resolution?\", \"A: Super resolution methods involve combining multiple exposures of the same scene to enhance the resolution and quality of an image.\"],\n",
    "    ['Q: What is a Bayer pattern?', \"A: An array of red, green, and blue color filters arranged in a specific repeating pattern.\"],\n",
    "    ['Q: Out of red, green and blue, which colour are our eyes drawn to most?', \"A: Green.\"],\n",
    "    [\"Q: What is color intensity in an image?\", \"A: Color intensity refers to the level of intensity or brightness of a color in an image. It determines the perceived lightness or darkness of a color.\"],\n",
    "    [\"Q: How can aliasing be avoided in image sampling?\", \"A: Aliasing can be avoided in image sampling by ensuring that the sampling rate is at least twice the frequency of the highest frequency component to be reconstructed.\"],\n",
    "    [\"Q: How does interpolation help in image sampling?\", \"A: Interpolation helps in image sampling by estimating unknown pixel values based on known neighboring pixel values, filling in the gaps and producing a smoother representation of the image.\"],\n",
    "    [\"Q: Can gray level resolution be increased in a single image?\", \"A: No\"],\n",
    "    ['Q: Why is greyscale used in image processing', \"A: It makes processing easier, reduces the amount of information in the image, and makes some of the theory simpler.\"],\n",
    "    ['Q: How can you convert between colour and greyscale images?', \"A: By using a weighted average of the red, green, and blue components, with a higher weighting on green (30% red, 59% green, and 11% blue).\"],\n",
    "    [\"Q: What is HSV color space?\", \"A: HSV color space stands for hue, saturation, and value. It is a color model that represents colors based on their hue (the dominant wavelength), saturation (purity of color), and value (brightness). It is based on colour rather than light.\"],\n",
    "    [\"Q: Is HSV more or less sensitive to illumination changes relative to RGB?\", \"A: Less sensitive\"],\n",
    "    ['Q: What two components of HSV is human skin most captured by, out of hue, saturation, and value? ', \"A: Human skin is most captured by the hue and saturation components of HSV.\"],\n",
    "\n",
    "    [\"Q: What are the main components of image processing?\", \"A: The main components of image processing include image enhancement, image restoration, image compression, image analysis, and image recognition.\"],\n",
    "    [\"Q: What is the purpose of image enhancement?\", \"A: The purpose of image enhancement is to improve the visual quality of an image by adjusting its contrast, brightness, sharpness, or removing noise or artifacts.\"],\n",
    "    [\"Q: What is image restoration?\", \"A: Image restoration involves the process of recovering an image\"]\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_file_folder = 'assets/'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mSSLCertVerificationError\u001B[0m                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:1346\u001B[0m, in \u001B[0;36mAbstractHTTPHandler.do_open\u001B[1;34m(self, http_class, req, **http_conn_args)\u001B[0m\n\u001B[0;32m   1345\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1346\u001B[0m     \u001B[43mh\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1347\u001B[0m \u001B[43m              \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhas_header\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTransfer-encoding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1348\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:1285\u001B[0m, in \u001B[0;36mHTTPConnection.request\u001B[1;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[0;32m   1284\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1285\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:1331\u001B[0m, in \u001B[0;36mHTTPConnection._send_request\u001B[1;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[0;32m   1330\u001B[0m     body \u001B[38;5;241m=\u001B[39m _encode(body, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m-> 1331\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mendheaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:1280\u001B[0m, in \u001B[0;36mHTTPConnection.endheaders\u001B[1;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[0;32m   1279\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CannotSendHeader()\n\u001B[1;32m-> 1280\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:1040\u001B[0m, in \u001B[0;36mHTTPConnection._send_output\u001B[1;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[0;32m   1039\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer[:]\n\u001B[1;32m-> 1040\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1042\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m message_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1043\u001B[0m \n\u001B[0;32m   1044\u001B[0m     \u001B[38;5;66;03m# create a consistent interface to message_body\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:980\u001B[0m, in \u001B[0;36mHTTPConnection.send\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m    979\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_open:\n\u001B[1;32m--> 980\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    981\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:1454\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1452\u001B[0m     server_hostname \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost\n\u001B[1;32m-> 1454\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrap_socket\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1455\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mserver_hostname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\ssl.py:501\u001B[0m, in \u001B[0;36mSSLContext.wrap_socket\u001B[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001B[0m\n\u001B[0;32m    495\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrap_socket\u001B[39m(\u001B[38;5;28mself\u001B[39m, sock, server_side\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    496\u001B[0m                 do_handshake_on_connect\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    497\u001B[0m                 suppress_ragged_eofs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    498\u001B[0m                 server_hostname\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    499\u001B[0m     \u001B[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001B[39;00m\n\u001B[0;32m    500\u001B[0m     \u001B[38;5;66;03m# ctx._wrap_socket()\u001B[39;00m\n\u001B[1;32m--> 501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msslsocket_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43msock\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mserver_side\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mserver_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdo_handshake_on_connect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_handshake_on_connect\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    505\u001B[0m \u001B[43m        \u001B[49m\u001B[43msuppress_ragged_eofs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msuppress_ragged_eofs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    506\u001B[0m \u001B[43m        \u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mserver_hostname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    507\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    508\u001B[0m \u001B[43m        \u001B[49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msession\u001B[49m\n\u001B[0;32m    509\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\ssl.py:1041\u001B[0m, in \u001B[0;36mSSLSocket._create\u001B[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001B[0m\n\u001B[0;32m   1040\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1041\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_handshake\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1042\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mOSError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\ssl.py:1310\u001B[0m, in \u001B[0;36mSSLSocket.do_handshake\u001B[1;34m(self, block)\u001B[0m\n\u001B[0;32m   1309\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msettimeout(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m-> 1310\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_handshake\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1311\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[1;31mSSLCertVerificationError\u001B[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mURLError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m target_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://homepages.inf.ed.ac.uk/rbf/HIPR2/median.htm\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01murllib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrequest\u001B[39;00m  \u001B[38;5;66;03m# the lib that handles the url stuff\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m \u001B[43murllib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget_url\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28mprint\u001B[39m(line\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:214\u001B[0m, in \u001B[0;36murlopen\u001B[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[0m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    213\u001B[0m     opener \u001B[38;5;241m=\u001B[39m _opener\n\u001B[1;32m--> 214\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:517\u001B[0m, in \u001B[0;36mOpenerDirector.open\u001B[1;34m(self, fullurl, data, timeout)\u001B[0m\n\u001B[0;32m    514\u001B[0m     req \u001B[38;5;241m=\u001B[39m meth(req)\n\u001B[0;32m    516\u001B[0m sys\u001B[38;5;241m.\u001B[39maudit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124murllib.Request\u001B[39m\u001B[38;5;124m'\u001B[39m, req\u001B[38;5;241m.\u001B[39mfull_url, req\u001B[38;5;241m.\u001B[39mdata, req\u001B[38;5;241m.\u001B[39mheaders, req\u001B[38;5;241m.\u001B[39mget_method())\n\u001B[1;32m--> 517\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    519\u001B[0m \u001B[38;5;66;03m# post-process response\u001B[39;00m\n\u001B[0;32m    520\u001B[0m meth_name \u001B[38;5;241m=\u001B[39m protocol\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_response\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:534\u001B[0m, in \u001B[0;36mOpenerDirector._open\u001B[1;34m(self, req, data)\u001B[0m\n\u001B[0;32m    531\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[0;32m    533\u001B[0m protocol \u001B[38;5;241m=\u001B[39m req\u001B[38;5;241m.\u001B[39mtype\n\u001B[1;32m--> 534\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_open\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\n\u001B[0;32m    535\u001B[0m \u001B[43m                          \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_open\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result:\n\u001B[0;32m    537\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:494\u001B[0m, in \u001B[0;36mOpenerDirector._call_chain\u001B[1;34m(self, chain, kind, meth_name, *args)\u001B[0m\n\u001B[0;32m    492\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m handler \u001B[38;5;129;01min\u001B[39;00m handlers:\n\u001B[0;32m    493\u001B[0m     func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[1;32m--> 494\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    495\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    496\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:1389\u001B[0m, in \u001B[0;36mHTTPSHandler.https_open\u001B[1;34m(self, req)\u001B[0m\n\u001B[0;32m   1388\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhttps_open\u001B[39m(\u001B[38;5;28mself\u001B[39m, req):\n\u001B[1;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhttp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mHTTPSConnection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1390\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_hostname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_hostname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:1349\u001B[0m, in \u001B[0;36mAbstractHTTPHandler.do_open\u001B[1;34m(self, http_class, req, **http_conn_args)\u001B[0m\n\u001B[0;32m   1346\u001B[0m         h\u001B[38;5;241m.\u001B[39mrequest(req\u001B[38;5;241m.\u001B[39mget_method(), req\u001B[38;5;241m.\u001B[39mselector, req\u001B[38;5;241m.\u001B[39mdata, headers,\n\u001B[0;32m   1347\u001B[0m                   encode_chunked\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39mhas_header(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTransfer-encoding\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m   1348\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n\u001B[1;32m-> 1349\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m URLError(err)\n\u001B[0;32m   1350\u001B[0m     r \u001B[38;5;241m=\u001B[39m h\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[0;32m   1351\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "\u001B[1;31mURLError\u001B[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>"
     ]
    }
   ],
   "source": [
    "target_url = 'https://homepages.inf.ed.ac.uk/rbf/HIPR2/median.htm'\n",
    "import urllib.request  # the lib that handles the url stuff\n",
    "\n",
    "for line in urllib.request.urlopen(target_url):\n",
    "    print(line.decode('utf-8')) #utf-8 or iso8859-1 or whatever the page encoding scheme is"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Subheading</th>\n",
       "      <th>Content</th>\n",
       "      <th>Section</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\nComputer vision tasks include methods for ac...</td>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>290</td>\n",
       "      <td>[-0.5566069483757019, 0.6151323318481445, 0.70...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Definition</td>\n",
       "      <td></td>\n",
       "      <td>\\nComputer vision is an interdisciplinary fiel...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Definition</td>\n",
       "      <td>162</td>\n",
       "      <td>[-0.18940897285938263, 0.5564344525337219, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>History</td>\n",
       "      <td></td>\n",
       "      <td>\\nIn the late 1960s, computer vision began at ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;History</td>\n",
       "      <td>243</td>\n",
       "      <td>[-0.5624328255653381, 0.35494446754455566, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>History</td>\n",
       "      <td></td>\n",
       "      <td>\\nBy the 1990s, some of the previous research ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;History</td>\n",
       "      <td>264</td>\n",
       "      <td>[-0.8420819044113159, 0.011862404644489288, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Solid-state physics</td>\n",
       "      <td>\\nSolid-state physics is another field that is...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;S...</td>\n",
       "      <td>123</td>\n",
       "      <td>[-0.1766018569469452, 0.5509802103042603, 0.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Neurobiology</td>\n",
       "      <td>\\nNeurobiology has greatly influenced the deve...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;N...</td>\n",
       "      <td>296</td>\n",
       "      <td>[0.07454751431941986, 0.42800015211105347, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Signal processing</td>\n",
       "      <td>\\nYet another field related to computer vision...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;S...</td>\n",
       "      <td>106</td>\n",
       "      <td>[-0.1562240570783615, 0.18830031156539917, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Robotic navigation</td>\n",
       "      <td>\\nRobot navigation sometimes deals with autono...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;R...</td>\n",
       "      <td>65</td>\n",
       "      <td>[0.09209920465946198, 0.5207788944244385, 1.26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Other fields</td>\n",
       "      <td>\\nBesides the above-mentioned views on compute...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;O...</td>\n",
       "      <td>122</td>\n",
       "      <td>[-0.34635502099990845, 0.12120083719491959, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Distinctions</td>\n",
       "      <td>\\nThe fields most closely related to computer ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;D...</td>\n",
       "      <td>244</td>\n",
       "      <td>[-0.011814514175057411, 0.9892112612724304, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Distinctions</td>\n",
       "      <td>\\nImage processing and image analysis tend to ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;D...</td>\n",
       "      <td>395</td>\n",
       "      <td>[-0.25189733505249023, 0.7090330123901367, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td></td>\n",
       "      <td>\\nApplications range from tasks such as indust...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications</td>\n",
       "      <td>294</td>\n",
       "      <td>[0.054026536643505096, 0.536332368850708, 0.80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>\\nOne of the most prominent application fields...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Med...</td>\n",
       "      <td>145</td>\n",
       "      <td>[0.07362960278987885, 0.8047475218772888, 1.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Machine vision</td>\n",
       "      <td>\\nA second application area in computer vision...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Mac...</td>\n",
       "      <td>148</td>\n",
       "      <td>[0.23051360249519348, 0.6428527235984802, 0.69...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Military</td>\n",
       "      <td>\\nMilitary applications are probably one of th...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Mil...</td>\n",
       "      <td>129</td>\n",
       "      <td>[-0.45685380697250366, 0.40300095081329346, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Autonomous vehicles</td>\n",
       "      <td>\\nOne of the newer application areas is autono...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Aut...</td>\n",
       "      <td>258</td>\n",
       "      <td>[-0.5068467855453491, 0.4222138524055481, 1.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Tactile feedback</td>\n",
       "      <td>\\nMaterials such as rubber and silicon are bei...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Tac...</td>\n",
       "      <td>286</td>\n",
       "      <td>[-0.006287522614002228, 0.3353821039199829, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td></td>\n",
       "      <td>\\nEach of the application areas described abov...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks</td>\n",
       "      <td>166</td>\n",
       "      <td>[-0.3646470308303833, 0.5144392848014832, 1.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Recognition</td>\n",
       "      <td>\\nThe classical problem in computer vision, im...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Re...</td>\n",
       "      <td>427</td>\n",
       "      <td>[-0.11414705961942673, 0.8190616965293884, 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Recognition</td>\n",
       "      <td>\\nContent-based image retrieval – finding all ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Re...</td>\n",
       "      <td>264</td>\n",
       "      <td>[-0.3213890492916107, 1.116416335105896, 0.800...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Motion analysis</td>\n",
       "      <td>\\nSeveral tasks relate to motion estimation wh...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Mo...</td>\n",
       "      <td>194</td>\n",
       "      <td>[-0.08507562428712845, 0.058049630373716354, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Scene reconstruction</td>\n",
       "      <td>\\nGiven one or (typically) more images of a sc...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Sc...</td>\n",
       "      <td>115</td>\n",
       "      <td>[-0.6162410378456116, 0.3802846670150757, 1.66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Image restoration</td>\n",
       "      <td>\\nImage restoration comes into picture when th...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Im...</td>\n",
       "      <td>204</td>\n",
       "      <td>[-0.25118744373321533, 0.52935791015625, 1.085...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>System methods</td>\n",
       "      <td></td>\n",
       "      <td>\\nThe organization of a computer vision system...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;System methods</td>\n",
       "      <td>122</td>\n",
       "      <td>[0.13426706194877625, 0.5260908603668213, 1.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>System methods</td>\n",
       "      <td></td>\n",
       "      <td>\\nImage acquisition – A digital image is produ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;System methods</td>\n",
       "      <td>253</td>\n",
       "      <td>[0.13420583307743073, 0.20847204327583313, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>System methods</td>\n",
       "      <td></td>\n",
       "      <td>\\nLocalized interest points such as corners, b...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;System methods</td>\n",
       "      <td>297</td>\n",
       "      <td>[-0.2144298553466797, -0.15774108469486237, 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>System methods</td>\n",
       "      <td>Image-understanding systems</td>\n",
       "      <td>\\nImage-understanding systems (IUS) include th...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;System methods-&gt;I...</td>\n",
       "      <td>202</td>\n",
       "      <td>[-0.3703722059726715, 0.7370752692222595, 0.51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Hardware</td>\n",
       "      <td></td>\n",
       "      <td>\\nThere are many kinds of computer vision syst...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Hardware</td>\n",
       "      <td>409</td>\n",
       "      <td>[0.02258257381618023, 0.39461076259613037, 1.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Source         Heading                   Subheading  \\\n",
       "0   Wikipedia (Computer vision)                                                \n",
       "1   Wikipedia (Computer vision)      Definition                                \n",
       "2   Wikipedia (Computer vision)         History                                \n",
       "3   Wikipedia (Computer vision)         History                                \n",
       "5   Wikipedia (Computer vision)  Related fields          Solid-state physics   \n",
       "6   Wikipedia (Computer vision)  Related fields                 Neurobiology   \n",
       "7   Wikipedia (Computer vision)  Related fields            Signal processing   \n",
       "8   Wikipedia (Computer vision)  Related fields           Robotic navigation   \n",
       "9   Wikipedia (Computer vision)  Related fields                 Other fields   \n",
       "10  Wikipedia (Computer vision)  Related fields                 Distinctions   \n",
       "11  Wikipedia (Computer vision)  Related fields                 Distinctions   \n",
       "12  Wikipedia (Computer vision)    Applications                                \n",
       "13  Wikipedia (Computer vision)    Applications                     Medicine   \n",
       "14  Wikipedia (Computer vision)    Applications               Machine vision   \n",
       "15  Wikipedia (Computer vision)    Applications                     Military   \n",
       "16  Wikipedia (Computer vision)    Applications          Autonomous vehicles   \n",
       "17  Wikipedia (Computer vision)    Applications             Tactile feedback   \n",
       "18  Wikipedia (Computer vision)   Typical tasks                                \n",
       "19  Wikipedia (Computer vision)   Typical tasks                  Recognition   \n",
       "20  Wikipedia (Computer vision)   Typical tasks                  Recognition   \n",
       "21  Wikipedia (Computer vision)   Typical tasks              Motion analysis   \n",
       "22  Wikipedia (Computer vision)   Typical tasks         Scene reconstruction   \n",
       "23  Wikipedia (Computer vision)   Typical tasks            Image restoration   \n",
       "24  Wikipedia (Computer vision)  System methods                                \n",
       "25  Wikipedia (Computer vision)  System methods                                \n",
       "26  Wikipedia (Computer vision)  System methods                                \n",
       "27  Wikipedia (Computer vision)  System methods  Image-understanding systems   \n",
       "28  Wikipedia (Computer vision)        Hardware                                \n",
       "\n",
       "                                              Content  \\\n",
       "0   \\nComputer vision tasks include methods for ac...   \n",
       "1   \\nComputer vision is an interdisciplinary fiel...   \n",
       "2   \\nIn the late 1960s, computer vision began at ...   \n",
       "3   \\nBy the 1990s, some of the previous research ...   \n",
       "5   \\nSolid-state physics is another field that is...   \n",
       "6   \\nNeurobiology has greatly influenced the deve...   \n",
       "7   \\nYet another field related to computer vision...   \n",
       "8   \\nRobot navigation sometimes deals with autono...   \n",
       "9   \\nBesides the above-mentioned views on compute...   \n",
       "10  \\nThe fields most closely related to computer ...   \n",
       "11  \\nImage processing and image analysis tend to ...   \n",
       "12  \\nApplications range from tasks such as indust...   \n",
       "13  \\nOne of the most prominent application fields...   \n",
       "14  \\nA second application area in computer vision...   \n",
       "15  \\nMilitary applications are probably one of th...   \n",
       "16  \\nOne of the newer application areas is autono...   \n",
       "17  \\nMaterials such as rubber and silicon are bei...   \n",
       "18  \\nEach of the application areas described abov...   \n",
       "19  \\nThe classical problem in computer vision, im...   \n",
       "20  \\nContent-based image retrieval – finding all ...   \n",
       "21  \\nSeveral tasks relate to motion estimation wh...   \n",
       "22  \\nGiven one or (typically) more images of a sc...   \n",
       "23  \\nImage restoration comes into picture when th...   \n",
       "24  \\nThe organization of a computer vision system...   \n",
       "25  \\nImage acquisition – A digital image is produ...   \n",
       "26  \\nLocalized interest points such as corners, b...   \n",
       "27  \\nImage-understanding systems (IUS) include th...   \n",
       "28  \\nThere are many kinds of computer vision syst...   \n",
       "\n",
       "                                              Section  Tokens  \\\n",
       "0                         Wikipedia (Computer vision)     290   \n",
       "1             Wikipedia (Computer vision)->Definition     162   \n",
       "2                Wikipedia (Computer vision)->History     243   \n",
       "3                Wikipedia (Computer vision)->History     264   \n",
       "5   Wikipedia (Computer vision)->Related fields->S...     123   \n",
       "6   Wikipedia (Computer vision)->Related fields->N...     296   \n",
       "7   Wikipedia (Computer vision)->Related fields->S...     106   \n",
       "8   Wikipedia (Computer vision)->Related fields->R...      65   \n",
       "9   Wikipedia (Computer vision)->Related fields->O...     122   \n",
       "10  Wikipedia (Computer vision)->Related fields->D...     244   \n",
       "11  Wikipedia (Computer vision)->Related fields->D...     395   \n",
       "12          Wikipedia (Computer vision)->Applications     294   \n",
       "13  Wikipedia (Computer vision)->Applications->Med...     145   \n",
       "14  Wikipedia (Computer vision)->Applications->Mac...     148   \n",
       "15  Wikipedia (Computer vision)->Applications->Mil...     129   \n",
       "16  Wikipedia (Computer vision)->Applications->Aut...     258   \n",
       "17  Wikipedia (Computer vision)->Applications->Tac...     286   \n",
       "18         Wikipedia (Computer vision)->Typical tasks     166   \n",
       "19  Wikipedia (Computer vision)->Typical tasks->Re...     427   \n",
       "20  Wikipedia (Computer vision)->Typical tasks->Re...     264   \n",
       "21  Wikipedia (Computer vision)->Typical tasks->Mo...     194   \n",
       "22  Wikipedia (Computer vision)->Typical tasks->Sc...     115   \n",
       "23  Wikipedia (Computer vision)->Typical tasks->Im...     204   \n",
       "24        Wikipedia (Computer vision)->System methods     122   \n",
       "25        Wikipedia (Computer vision)->System methods     253   \n",
       "26        Wikipedia (Computer vision)->System methods     297   \n",
       "27  Wikipedia (Computer vision)->System methods->I...     202   \n",
       "28              Wikipedia (Computer vision)->Hardware     409   \n",
       "\n",
       "                                            Embedding  \n",
       "0   [-0.5566069483757019, 0.6151323318481445, 0.70...  \n",
       "1   [-0.18940897285938263, 0.5564344525337219, 0.5...  \n",
       "2   [-0.5624328255653381, 0.35494446754455566, 0.6...  \n",
       "3   [-0.8420819044113159, 0.011862404644489288, 0....  \n",
       "5   [-0.1766018569469452, 0.5509802103042603, 0.11...  \n",
       "6   [0.07454751431941986, 0.42800015211105347, 0.1...  \n",
       "7   [-0.1562240570783615, 0.18830031156539917, 0.4...  \n",
       "8   [0.09209920465946198, 0.5207788944244385, 1.26...  \n",
       "9   [-0.34635502099990845, 0.12120083719491959, 0....  \n",
       "10  [-0.011814514175057411, 0.9892112612724304, 0....  \n",
       "11  [-0.25189733505249023, 0.7090330123901367, 1.0...  \n",
       "12  [0.054026536643505096, 0.536332368850708, 0.80...  \n",
       "13  [0.07362960278987885, 0.8047475218772888, 1.09...  \n",
       "14  [0.23051360249519348, 0.6428527235984802, 0.69...  \n",
       "15  [-0.45685380697250366, 0.40300095081329346, 0....  \n",
       "16  [-0.5068467855453491, 0.4222138524055481, 1.11...  \n",
       "17  [-0.006287522614002228, 0.3353821039199829, 0....  \n",
       "18  [-0.3646470308303833, 0.5144392848014832, 1.08...  \n",
       "19  [-0.11414705961942673, 0.8190616965293884, 0.9...  \n",
       "20  [-0.3213890492916107, 1.116416335105896, 0.800...  \n",
       "21  [-0.08507562428712845, 0.058049630373716354, 1...  \n",
       "22  [-0.6162410378456116, 0.3802846670150757, 1.66...  \n",
       "23  [-0.25118744373321533, 0.52935791015625, 1.085...  \n",
       "24  [0.13426706194877625, 0.5260908603668213, 1.00...  \n",
       "25  [0.13420583307743073, 0.20847204327583313, 0.6...  \n",
       "26  [-0.2144298553466797, -0.15774108469486237, 1....  \n",
       "27  [-0.3703722059726715, 0.7370752692222595, 0.51...  \n",
       "28  [0.02258257381618023, 0.39461076259613037, 1.0...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CompVisionKnowledgeBERT = Knowledge(WIKI_PAGE, 'BERT')\n",
    "CompVisionKnowledgeBERT.append_wikipedia_page(WIKI_PAGE)\n",
    "# save document chunks and embeddings\n",
    "CompVisionKnowledgeBERT.export_to_csv(BERT_KNOWLEDGE_FILENAME)\n",
    "CompVisionKnowledgeBERT.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Search\n",
    "Now we'll define a search function that:\n",
    "\n",
    "Takes a user query and a dataframe with text & embedding columns\n",
    "Embeds the user query with the OpenAI API\n",
    "Uses distance between query embedding and text embeddings to rank the texts\n",
    "Returns two lists:\n",
    "The top N texts, ranked by relevance\n",
    "Their corresponding relevance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Todo:\n",
    "# I need to make it more efficient on the number of tokens\n",
    "# Check num_tokens usage and specifying the encoding model\n",
    "# Fix the potential issue fo GPT sections being longer than 1024 tokens when using BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\point\\OneDrive\\Documents\\Standard Files\\University\\Masters\\Diss\\MSc-Diss\\code\\modules\\query.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Index'] = np.arange(len(self.knowledge_used)) + 1\n",
      "C:\\Users\\point\\OneDrive\\Documents\\Standard Files\\University\\Masters\\Diss\\MSc-Diss\\code\\modules\\query.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Cumulative_tokens'] = self.knowledge_used['Tokens'].cumsum()\n",
      "C:\\Users\\point\\OneDrive\\Documents\\Standard Files\\University\\Masters\\Diss\\MSc-Diss\\code\\modules\\query.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Cumulative_tokens'] += message_and_question_tokens  # add the initial number of tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA stands for Principal Component Analysis, which is a statistical technique used for dimensionality reduction of correlated data. It involves finding a new set of uncorrelated variables (or axes) that are linear combinations of the original variables, and which capture the maximum amount of variance in the data. PCA is commonly used in digital imaging for tasks such as data compression, automated facial recognition, and determination of the orientation of basic shapes.\n",
      "\n",
      "To construct this answer, I used the following documents : \n",
      "\n",
      "1. CompVisionPDF-> p.265/266/267:\n",
      "The reader is probably familiar with the common saying that goes something along the\n",
      "lines of 'Why ...\n",
      "\n",
      "2. CompVisionPDF-> p.268/269/270:\n",
      "For a 2-D space, we must stop here (there are no more dimensions left). Figure 9.9\n",
      "shows our exampl...\n",
      "\n",
      "Total tokens used: 3227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\point\\OneDrive\\Documents\\Standard Files\\University\\Masters\\Diss\\MSc-Diss\\code\\modules\\query.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Output'] = '\\n\\n' + self.knowledge_used['Index'].astype(str) + '. ' + self.knowledge_used[\n"
     ]
    }
   ],
   "source": [
    "CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\n",
    "print(Query.ask('What is PCA?', CompVisionGPT, show_source=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d07a56b31944c4bb14a999b57fd3c4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (768,) (1536,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# CompVisionBERT = ChatBot(\"Computer Vision\", 'assets/' + BERT_KNOWLEDGE_FILENAME)\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mQuery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mask_bert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mWhen did universities begin teaching Computer Vision?\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCompVisionGPT\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.ask_bert\u001B[1;34m(cls, query_text, chatbot_instance, embedding_model, encoding_model, bert_model, show_source)\u001B[0m\n\u001B[0;32m    126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQuestion is too long, please try again with a shorter question.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    127\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(query_text, chatbot_instance)\n\u001B[1;32m--> 128\u001B[0m response_message, answer_index \u001B[38;5;241m=\u001B[39m \u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_bert_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membedding_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbert_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbert_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m show_source \u001B[38;5;129;01mand\u001B[39;00m response_message\u001B[38;5;241m!=\u001B[39mANSWER_NOT_FOUND_MSG: \u001B[38;5;66;03m# Display the sources used:\u001B[39;00m\n\u001B[0;32m    131\u001B[0m     response_message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m query\u001B[38;5;241m.\u001B[39mshow_source_message(answer_index\u001B[38;5;241m=\u001B[39manswer_index)\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.get_bert_output\u001B[1;34m(self, embedding_model, encoding_model, bert_model)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_bert_output\u001B[39m(\n\u001B[0;32m     77\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     78\u001B[0m         embedding_model: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m     79\u001B[0m         encoding_model: BertTokenizer \u001B[38;5;241m=\u001B[39m BERT_ENCODING,\n\u001B[0;32m     80\u001B[0m         bert_model: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m BERT_MODEL\n\u001B[0;32m     81\u001B[0m ):\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;124;03m\"\"\"Uses the most relevant texts from the knowledge dataframe to construct a message that can then be fed into GPT.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 83\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mknowledge_ranked_by_similarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membedding_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m     answer_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     86\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.knowledge_ranked_by_similarity\u001B[1;34m(self, max_num_sections, confidence_level, embedding_model)\u001B[0m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# knowledge_with_similarities[\"similarity\"] = knowledge_with_similarities[\"Embedding\"].apply(lambda x: self.similarity(query_embedding, x))\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(query_embedding_response)\n\u001B[1;32m---> 34\u001B[0m knowledge_with_similarities[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mknowledge_with_similarities\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEmbedding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_embedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     36\u001B[0m knowledge_with_similarities\u001B[38;5;241m.\u001B[39msort_values(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     37\u001B[0m top_n_sections \u001B[38;5;241m=\u001B[39m knowledge_with_similarities\u001B[38;5;241m.\u001B[39mhead(max_num_sections)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[0;32m   4323\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4324\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4325\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4328\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4329\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4330\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4331\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4332\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4431\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4432\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4433\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1078\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m   1079\u001B[0m     \u001B[38;5;66;03m# if we are a string, try to dispatch\u001B[39;00m\n\u001B[0;32m   1080\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[1;32m-> 1082\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1131\u001B[0m         values \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values\n\u001B[0;32m   1132\u001B[0m         \u001B[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001B[39;00m\n\u001B[0;32m   1133\u001B[0m         \u001B[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001B[39;00m\n\u001B[0;32m   1134\u001B[0m         \u001B[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001B[39;00m\n\u001B[0;32m   1135\u001B[0m         \u001B[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001B[39;00m\n\u001B[0;32m   1136\u001B[0m         \u001B[38;5;66;03m# \"Callable[[Any], Any]\"\u001B[39;00m\n\u001B[1;32m-> 1137\u001B[0m         mapped \u001B[38;5;241m=\u001B[39m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1138\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1139\u001B[0m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m   1140\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1141\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1144\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1145\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1146\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.knowledge_ranked_by_similarity.<locals>.<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# knowledge_with_similarities[\"similarity\"] = knowledge_with_similarities[\"Embedding\"].apply(lambda x: self.similarity(query_embedding, x))\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(query_embedding_response)\n\u001B[1;32m---> 34\u001B[0m knowledge_with_similarities[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m knowledge_with_similarities[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmbedding\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_embedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     36\u001B[0m knowledge_with_similarities\u001B[38;5;241m.\u001B[39msort_values(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     37\u001B[0m top_n_sections \u001B[38;5;241m=\u001B[39m knowledge_with_similarities\u001B[38;5;241m.\u001B[39mhead(max_num_sections)\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.similarity\u001B[1;34m(query_embedding, knowledge_embedding)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msimilarity\u001B[39m(query_embedding: \u001B[38;5;28mlist\u001B[39m,\n\u001B[0;32m     13\u001B[0m                knowledge_embedding: \u001B[38;5;28mlist\u001B[39m\n\u001B[0;32m     14\u001B[0m                ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;124;03m\"\"\"Calculates the cosine similarity score between the query and knowledge embedding vectors.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m1\u001B[39m\u001B[38;5;241m-\u001B[39m \u001B[43mspatial\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcosine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_embedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mknowledge_embedding\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:670\u001B[0m, in \u001B[0;36mcosine\u001B[1;34m(u, v, w)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;124;03mCompute the Cosine distance between 1-D arrays.\u001B[39;00m\n\u001B[0;32m    630\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    665\u001B[0m \n\u001B[0;32m    666\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    667\u001B[0m \u001B[38;5;66;03m# cosine distance is also referred to as 'uncentered correlation',\u001B[39;00m\n\u001B[0;32m    668\u001B[0m \u001B[38;5;66;03m#   or 'reflective correlation'\u001B[39;00m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;66;03m# clamp the result to 0-2\u001B[39;00m\n\u001B[1;32m--> 670\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mmin\u001B[39m(\u001B[43mcorrelation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mu\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcentered\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m, \u001B[38;5;241m2.0\u001B[39m))\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:619\u001B[0m, in \u001B[0;36mcorrelation\u001B[1;34m(u, v, w, centered)\u001B[0m\n\u001B[0;32m    617\u001B[0m     u \u001B[38;5;241m=\u001B[39m u \u001B[38;5;241m-\u001B[39m umu\n\u001B[0;32m    618\u001B[0m     v \u001B[38;5;241m=\u001B[39m v \u001B[38;5;241m-\u001B[39m vmu\n\u001B[1;32m--> 619\u001B[0m uv \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39maverage(\u001B[43mu\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m, weights\u001B[38;5;241m=\u001B[39mw)\n\u001B[0;32m    620\u001B[0m uu \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39maverage(np\u001B[38;5;241m.\u001B[39msquare(u), weights\u001B[38;5;241m=\u001B[39mw)\n\u001B[0;32m    621\u001B[0m vv \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39maverage(np\u001B[38;5;241m.\u001B[39msquare(v), weights\u001B[38;5;241m=\u001B[39mw)\n",
      "\u001B[1;31mValueError\u001B[0m: operands could not be broadcast together with shapes (768,) (1536,) "
     ]
    }
   ],
   "source": [
    "CompVisionBERT = ChatBot(\"Computer Vision\", 'assets/' + BERT_KNOWLEDGE_FILENAME)\n",
    "print(Query.ask_bert('When did universities begin teaching Computer Vision?', CompVisionBERT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96c7a743564c4afda124d34911ac58d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [20]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mQuery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mask_bart\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mWhen did Universities begin teaching Computer Vision?\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCompVisionBERT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_source\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.ask_bart\u001B[1;34m(cls, query_text, chatbot_instance, show_source, confidence_level)\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQuestion is too long, please try again with a shorter question.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    214\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(query_text, chatbot_instance)\n\u001B[1;32m--> 215\u001B[0m response_message \u001B[38;5;241m=\u001B[39m \u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_bart_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfidence_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfidence_level\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m show_source \u001B[38;5;129;01mand\u001B[39;00m response_message\u001B[38;5;241m!=\u001B[39mANSWER_NOT_FOUND_MSG: \u001B[38;5;66;03m# Display the sources used:\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     response_message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m query\u001B[38;5;241m.\u001B[39mshow_source_message()\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.get_bart_output\u001B[1;34m(self, encoding_model, bert_model, confidence_level)\u001B[0m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_bart_output\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    171\u001B[0m                     \u001B[38;5;66;03m# chatbot_instance: ChatBot,\u001B[39;00m\n\u001B[0;32m    172\u001B[0m                     \u001B[38;5;66;03m# embedding_model: str = BART_EMBEDDING_MODEL,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    175\u001B[0m                     confidence_level: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m,\n\u001B[0;32m    176\u001B[0m                     ):\n\u001B[1;32m--> 177\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mknowledge_ranked_by_similarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfidence_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfidence_level\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mknowledge_used)\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m ANSWER_NOT_FOUND_MSG\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.knowledge_ranked_by_similarity\u001B[1;34m(self, max_num_sections, confidence_level, embedding_model)\u001B[0m\n\u001B[0;32m     28\u001B[0m query_embedding_response \u001B[38;5;241m=\u001B[39m get_embedding(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontent, embedding_model\u001B[38;5;241m=\u001B[39mBERT_EMBEDDING_MODEL)\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m embedding_model \u001B[38;5;241m==\u001B[39m GPT_EMBEDDING_MODEL:\n\u001B[1;32m---> 30\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[43mquery_embedding_response\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# knowledge_with_similarities[\"similarity\"] = knowledge_with_similarities[\"Embedding\"].apply(lambda x: self.similarity(query_embedding, x))\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(query_embedding_response)\n",
      "\u001B[1;31mIndexError\u001B[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\n",
    "print(Query.ask_bart('When did Universities begin teaching Computer Vision?', CompVisionGPT, show_source=True))  # What if the GPT knowledge sections are longer than 1024 tokens?? Need to account for this!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[6.7410e+22, 2.6729e+23, 5.3689e-05],\n",
      "        [1.3542e-05, 5.2905e-08, 6.7942e-07]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def describe(x):\n",
    "    print(\"Type: {}\".format(x.type()))\n",
    "    print(\"Shape/size: {}\".format(x.shape))\n",
    "    print(\"Values: \\n{}\".format(x))\n",
    "\n",
    "describe(torch.Tensor(2, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tasks\n",
    "Clean the parsed text so we have pure english words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", '’', 'crazy', 'ones', ',', 'misfits', ',', 'rebels', ',', 'troublemakers', ',', 'round', 'pegs', 'square', 'holes', '.', 'ones', 'see', 'things', 'differently', '—', '’', 'fond', 'rules', '.', 'quote', ',', 'disagree', ',', 'glorify', 'vilify', ',', 'thing', '’', 'ignore', 'change', 'things', '.', 'push', 'human', 'race', 'forward', ',', 'may', 'see', 'crazy', 'ones', ',', 'see', 'genius', ',', 'ones', 'crazy', 'enough', 'think', 'change', 'world', ',', 'ones', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"It's Here’s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. The ones who see things differently — they’re not fond of rules. You can quote them, disagree with them, glorify or vilify them, but the only thing you can’t do is ignore them because they change things. They push the human race forward, and while some may see them as the crazy ones, we see genius, because the ones who are crazy enough to think that they can change the world, are the ones who do.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "sentences = [\n",
    "\"Artificial intelligence (AI) is revolutionising industries and transforming the way we live and work.\",\n",
    "\"Machine learning, a subset of AI, enables computers to learn and make predictions or decisions without being explicitly programmed.\",\n",
    "\"AI algorithms can analyse vast amounts of data to uncover patterns, detect anomalies, and extract valuable insights.\",\n",
    "\"Natural language processing (NLP) allows machines to understand and process human language, facilitating communication between humans and computers.\",\n",
    "\"Computer vision enables machines to interpret and understand visual information, enabling applications such as image recognition and object detection.\",\n",
    "\"AI-powered virtual assistants like Siri, Alexa, and Google Assistant are becoming increasingly common, providing voice-based interactions and personalised experiences.\",\n",
    "\"AI is driving advancements in healthcare, including disease diagnosis, drug discovery, and personalised medicine, leading to improved patient outcomes.\",\n",
    "\"In the field of autonomous vehicles, AI plays a crucial role in enabling self-driving cars to perceive their surroundings and make informed decisions.\",\n",
    "\"AI is transforming the customer service industry by utilising chatbots and automated systems to provide faster and more efficient support to customers.\",\n",
    "\"Ethical considerations, such as transparency, fairness, and privacy, are essential in the development and deployment of AI systems to ensure responsible and accountable use.\",\n",
    "\"Cars have revolutionised transportation, providing a convenient and efficient means of travel for people around the world.\",\n",
    "\"Automobile manufacturing involves a complex process of designing, engineering, and assembling various components to create a functional vehicle.\",\n",
    "\"Safety features such as seat belts, airbags, and anti-lock braking systems have greatly improved the overall safety of cars.\",\n",
    "\"Electric vehicles (EVs) are gaining popularity as eco-friendly alternatives to traditional petrol-powered cars, reducing carbon emissions and dependence on fossil fuels.\",\n",
    "\"Advanced driver-assistance systems (ADAS) enhance car safety by incorporating technologies like adaptive cruise control and lane-keeping assist.\",\n",
    "\"Sports cars are known for their high-performance capabilities, offering speed, agility, and an exhilarating driving experience.\",\n",
    "\"Classic cars hold a special place in automotive history, with their timeless designs and nostalgic appeal capturing the hearts of car enthusiasts.\",\n",
    "\"Car customisation allows owners to personalise their vehicles, from unique paint jobs and body modifications to performance upgrades.\",\n",
    "\"Car-sharing services and ride-hailing apps have transformed the way people access transportation, providing convenient alternatives to car ownership.\",\n",
    "\"The future of cars is expected to bring autonomous vehicles, where cars can navigate and operate without human intervention, promising increased safety and efficiency.\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_text(text: str, remove_stopwords: bool) -> str:\n",
    "    \"\"\"Function that cleans the input text by going to:\n",
    "    - remove links\n",
    "    - remove special characters\n",
    "    - remove numbers\n",
    "    - remove stopwords\n",
    "    - convert to lowercase\n",
    "    - remove excessive white spaces\n",
    "    Arguments:\n",
    "        text (str): text to clean\n",
    "        remove_stopwords (bool): whether to remove stopwords\n",
    "    Returns:\n",
    "        str: cleaned text\n",
    "    \"\"\"\n",
    "    # remove links\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # remove numbers and special characters\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
    "    # remove stopwords\n",
    "    if remove_stopwords:\n",
    "        # 1. create tokens\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # 2. check if it's a stopword\n",
    "        tokens = [w.lower().strip() for w in tokens if not w.lower() in stopwords.words(\"english\")]\n",
    "        # return a list of cleaned tokens\n",
    "        return tokens\n",
    "\n",
    "tokenised_sentences = [preprocess_text(sentence, remove_stopwords=True) for sentence in sentences]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "[['artificial',\n  'intelligence',\n  'ai',\n  'revolutionising',\n  'industries',\n  'transforming',\n  'way',\n  'live',\n  'work'],\n ['machine',\n  'learning',\n  'subset',\n  'ai',\n  'enables',\n  'computers',\n  'learn',\n  'make',\n  'predictions',\n  'decisions',\n  'without',\n  'explicitly',\n  'programmed'],\n ['ai',\n  'algorithms',\n  'analyse',\n  'vast',\n  'amounts',\n  'data',\n  'uncover',\n  'patterns',\n  'detect',\n  'anomalies',\n  'extract',\n  'valuable',\n  'insights'],\n ['natural',\n  'language',\n  'processing',\n  'nlp',\n  'allows',\n  'machines',\n  'understand',\n  'process',\n  'human',\n  'language',\n  'facilitating',\n  'communication',\n  'humans',\n  'computers'],\n ['computer',\n  'vision',\n  'enables',\n  'machines',\n  'interpret',\n  'understand',\n  'visual',\n  'information',\n  'enabling',\n  'applications',\n  'image',\n  'recognition',\n  'object',\n  'detection'],\n ['ai',\n  'powered',\n  'virtual',\n  'assistants',\n  'like',\n  'siri',\n  'alexa',\n  'google',\n  'assistant',\n  'becoming',\n  'increasingly',\n  'common',\n  'providing',\n  'voice',\n  'based',\n  'interactions',\n  'personalised',\n  'experiences'],\n ['ai',\n  'driving',\n  'advancements',\n  'healthcare',\n  'including',\n  'disease',\n  'diagnosis',\n  'drug',\n  'discovery',\n  'personalised',\n  'medicine',\n  'leading',\n  'improved',\n  'patient',\n  'outcomes'],\n ['field',\n  'autonomous',\n  'vehicles',\n  'ai',\n  'plays',\n  'crucial',\n  'role',\n  'enabling',\n  'self',\n  'driving',\n  'cars',\n  'perceive',\n  'surroundings',\n  'make',\n  'informed',\n  'decisions'],\n ['ai',\n  'transforming',\n  'customer',\n  'service',\n  'industry',\n  'utilising',\n  'chatbots',\n  'automated',\n  'systems',\n  'provide',\n  'faster',\n  'efficient',\n  'support',\n  'customers'],\n ['ethical',\n  'considerations',\n  'transparency',\n  'fairness',\n  'privacy',\n  'essential',\n  'development',\n  'deployment',\n  'ai',\n  'systems',\n  'ensure',\n  'responsible',\n  'accountable',\n  'use'],\n ['cars',\n  'revolutionised',\n  'transportation',\n  'providing',\n  'convenient',\n  'efficient',\n  'means',\n  'travel',\n  'people',\n  'around',\n  'world'],\n ['automobile',\n  'manufacturing',\n  'involves',\n  'complex',\n  'process',\n  'designing',\n  'engineering',\n  'assembling',\n  'various',\n  'components',\n  'create',\n  'functional',\n  'vehicle'],\n ['safety',\n  'features',\n  'seat',\n  'belts',\n  'airbags',\n  'anti',\n  'lock',\n  'braking',\n  'systems',\n  'greatly',\n  'improved',\n  'overall',\n  'safety',\n  'cars'],\n ['electric',\n  'vehicles',\n  'evs',\n  'gaining',\n  'popularity',\n  'eco',\n  'friendly',\n  'alternatives',\n  'traditional',\n  'petrol',\n  'powered',\n  'cars',\n  'reducing',\n  'carbon',\n  'emissions',\n  'dependence',\n  'fossil',\n  'fuels'],\n ['advanced',\n  'driver',\n  'assistance',\n  'systems',\n  'adas',\n  'enhance',\n  'car',\n  'safety',\n  'incorporating',\n  'technologies',\n  'like',\n  'adaptive',\n  'cruise',\n  'control',\n  'lane',\n  'keeping',\n  'assist'],\n ['sports',\n  'cars',\n  'known',\n  'high',\n  'performance',\n  'capabilities',\n  'offering',\n  'speed',\n  'agility',\n  'exhilarating',\n  'driving',\n  'experience'],\n ['classic',\n  'cars',\n  'hold',\n  'special',\n  'place',\n  'automotive',\n  'history',\n  'timeless',\n  'designs',\n  'nostalgic',\n  'appeal',\n  'capturing',\n  'hearts',\n  'car',\n  'enthusiasts'],\n ['car',\n  'customisation',\n  'allows',\n  'owners',\n  'personalise',\n  'vehicles',\n  'unique',\n  'paint',\n  'jobs',\n  'body',\n  'modifications',\n  'performance',\n  'upgrades'],\n ['car',\n  'sharing',\n  'services',\n  'ride',\n  'hailing',\n  'apps',\n  'transformed',\n  'way',\n  'people',\n  'access',\n  'transportation',\n  'providing',\n  'convenient',\n  'alternatives',\n  'car',\n  'ownership'],\n ['future',\n  'cars',\n  'expected',\n  'bring',\n  'autonomous',\n  'vehicles',\n  'cars',\n  'navigate',\n  'operate',\n  'without',\n  'human',\n  'intervention',\n  'promising',\n  'increased',\n  'safety',\n  'efficiency']]"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised_sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "[['artificial',\n  'intelligence',\n  'ai',\n  'revolutionise',\n  'industry',\n  'transform',\n  'way',\n  'live',\n  'work'],\n ['machine',\n  'learn',\n  'subset',\n  'ai',\n  'enable',\n  'computer',\n  'learn',\n  'make',\n  'prediction',\n  'decision',\n  'without',\n  'explicitly',\n  'program'],\n ['ai',\n  'algorithm',\n  'analyse',\n  'vast',\n  'amount',\n  'data',\n  'uncover',\n  'pattern',\n  'detect',\n  'anomaly',\n  'extract',\n  'valuable',\n  'insight'],\n ['natural',\n  'language',\n  'processing',\n  'nlp',\n  'allow',\n  'machine',\n  'understand',\n  'process',\n  'human',\n  'language',\n  'facilitate',\n  'communication',\n  'human',\n  'computer'],\n ['computer',\n  'vision',\n  'enable',\n  'machine',\n  'interpret',\n  'understand',\n  'visual',\n  'information',\n  'enable',\n  'application',\n  'image',\n  'recognition',\n  'object',\n  'detection'],\n ['ai',\n  'power',\n  'virtual',\n  'assistant',\n  'like',\n  'siri',\n  'alexa',\n  'google',\n  'assistant',\n  'become',\n  'increasingly',\n  'common',\n  'provide',\n  'voice',\n  'base',\n  'interaction',\n  'personalise',\n  'experience'],\n ['ai',\n  'drive',\n  'advancement',\n  'healthcare',\n  'include',\n  'disease',\n  'diagnosis',\n  'drug',\n  'discovery',\n  'personalise',\n  'medicine',\n  'lead',\n  'improved',\n  'patient',\n  'outcome'],\n ['field',\n  'autonomous',\n  'vehicle',\n  'ai',\n  'play',\n  'crucial',\n  'role',\n  'enable',\n  'self',\n  'drive',\n  'car',\n  'perceive',\n  'surroundings',\n  'make',\n  'informed',\n  'decision'],\n ['ai',\n  'transform',\n  'customer',\n  'service',\n  'industry',\n  'utilise',\n  'chatbots',\n  'automate',\n  'system',\n  'provide',\n  'faster',\n  'efficient',\n  'support',\n  'customer'],\n ['ethical',\n  'consideration',\n  'transparency',\n  'fairness',\n  'privacy',\n  'essential',\n  'development',\n  'deployment',\n  'ai',\n  'system',\n  'ensure',\n  'responsible',\n  'accountable',\n  'use'],\n ['car',\n  'revolutionise',\n  'transportation',\n  'provide',\n  'convenient',\n  'efficient',\n  'mean',\n  'travel',\n  'people',\n  'around',\n  'world'],\n ['automobile',\n  'manufacture',\n  'involves',\n  'complex',\n  'process',\n  'design',\n  'engineering',\n  'assemble',\n  'various',\n  'component',\n  'create',\n  'functional',\n  'vehicle'],\n ['safety',\n  'feature',\n  'seat',\n  'belt',\n  'airbags',\n  'anti',\n  'lock',\n  'brake',\n  'system',\n  'greatly',\n  'improve',\n  'overall',\n  'safety',\n  'car'],\n ['electric',\n  'vehicle',\n  'evs',\n  'gain',\n  'popularity',\n  'eco',\n  'friendly',\n  'alternative',\n  'traditional',\n  'petrol',\n  'power',\n  'car',\n  'reduce',\n  'carbon',\n  'emission',\n  'dependence',\n  'fossil',\n  'fuel'],\n ['advanced',\n  'driver',\n  'assistance',\n  'system',\n  'adas',\n  'enhance',\n  'car',\n  'safety',\n  'incorporate',\n  'technology',\n  'like',\n  'adaptive',\n  'cruise',\n  'control',\n  'lane',\n  'keep',\n  'assist'],\n ['sport',\n  'car',\n  'know',\n  'high',\n  'performance',\n  'capability',\n  'offer',\n  'speed',\n  'agility',\n  'exhilarate',\n  'drive',\n  'experience'],\n ['classic',\n  'car',\n  'hold',\n  'special',\n  'place',\n  'automotive',\n  'history',\n  'timeless',\n  'design',\n  'nostalgic',\n  'appeal',\n  'capture',\n  'heart',\n  'car',\n  'enthusiast'],\n ['car',\n  'customisation',\n  'allow',\n  'owner',\n  'personalise',\n  'vehicle',\n  'unique',\n  'paint',\n  'job',\n  'body',\n  'modification',\n  'performance',\n  'upgrade'],\n ['car',\n  'share',\n  'service',\n  'ride',\n  'hail',\n  'apps',\n  'transform',\n  'way',\n  'people',\n  'access',\n  'transportation',\n  'provide',\n  'convenient',\n  'alternative',\n  'car',\n  'ownership'],\n ['future',\n  'car',\n  'expect',\n  'bring',\n  'autonomous',\n  'vehicle',\n  'car',\n  'navigate',\n  'operate',\n  'without',\n  'human',\n  'intervention',\n  'promising',\n  'increase',\n  'safety',\n  'efficiency']]"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN # Default to noun if the POS tag is unknown/ambiguous\n",
    "\n",
    "def lemmatize_sentence(tokenised_sentence):\n",
    "    tagged_tokens = nltk.pos_tag(tokenised_sentence)\n",
    "    lemmatized_words = []\n",
    "    for word, tag in tagged_tokens:\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(word, pos=pos))\n",
    "    return lemmatized_words\n",
    "\n",
    "# refined_sentences = [sentence.strip()]\n",
    "lemmatised_sentences = [lemmatize_sentence(s) for s in tokenised_sentences]\n",
    "lemmatised_sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "VECTOR_SIZE = 100\n",
    "MIN_COUNT = 1\n",
    "WINDOW = 3\n",
    "SG = 1\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=lemmatised_sentences,\n",
    "    vector_size=VECTOR_SIZE,\n",
    "    min_count=MIN_COUNT,\n",
    "    sg=SG\n",
    ")\n",
    "\n",
    "model = Word2Vec(sentences=lemmatised_sentences, min_count=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.052282296"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('ai', 'algorithm')\n",
    "\n",
    "# model.wv.most_similar(positive=['ai'], topn=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\point\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning:\n",
      "\n",
      "The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "\n",
      "C:\\Users\\point\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning:\n",
      "\n",
      "The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "markers",
         "text": [
          "cars",
          "ai",
          "car",
          "vehicles",
          "systems",
          "safety",
          "driving",
          "providing",
          "personalized",
          "like",
          "enabling",
          "alternatives",
          "people",
          "performance",
          "improved",
          "human",
          "process",
          "understand",
          "machines",
          "allows",
          "language",
          "powered",
          "autonomous",
          "way",
          "convenient",
          "enables",
          "efficient",
          "transforming",
          "transportation",
          "computers",
          "without",
          "make",
          "decisions",
          "advancements",
          "diagnosis",
          "provide",
          "disease",
          "including",
          "healthcare",
          "customer",
          "faster",
          "discovery",
          "experiences",
          "interactions",
          "based",
          "voice",
          "support",
          "customers",
          "drug",
          "chatbots",
          "medicine",
          "leading",
          "informed",
          "surroundings",
          "perceive",
          "self",
          "role",
          "crucial",
          "plays",
          "increasingly",
          "field",
          "outcomes",
          "industry",
          "utilizing",
          "patient",
          "service",
          "automated",
          "common",
          "efficiency",
          "becoming",
          "valuable",
          "anomalies",
          "detect",
          "patterns",
          "uncover",
          "data",
          "amounts",
          "vast",
          "analyze",
          "algorithms",
          "programmed",
          "explicitly",
          "predictions",
          "learn",
          "subset",
          "learning",
          "machine",
          "work",
          "live",
          "industries",
          "revolutionizing",
          "intelligence",
          "extract",
          "insights",
          "assistant",
          "natural",
          "google",
          "alexa",
          "siri",
          "assistants",
          "virtual",
          "considerations",
          "detection",
          "object",
          "recognition",
          "image",
          "applications",
          "information",
          "visual",
          "interpret",
          "vision",
          "computer",
          "humans",
          "communication",
          "facilitating",
          "nlp",
          "processing",
          "ethical",
          "essential",
          "transparency",
          "agility",
          "nostalgic",
          "designs",
          "timeless",
          "history",
          "automotive",
          "place",
          "special",
          "hold",
          "classic",
          "experience",
          "thrilling",
          "speed",
          "capturing",
          "offering",
          "capabilities",
          "high",
          "known",
          "sports",
          "assist",
          "keeping",
          "lane",
          "control",
          "cruise",
          "adaptive",
          "appeal",
          "hearts",
          "fairness",
          "hailing",
          "promising",
          "intervention",
          "operate",
          "navigate",
          "bring",
          "expected",
          "future",
          "ownership",
          "access",
          "transformed",
          "apps",
          "ride",
          "enthusiasts",
          "services",
          "sharing",
          "upgrades",
          "modifications",
          "body",
          "jobs",
          "paint",
          "unique",
          "personalize",
          "owners",
          "customization",
          "technologies",
          "incorporating",
          "enhance",
          "world",
          "functional",
          "create",
          "components",
          "various",
          "assembling",
          "engineering",
          "designing",
          "complex",
          "involves",
          "manufacturing",
          "automobile",
          "around",
          "adas",
          "travel",
          "means",
          "revolutionized",
          "use",
          "accountable",
          "responsible",
          "ensure",
          "deployment",
          "development",
          "increased",
          "privacy",
          "vehicle",
          "features",
          "seat",
          "belts",
          "assistance",
          "driver",
          "advanced",
          "fuels",
          "fossil",
          "dependence",
          "emissions",
          "carbon",
          "reducing",
          "gasoline",
          "traditional",
          "friendly",
          "eco",
          "popularity",
          "gaining",
          "evs",
          "electric",
          "overall",
          "greatly",
          "braking",
          "lock",
          "anti",
          "airbags",
          "artificial"
         ],
         "x": [
          -3.5569868087768555,
          -3.345669984817505,
          -1.1631700992584229,
          -4.823192596435547,
          1.4799708127975464,
          -2.5234670639038086,
          3.3336856365203857,
          4.033675193786621,
          -1.097055435180664,
          1.8911492824554443,
          -4.095557689666748,
          -6.807097434997559,
          3.94748592376709,
          1.8565915822982788,
          -6.009923458099365,
          0.7721247673034668,
          -2.0102412700653076,
          -7.494566917419434,
          -5.4318718910217285,
          4.244304180145264,
          1.411681890487671,
          5.025524139404297,
          -3.7039387226104736,
          -8.411649703979492,
          -0.7734807133674622,
          0.2362508475780487,
          2.539975643157959,
          -1.1920018196105957,
          7.879120826721191,
          -4.905570030212402,
          4.294675827026367,
          7.35902738571167,
          -6.065314292907715,
          5.191819190979004,
          -5.435687065124512,
          0.6472213864326477,
          8.186429023742676,
          3.4760055541992188,
          6.416111946105957,
          3.8564202785491943,
          0.012898512184619904,
          -2.008552312850952,
          -3.6230485439300537,
          -4.542641639709473,
          7.342559814453125,
          -0.9007460474967957,
          -5.753945827484131,
          -2.073754072189331,
          -1.8793163299560547,
          -1.26099693775177,
          -5.374608993530273,
          -6.9775896072387695,
          3.9699244499206543,
          1.5151407718658447,
          4.543593883514404,
          -4.996665954589844,
          -6.796648025512695,
          -0.796980082988739,
          -5.379490852355957,
          -0.549635648727417,
          -9.417557716369629,
          -2.632219076156616,
          1.7778784036636353,
          1.6346489191055298,
          -7.801143169403076,
          7.2114739418029785,
          -9.09225082397461,
          0.9202542901039124,
          -1.1646091938018799,
          2.6558477878570557,
          -5.547433853149414,
          1.9413443803787231,
          -3.9593071937561035,
          2.2311630249023438,
          1.97158944606781,
          6.333553791046143,
          -2.814406633377075,
          0.1673775613307953,
          -8.345366477966309,
          3.1074397563934326,
          -3.0089330673217773,
          -4.5951337814331055,
          -3.7368662357330322,
          -5.906049728393555,
          0.728748619556427,
          0.3020936846733093,
          -1.9193578958511353,
          -3.302121877670288,
          -2.283527135848999,
          1.0822992324829102,
          1.084127426147461,
          0.7212051153182983,
          -0.2904839813709259,
          0.0067567480728030205,
          -6.094676494598389,
          -0.402170330286026,
          0.21640515327453613,
          5.0021562576293945,
          -2.4156148433685303,
          4.747771739959717,
          3.1189804077148438,
          6.701613903045654,
          7.1602253913879395,
          6.287492275238037,
          0.8026138544082642,
          6.452852725982666,
          -2.5331742763519287,
          -4.900890827178955,
          2.9736883640289307,
          4.881409645080566,
          -1.4666893482208252,
          -4.851039409637451,
          2.059319257736206,
          2.250230073928833,
          3.969353437423706,
          3.364682912826538,
          -0.05929283797740936,
          -0.6684280037879944,
          5.829762935638428,
          -0.979670524597168,
          -3.566016912460327,
          -1.4442508220672607,
          -2.588890552520752,
          -3.3296337127685547,
          -2.7897987365722656,
          6.3759918212890625,
          1.5154504776000977,
          -8.110496520996094,
          0.17165987193584442,
          -6.858039855957031,
          -3.5445096492767334,
          2.5935659408569336,
          2.885282516479492,
          1.8959251642227173,
          -5.664699077606201,
          -3.4687421321868896,
          -2.4294581413269043,
          -7.189571857452393,
          -2.8525397777557373,
          5.089686870574951,
          0.6043485999107361,
          3.9126408100128174,
          -2.7547128200531006,
          -5.008999824523926,
          -3.657480001449585,
          -9.955078125,
          3.3863651752471924,
          -3.036243200302124,
          4.4467034339904785,
          -0.05079968646168709,
          -0.4907218813896179,
          -3.036567211151123,
          4.6397705078125,
          1.6668617725372314,
          -1.2682503461837769,
          1.3137140274047852,
          -6.264095783233643,
          4.439817905426025,
          -6.315651893615723,
          -5.666043281555176,
          -0.6006585955619812,
          5.461076259613037,
          -0.24642640352249146,
          -2.679663896560669,
          -3.422029495239258,
          -0.013748754747211933,
          4.076957702636719,
          6.95869255065918,
          2.3676674365997314,
          -8.981572151184082,
          -8.151212692260742,
          3.518017053604126,
          -0.759570300579071,
          -0.42632266879081726,
          7.180819034576416,
          1.6771918535232544,
          -3.5160317420959473,
          0.8269298076629639,
          1.8569263219833374,
          2.010329484939575,
          -3.7082324028015137,
          6.012582302093506,
          4.621157169342041,
          0.9195900559425354,
          -1.7041544914245605,
          3.4600207805633545,
          2.8068747520446777,
          -4.070095062255859,
          3.804140567779541,
          -3.074394702911377,
          -4.536877155303955,
          -1.7962968349456787,
          -2.7294981479644775,
          2.9486894607543945,
          -6.354874134063721,
          -8.988039016723633,
          8.563080787658691,
          6.317517280578613,
          -0.9279513359069824,
          -1.9361271858215332,
          -9.039132118225098,
          7.131534099578857,
          2.7934367656707764,
          -5.968392848968506,
          -0.06545160710811615,
          3.7102925777435303,
          2.258908987045288,
          4.861087799072266,
          -0.05108696222305298,
          -4.49846887588501,
          -4.576083660125732,
          -6.251190185546875,
          -3.3793859481811523,
          1.4788758754730225,
          4.066930294036865,
          -0.9180363416671753,
          5.740225315093994,
          -1.3265256881713867,
          5.262291431427002,
          0.7175686359405518,
          2.9360263347625732,
          -7.619258880615234,
          5.261261463165283,
          1.1680265665054321,
          -4.450006008148193,
          5.410119533538818,
          -3.2600202560424805,
          4.232682228088379,
          -6.257768154144287
         ],
         "y": [
          -2.366219997406006,
          1.4157464504241943,
          -8.381683349609375,
          -4.209931373596191,
          -5.979645252227783,
          -9.352097511291504,
          -1.8068146705627441,
          -7.144293785095215,
          6.021297931671143,
          6.609764575958252,
          -1.495269536972046,
          -0.639616072177887,
          -1.1818455457687378,
          -6.758322715759277,
          1.8673317432403564,
          1.815646767616272,
          0.16977642476558685,
          0.9171539545059204,
          -0.28289005160331726,
          -7.2281694412231445,
          0.7534630298614502,
          1.7798832654953003,
          2.778804302215576,
          3.790527105331421,
          -5.351175785064697,
          -3.560941457748413,
          4.08440637588501,
          5.833236217498779,
          -4.1663336753845215,
          0.3943403661251068,
          4.310480117797852,
          0.09831395000219345,
          0.24064360558986664,
          -1.9963525533676147,
          -1.881211519241333,
          4.180385589599609,
          -0.9039490222930908,
          -1.5471729040145874,
          0.5280646681785583,
          2.6742937564849854,
          4.0620808601379395,
          -6.513219356536865,
          -9.472671508789062,
          -1.1084809303283691,
          2.9618706703186035,
          4.339135646820068,
          -5.158017635345459,
          -5.029111862182617,
          4.206965923309326,
          2.9538791179656982,
          5.694890022277832,
          -0.4904268980026245,
          6.932773113250732,
          7.793333053588867,
          -4.360994338989258,
          4.613007545471191,
          2.7395997047424316,
          0.7967098355293274,
          3.114056348800659,
          -10.288593292236328,
          -1.2579964399337769,
          8.089855194091797,
          -2.3647656440734863,
          -3.7294106483459473,
          -2.29291033744812,
          2.8865668773651123,
          -3.3214125633239746,
          -7.906168460845947,
          -8.579545021057129,
          -7.941169261932373,
          -5.88237190246582,
          -2.719790458679199,
          -2.8582077026367188,
          -5.134355545043945,
          1.7824652194976807,
          6.741681098937988,
          5.792266845703125,
          -2.9503769874572754,
          -5.39525842666626,
          -5.694397926330566,
          5.918738842010498,
          -2.7563304901123047,
          -3.7066729068756104,
          -8.668044090270996,
          -5.506378650665283,
          -4.30855131149292,
          2.2824041843414307,
          0.5046464800834656,
          -5.011874675750732,
          -9.552413940429688,
          -7.896369457244873,
          0.7081783413887024,
          3.2146520614624023,
          -6.3050384521484375,
          -2.3505775928497314,
          -4.481626510620117,
          -1.7087410688400269,
          -4.868049621582031,
          -1.1532683372497559,
          0.6817308068275452,
          3.0158650875091553,
          1.4656099081039429,
          -7.470815658569336,
          -3.990213632583618,
          2.7786660194396973,
          -5.667232513427734,
          2.627997875213623,
          1.2886537313461304,
          4.768289566040039,
          -0.20373012125492096,
          -1.107700228691101,
          2.234194040298462,
          2.0606625080108643,
          -0.11353302001953125,
          -4.433785915374756,
          -0.016857877373695374,
          -6.968067646026611,
          -1.3603588342666626,
          -4.8725175857543945,
          -0.11705698072910309,
          -5.689452648162842,
          -3.5955538749694824,
          5.520202159881592,
          -1.0861376523971558,
          2.7821645736694336,
          -1.5905102491378784,
          -0.9805388450622559,
          -5.05893087387085,
          0.7785710096359253,
          4.528586387634277,
          -6.244285583496094,
          -7.922395706176758,
          1.2706235647201538,
          -4.036782741546631,
          -3.101402521133423,
          -8.064379692077637,
          -2.1249098777770996,
          -2.389819622039795,
          -3.1695380210876465,
          4.600878715515137,
          -1.2858613729476929,
          -9.436290740966797,
          -2.359637975692749,
          0.10975052416324615,
          3.9018781185150146,
          0.8384222984313965,
          -8.780622482299805,
          0.42526963353157043,
          1.231698751449585,
          -0.5188021063804626,
          -2.5711145401000977,
          -1.3426936864852905,
          -7.813931465148926,
          -3.1462206840515137,
          -2.4722371101379395,
          -1.0157849788665771,
          -4.328769207000732,
          6.737553596496582,
          -6.855597019195557,
          -2.030759572982788,
          -4.499425411224365,
          1.6545190811157227,
          6.392116546630859,
          -3.9923131465911865,
          3.8821256160736084,
          6.069917678833008,
          -0.25490161776542664,
          -0.4267646372318268,
          1.0929652452468872,
          3.285449504852295,
          1.4321503639221191,
          1.9575225114822388,
          8.271814346313477,
          1.9291417598724365,
          -1.3109941482543945,
          7.994966983795166,
          -5.386274814605713,
          4.301878452301025,
          2.89518141746521,
          6.410716533660889,
          6.38617467880249,
          -1.6168067455291748,
          6.841063976287842,
          0.05820506066083908,
          1.640087604522705,
          -2.8759775161743164,
          4.6040825843811035,
          -6.035190105438232,
          2.152789831161499,
          -3.5014779567718506,
          4.5739216804504395,
          -3.7116336822509766,
          0.06453826278448105,
          -5.643316268920898,
          -3.0912952423095703,
          -1.396384835243225,
          -2.5754761695861816,
          -3.2645411491394043,
          4.257584095001221,
          -6.593636989593506,
          -3.2896602153778076,
          -7.411569118499756,
          -1.3092409372329712,
          -6.6785125732421875,
          1.808240294456482,
          -0.3113737404346466,
          5.845041751861572,
          3.1287853717803955,
          -2.0365591049194336,
          -4.636128902435303,
          2.706270456314087,
          6.708128452301025,
          -9.077188491821289,
          -1.6567959785461426,
          -4.118268013000488,
          0.3064400553703308,
          -0.6685023903846741,
          0.6971976161003113,
          4.906248569488525,
          0.9191274642944336,
          -2.744675636291504,
          -4.471009731292725,
          4.209408283233643,
          3.0564539432525635,
          -7.672701358795166,
          -2.7060420513153076,
          -7.385237693786621,
          -2.326617479324341,
          -1.1584327220916748
         ],
         "type": "scatter"
        }
       ],
       "layout": {
        "template": {
         "data": {
          "histogram2dcontour": [
           {
            "type": "histogram2dcontour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "choropleth": [
           {
            "type": "choropleth",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "histogram2d": [
           {
            "type": "histogram2d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmap": [
           {
            "type": "heatmap",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmapgl": [
           {
            "type": "heatmapgl",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "contourcarpet": [
           {
            "type": "contourcarpet",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "contour": [
           {
            "type": "contour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "surface": [
           {
            "type": "surface",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "mesh3d": [
           {
            "type": "mesh3d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "parcoords": [
           {
            "type": "parcoords",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolargl": [
           {
            "type": "scatterpolargl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "scattergeo": [
           {
            "type": "scattergeo",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolar": [
           {
            "type": "scatterpolar",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "scattergl": [
           {
            "type": "scattergl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatter3d": [
           {
            "type": "scatter3d",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermapbox": [
           {
            "type": "scattermapbox",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterternary": [
           {
            "type": "scatterternary",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattercarpet": [
           {
            "type": "scattercarpet",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ]
         },
         "layout": {
          "autotypenumbers": "strict",
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "hovermode": "closest",
          "hoverlabel": {
           "align": "left"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "bgcolor": "#E5ECF6",
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "ternary": {
           "bgcolor": "#E5ECF6",
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "sequential": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ]
          },
          "xaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "automargin": true,
           "zerolinewidth": 2
          },
          "yaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "automargin": true,
           "zerolinewidth": 2
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "geo": {
           "bgcolor": "white",
           "landcolor": "#E5ECF6",
           "subunitcolor": "white",
           "showland": true,
           "showlakes": true,
           "lakecolor": "white"
          },
          "title": {
           "x": 0.05
          },
          "mapbox": {
           "style": "light"
          }
         }
        },
        "title": {
         "text": "Word2Vec - Visualising embeddings with TSNE"
        }
       },
       "config": {
        "plotlyServerURL": "https://plot.ly"
       }
      },
      "text/html": "<div>                            <div id=\"5c39eac1-352f-4e2a-af06-888c12c0a8bd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5c39eac1-352f-4e2a-af06-888c12c0a8bd\")) {                    Plotly.newPlot(                        \"5c39eac1-352f-4e2a-af06-888c12c0a8bd\",                        [{\"mode\":\"markers\",\"text\":[\"cars\",\"ai\",\"car\",\"vehicles\",\"systems\",\"safety\",\"driving\",\"providing\",\"personalized\",\"like\",\"enabling\",\"alternatives\",\"people\",\"performance\",\"improved\",\"human\",\"process\",\"understand\",\"machines\",\"allows\",\"language\",\"powered\",\"autonomous\",\"way\",\"convenient\",\"enables\",\"efficient\",\"transforming\",\"transportation\",\"computers\",\"without\",\"make\",\"decisions\",\"advancements\",\"diagnosis\",\"provide\",\"disease\",\"including\",\"healthcare\",\"customer\",\"faster\",\"discovery\",\"experiences\",\"interactions\",\"based\",\"voice\",\"support\",\"customers\",\"drug\",\"chatbots\",\"medicine\",\"leading\",\"informed\",\"surroundings\",\"perceive\",\"self\",\"role\",\"crucial\",\"plays\",\"increasingly\",\"field\",\"outcomes\",\"industry\",\"utilizing\",\"patient\",\"service\",\"automated\",\"common\",\"efficiency\",\"becoming\",\"valuable\",\"anomalies\",\"detect\",\"patterns\",\"uncover\",\"data\",\"amounts\",\"vast\",\"analyze\",\"algorithms\",\"programmed\",\"explicitly\",\"predictions\",\"learn\",\"subset\",\"learning\",\"machine\",\"work\",\"live\",\"industries\",\"revolutionizing\",\"intelligence\",\"extract\",\"insights\",\"assistant\",\"natural\",\"google\",\"alexa\",\"siri\",\"assistants\",\"virtual\",\"considerations\",\"detection\",\"object\",\"recognition\",\"image\",\"applications\",\"information\",\"visual\",\"interpret\",\"vision\",\"computer\",\"humans\",\"communication\",\"facilitating\",\"nlp\",\"processing\",\"ethical\",\"essential\",\"transparency\",\"agility\",\"nostalgic\",\"designs\",\"timeless\",\"history\",\"automotive\",\"place\",\"special\",\"hold\",\"classic\",\"experience\",\"thrilling\",\"speed\",\"capturing\",\"offering\",\"capabilities\",\"high\",\"known\",\"sports\",\"assist\",\"keeping\",\"lane\",\"control\",\"cruise\",\"adaptive\",\"appeal\",\"hearts\",\"fairness\",\"hailing\",\"promising\",\"intervention\",\"operate\",\"navigate\",\"bring\",\"expected\",\"future\",\"ownership\",\"access\",\"transformed\",\"apps\",\"ride\",\"enthusiasts\",\"services\",\"sharing\",\"upgrades\",\"modifications\",\"body\",\"jobs\",\"paint\",\"unique\",\"personalize\",\"owners\",\"customization\",\"technologies\",\"incorporating\",\"enhance\",\"world\",\"functional\",\"create\",\"components\",\"various\",\"assembling\",\"engineering\",\"designing\",\"complex\",\"involves\",\"manufacturing\",\"automobile\",\"around\",\"adas\",\"travel\",\"means\",\"revolutionized\",\"use\",\"accountable\",\"responsible\",\"ensure\",\"deployment\",\"development\",\"increased\",\"privacy\",\"vehicle\",\"features\",\"seat\",\"belts\",\"assistance\",\"driver\",\"advanced\",\"fuels\",\"fossil\",\"dependence\",\"emissions\",\"carbon\",\"reducing\",\"gasoline\",\"traditional\",\"friendly\",\"eco\",\"popularity\",\"gaining\",\"evs\",\"electric\",\"overall\",\"greatly\",\"braking\",\"lock\",\"anti\",\"airbags\",\"artificial\"],\"x\":[-3.5569868087768555,-3.345669984817505,-1.1631700992584229,-4.823192596435547,1.4799708127975464,-2.5234670639038086,3.3336856365203857,4.033675193786621,-1.097055435180664,1.8911492824554443,-4.095557689666748,-6.807097434997559,3.94748592376709,1.8565915822982788,-6.009923458099365,0.7721247673034668,-2.0102412700653076,-7.494566917419434,-5.4318718910217285,4.244304180145264,1.411681890487671,5.025524139404297,-3.7039387226104736,-8.411649703979492,-0.7734807133674622,0.2362508475780487,2.539975643157959,-1.1920018196105957,7.879120826721191,-4.905570030212402,4.294675827026367,7.35902738571167,-6.065314292907715,5.191819190979004,-5.435687065124512,0.6472213864326477,8.186429023742676,3.4760055541992188,6.416111946105957,3.8564202785491943,0.012898512184619904,-2.008552312850952,-3.6230485439300537,-4.542641639709473,7.342559814453125,-0.9007460474967957,-5.753945827484131,-2.073754072189331,-1.8793163299560547,-1.26099693775177,-5.374608993530273,-6.9775896072387695,3.9699244499206543,1.5151407718658447,4.543593883514404,-4.996665954589844,-6.796648025512695,-0.796980082988739,-5.379490852355957,-0.549635648727417,-9.417557716369629,-2.632219076156616,1.7778784036636353,1.6346489191055298,-7.801143169403076,7.2114739418029785,-9.09225082397461,0.9202542901039124,-1.1646091938018799,2.6558477878570557,-5.547433853149414,1.9413443803787231,-3.9593071937561035,2.2311630249023438,1.97158944606781,6.333553791046143,-2.814406633377075,0.1673775613307953,-8.345366477966309,3.1074397563934326,-3.0089330673217773,-4.5951337814331055,-3.7368662357330322,-5.906049728393555,0.728748619556427,0.3020936846733093,-1.9193578958511353,-3.302121877670288,-2.283527135848999,1.0822992324829102,1.084127426147461,0.7212051153182983,-0.2904839813709259,0.0067567480728030205,-6.094676494598389,-0.402170330286026,0.21640515327453613,5.0021562576293945,-2.4156148433685303,4.747771739959717,3.1189804077148438,6.701613903045654,7.1602253913879395,6.287492275238037,0.8026138544082642,6.452852725982666,-2.5331742763519287,-4.900890827178955,2.9736883640289307,4.881409645080566,-1.4666893482208252,-4.851039409637451,2.059319257736206,2.250230073928833,3.969353437423706,3.364682912826538,-0.05929283797740936,-0.6684280037879944,5.829762935638428,-0.979670524597168,-3.566016912460327,-1.4442508220672607,-2.588890552520752,-3.3296337127685547,-2.7897987365722656,6.3759918212890625,1.5154504776000977,-8.110496520996094,0.17165987193584442,-6.858039855957031,-3.5445096492767334,2.5935659408569336,2.885282516479492,1.8959251642227173,-5.664699077606201,-3.4687421321868896,-2.4294581413269043,-7.189571857452393,-2.8525397777557373,5.089686870574951,0.6043485999107361,3.9126408100128174,-2.7547128200531006,-5.008999824523926,-3.657480001449585,-9.955078125,3.3863651752471924,-3.036243200302124,4.4467034339904785,-0.05079968646168709,-0.4907218813896179,-3.036567211151123,4.6397705078125,1.6668617725372314,-1.2682503461837769,1.3137140274047852,-6.264095783233643,4.439817905426025,-6.315651893615723,-5.666043281555176,-0.6006585955619812,5.461076259613037,-0.24642640352249146,-2.679663896560669,-3.422029495239258,-0.013748754747211933,4.076957702636719,6.95869255065918,2.3676674365997314,-8.981572151184082,-8.151212692260742,3.518017053604126,-0.759570300579071,-0.42632266879081726,7.180819034576416,1.6771918535232544,-3.5160317420959473,0.8269298076629639,1.8569263219833374,2.010329484939575,-3.7082324028015137,6.012582302093506,4.621157169342041,0.9195900559425354,-1.7041544914245605,3.4600207805633545,2.8068747520446777,-4.070095062255859,3.804140567779541,-3.074394702911377,-4.536877155303955,-1.7962968349456787,-2.7294981479644775,2.9486894607543945,-6.354874134063721,-8.988039016723633,8.563080787658691,6.317517280578613,-0.9279513359069824,-1.9361271858215332,-9.039132118225098,7.131534099578857,2.7934367656707764,-5.968392848968506,-0.06545160710811615,3.7102925777435303,2.258908987045288,4.861087799072266,-0.05108696222305298,-4.49846887588501,-4.576083660125732,-6.251190185546875,-3.3793859481811523,1.4788758754730225,4.066930294036865,-0.9180363416671753,5.740225315093994,-1.3265256881713867,5.262291431427002,0.7175686359405518,2.9360263347625732,-7.619258880615234,5.261261463165283,1.1680265665054321,-4.450006008148193,5.410119533538818,-3.2600202560424805,4.232682228088379,-6.257768154144287],\"y\":[-2.366219997406006,1.4157464504241943,-8.381683349609375,-4.209931373596191,-5.979645252227783,-9.352097511291504,-1.8068146705627441,-7.144293785095215,6.021297931671143,6.609764575958252,-1.495269536972046,-0.639616072177887,-1.1818455457687378,-6.758322715759277,1.8673317432403564,1.815646767616272,0.16977642476558685,0.9171539545059204,-0.28289005160331726,-7.2281694412231445,0.7534630298614502,1.7798832654953003,2.778804302215576,3.790527105331421,-5.351175785064697,-3.560941457748413,4.08440637588501,5.833236217498779,-4.1663336753845215,0.3943403661251068,4.310480117797852,0.09831395000219345,0.24064360558986664,-1.9963525533676147,-1.881211519241333,4.180385589599609,-0.9039490222930908,-1.5471729040145874,0.5280646681785583,2.6742937564849854,4.0620808601379395,-6.513219356536865,-9.472671508789062,-1.1084809303283691,2.9618706703186035,4.339135646820068,-5.158017635345459,-5.029111862182617,4.206965923309326,2.9538791179656982,5.694890022277832,-0.4904268980026245,6.932773113250732,7.793333053588867,-4.360994338989258,4.613007545471191,2.7395997047424316,0.7967098355293274,3.114056348800659,-10.288593292236328,-1.2579964399337769,8.089855194091797,-2.3647656440734863,-3.7294106483459473,-2.29291033744812,2.8865668773651123,-3.3214125633239746,-7.906168460845947,-8.579545021057129,-7.941169261932373,-5.88237190246582,-2.719790458679199,-2.8582077026367188,-5.134355545043945,1.7824652194976807,6.741681098937988,5.792266845703125,-2.9503769874572754,-5.39525842666626,-5.694397926330566,5.918738842010498,-2.7563304901123047,-3.7066729068756104,-8.668044090270996,-5.506378650665283,-4.30855131149292,2.2824041843414307,0.5046464800834656,-5.011874675750732,-9.552413940429688,-7.896369457244873,0.7081783413887024,3.2146520614624023,-6.3050384521484375,-2.3505775928497314,-4.481626510620117,-1.7087410688400269,-4.868049621582031,-1.1532683372497559,0.6817308068275452,3.0158650875091553,1.4656099081039429,-7.470815658569336,-3.990213632583618,2.7786660194396973,-5.667232513427734,2.627997875213623,1.2886537313461304,4.768289566040039,-0.20373012125492096,-1.107700228691101,2.234194040298462,2.0606625080108643,-0.11353302001953125,-4.433785915374756,-0.016857877373695374,-6.968067646026611,-1.3603588342666626,-4.8725175857543945,-0.11705698072910309,-5.689452648162842,-3.5955538749694824,5.520202159881592,-1.0861376523971558,2.7821645736694336,-1.5905102491378784,-0.9805388450622559,-5.05893087387085,0.7785710096359253,4.528586387634277,-6.244285583496094,-7.922395706176758,1.2706235647201538,-4.036782741546631,-3.101402521133423,-8.064379692077637,-2.1249098777770996,-2.389819622039795,-3.1695380210876465,4.600878715515137,-1.2858613729476929,-9.436290740966797,-2.359637975692749,0.10975052416324615,3.9018781185150146,0.8384222984313965,-8.780622482299805,0.42526963353157043,1.231698751449585,-0.5188021063804626,-2.5711145401000977,-1.3426936864852905,-7.813931465148926,-3.1462206840515137,-2.4722371101379395,-1.0157849788665771,-4.328769207000732,6.737553596496582,-6.855597019195557,-2.030759572982788,-4.499425411224365,1.6545190811157227,6.392116546630859,-3.9923131465911865,3.8821256160736084,6.069917678833008,-0.25490161776542664,-0.4267646372318268,1.0929652452468872,3.285449504852295,1.4321503639221191,1.9575225114822388,8.271814346313477,1.9291417598724365,-1.3109941482543945,7.994966983795166,-5.386274814605713,4.301878452301025,2.89518141746521,6.410716533660889,6.38617467880249,-1.6168067455291748,6.841063976287842,0.05820506066083908,1.640087604522705,-2.8759775161743164,4.6040825843811035,-6.035190105438232,2.152789831161499,-3.5014779567718506,4.5739216804504395,-3.7116336822509766,0.06453826278448105,-5.643316268920898,-3.0912952423095703,-1.396384835243225,-2.5754761695861816,-3.2645411491394043,4.257584095001221,-6.593636989593506,-3.2896602153778076,-7.411569118499756,-1.3092409372329712,-6.6785125732421875,1.808240294456482,-0.3113737404346466,5.845041751861572,3.1287853717803955,-2.0365591049194336,-4.636128902435303,2.706270456314087,6.708128452301025,-9.077188491821289,-1.6567959785461426,-4.118268013000488,0.3064400553703308,-0.6685023903846741,0.6971976161003113,4.906248569488525,0.9191274642944336,-2.744675636291504,-4.471009731292725,4.209408283233643,3.0564539432525635,-7.672701358795166,-2.7060420513153076,-7.385237693786621,-2.326617479324341,-1.1584327220916748],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Word2Vec - Visualising embeddings with TSNE\"}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('5c39eac1-352f-4e2a-af06-888c12c0a8bd');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "def reduce_dimensions(model):\n",
    "    num_components = 2  # number of dimensions to keep after compression\n",
    "\n",
    "    # extract vocabulary from model and vectors in order to associate them in the graph\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)\n",
    "\n",
    "    # apply TSNE\n",
    "    tsne = TSNE(n_components=num_components, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "def plot_embeddings(x_vals, y_vals, labels):\n",
    "    import plotly.graph_objs as go\n",
    "    fig = go.Figure()\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='markers', text=labels)\n",
    "    fig.add_trace(trace)\n",
    "    fig.update_layout(title=\"Word2Vec - Visualising embeddings with TSNE\")\n",
    "    fig.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "\n",
    "plot = plot_embeddings(x_vals, y_vals, labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
