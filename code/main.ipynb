{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GPT-3.5-Turbo Model\n",
    "Creating a question answering chatbot using GPT-3.5. Adapted from: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preamble\n",
    "import PyPDF2 # For parsing PDF documents\n",
    "import ast  # covert embeddings saved as strings back to arrays\n",
    "import openai  # OpenAI API\n",
    "import pandas as pd  # for storing text and embeddings data\n",
    "import numpy as np # for df manipulations\n",
    "import tiktoken  # for counting tokens\n",
    "from scipy import spatial  # for calculating vector similarities for search\n",
    "import wikipedia # For sourcing Wikipedia article text\n",
    "import re  # for cutting <ref> links out of Wikipedia articles\n",
    "import mwparserfromhell  # for splitting Wikipedia articles into sections\n",
    "from copy import deepcopy # for copying dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Config\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "SAVE_PATH = \"assets/computer_vision_wiki_embeddings.csv\"\n",
    "ENCODING = tiktoken.encoding_for_model(GPT_MODEL)\n",
    "BATCH_SIZE = 1000  # you can submit up to 2048 embedding inputs per request\n",
    "MAX_TOKENS = 1600\n",
    "ANSWER_NOT_FOUND_MSG = \"I could not find an answer in the text I\\'ve been provided, sorry! Please try again.\"\n",
    "WIKI_PAGE = \"Computer vision\"\n",
    "SECTIONS_TO_IGNORE = [\n",
    "    \"See also\",\n",
    "    \"References\",\n",
    "    \"External links\",\n",
    "    \"Further reading\",\n",
    "    \"Footnotes\",\n",
    "    \"Bibliography\",\n",
    "    \"Sources\",\n",
    "    \"Citations\",\n",
    "    \"Literature\",\n",
    "    \"Footnotes\",\n",
    "    \"Notes and references\",\n",
    "    \"Photo gallery\",\n",
    "    \"Works cited\",\n",
    "    \"Photos\",\n",
    "    \"Gallery\",\n",
    "    \"Notes\",\n",
    "    \"References and sources\",\n",
    "    \"References and notes\",\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "2.Veryfewvisual taskscanbesuccessfully performed inapurely data-driv en\n",
      "way(\\bottom-up\" image analysis). Consider thenextimage example:\n",
      "aged bytheirtextured backgrounds; thefoxes\n",
      "occlude eachother; theyappearinseveraldi\u000Beren tposesandperspective\n",
      "angles; etc.Howcanthere possibly existmathematical operators forsuch\n",
      "animage thatcan:\n",
      "\u000Fperform the\fgure-ground segmen tation ofthescene (intoitsobjects\n",
      "andbackground)\n",
      "\u000Finferthe3Darrangemen tsofobjectsfromtheirmutual occlusions\n",
      "\u000Finfersurface properties (texture, colour) fromthe2Dimage statistics\n",
      "\u000Finfervolumetric objectproperties fromtheir2Dimage projections\n",
      "\u000Fanddoallofthisin\\real time?\" (This matters quite alotinthe\n",
      "natural world\\redintoothandclaw,\"sincesurviv aldependsonit.)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# creating a pdf reader instance\n",
    "reader = PyPDF2.PdfReader('assets/online_notes.pdf')\n",
    "\n",
    "# print the number of pages in pdf file\n",
    "print(len(reader.pages))\n",
    "\n",
    "# print the text of the first page\n",
    "print(reader.pages[5].extract_text())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Used throughout\n",
    "def num_tokens(\n",
    "        text: str,\n",
    "        encoding: tiktoken.encoding_for_model = tiktoken.encoding_for_model(GPT_MODEL)\n",
    ") -> int:\n",
    "    \"\"\"Returns the number of tokens in a string.\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def get_embedding(content: str, model: str = EMBEDDING_MODEL):\n",
    "    return openai.Embedding.create(input=content, model=model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class Knowledge:\n",
    "    def __init__(self, topic):\n",
    "        self.topic: str = topic\n",
    "        self.df: pd.DataFrame = self.get_blank_knowledge_df() # need to add code to remove small sections (<16 chars?)\n",
    "        self.max_tokens: int = 500# MAX_TOKENS # max number of tokens per section\n",
    "\n",
    "    def get_blank_knowledge_df(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame(columns=['Source', 'Heading', 'Subheading', 'Content'])\n",
    "\n",
    "    def extract_wiki_sections(self,\n",
    "                              page_name: str,\n",
    "                              content: mwparserfromhell.wikicode.Wikicode,\n",
    "                              sections_to_ignore: list = SECTIONS_TO_IGNORE\n",
    "                              ) -> pd.DataFrame:\n",
    "        \"\"\"Creates a df of sections by extracting section content from a Wikicode\"\"\"\n",
    "\n",
    "        knowledge = self.get_blank_knowledge_df()\n",
    "        for section in content.get_sections(levels=[2]):\n",
    "            section_headings = section.filter_headings()\n",
    "            section_header = str(section_headings[0])\n",
    "            if len(section_headings)==1:# therefore a section title, not a subsection\n",
    "                section = section.strip(section_header)\n",
    "                if section_header.strip(\"=\" + \" \") not in sections_to_ignore: # append to df\n",
    "                    new_row = {'Source': f'Wikipedia ({page_name})', 'Heading': section_header.strip(\"=\" + \" \"), 'Content': section}\n",
    "                    knowledge = pd.concat([knowledge, pd.DataFrame.from_records([new_row])])\n",
    "            elif len(section_headings)>1 and section_header.strip(\"=\" + \" \") not in sections_to_ignore: # therefore subsections\n",
    "                for subsection in section.get_sections(levels=[3]):\n",
    "                    subsection_sections = subsection.get_sections(levels=[3])[0]\n",
    "                    subsection_headings = subsection_sections.filter_headings()\n",
    "                    subsection_header = str(subsection_headings[0])\n",
    "                    subsection = subsection.strip(subsection_header)\n",
    "                    if subsection_header.strip(\"=\" + \" \") not in sections_to_ignore: # append to df\n",
    "                        new_row = {'Source': f'Wikipedia ({page_name})', 'Heading': section_header.strip(\"=\" + \" \"), 'Subheading': subsection_header.strip(\"=\" + \" \"), 'Content': subsection}\n",
    "                        knowledge = pd.concat([knowledge, pd.DataFrame.from_records([new_row])])\n",
    "        return knowledge\n",
    "\n",
    "    def generate_source_column(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Creates a new column in the df which contains a summary of the source location\"\"\"\n",
    "\n",
    "        df.fillna('', inplace=True)\n",
    "        df['Section'] = df['Source'] + '->' + df['Heading'] + '->' + df['Subheading']\n",
    "        df['Section'] = df['Section'].str.replace('->->', '')\n",
    "        df['Section'] = df['Section'].str.rstrip('_->')\n",
    "        return df\n",
    "\n",
    "    def clean_section_contents(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Returns a cleaned up section with <ref>xyz</ref> patterns and leading/trailing whitespace removed\"\"\"\n",
    "\n",
    "        # text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n",
    "        df['Content'] = df['Content'].str.replace(r\"<ref.*?</ref>\", \"\", regex=True)\n",
    "        df['Content'] = df['Content'].str.strip() # removes whitespace\n",
    "        df['Content'] = '\\n' + df['Content'] # need to add the \\n back to the start of each title\n",
    "        return df\n",
    "\n",
    "    def merge_elements_of_list(self, list_of_strings: list, delimiter: str = \"\\n\"):\n",
    "        potential_for_more_merging = False\n",
    "        merged_list = []\n",
    "        skip_item = False\n",
    "        for i in range(len(list_of_strings)):\n",
    "            if not skip_item:\n",
    "                if i == len(list_of_strings)-1:\n",
    "                    merged_list.append(list_of_strings[i])\n",
    "                else:\n",
    "                    merged_strings = list_of_strings[i] + delimiter + list_of_strings[i+1]\n",
    "                    if num_tokens(merged_strings)<self.max_tokens:\n",
    "                        merged_list.append(merged_strings)\n",
    "                        skip_item = True # make it skip the element we just merged\n",
    "                        potential_for_more_merging = True\n",
    "                    else:\n",
    "                        merged_list.append(list_of_strings[i])\n",
    "            else:\n",
    "                skip_item = False # set the default back to False unless otherwise specified\n",
    "        return merged_list, potential_for_more_merging\n",
    "\n",
    "    def split_long_sections(self, df: pd.DataFrame, delimiter: str = \"\\n\"):\n",
    "        \"\"\"Splits long sections of text into smaller ones\"\"\"\n",
    "\n",
    "        df_as_dict = df.to_dict('records')\n",
    "        for section in df_as_dict:\n",
    "            if section['Tokens']>self.max_tokens:\n",
    "                # needs to be split up\n",
    "                text = section['Content'].split('\\n')\n",
    "                text.remove('')\n",
    "                potential_for_more_merging = True\n",
    "                i = 0\n",
    "                while potential_for_more_merging:\n",
    "                    if i>20:\n",
    "                        break\n",
    "                    else:\n",
    "                        text, potential_for_more_merging = self.merge_elements_of_list(text)\n",
    "\n",
    "                # The sections should be merged into acceptable sizes:\n",
    "                if len(text)>1:\n",
    "                    df_as_dict.remove(section)\n",
    "                    for string in text:\n",
    "                        item_to_append = {'Source': section['Source'], 'Heading': section['Heading'], 'Subheading': section['Subheading'], 'Content': string, 'Section': section['Section'], 'Tokens': num_tokens(string)}\n",
    "\n",
    "                        df_as_dict.append(item_to_append)\n",
    "                else:\n",
    "                    pass # we shouldn't have this because the text should be more than the acceptable number of tokens\n",
    "        return pd.DataFrame(df_as_dict)\n",
    "\n",
    "    def split_by_delimiter(self, df: pd.DataFrame, delimiter: str = \"\\n\") -> list[str, str]:\n",
    "        \"\"\"Split a string using a delimiter\"\"\"\n",
    "\n",
    "        # chunks = string.split(delimiter)\n",
    "        pass\n",
    "\n",
    "    def append_wikipedia_page(self, page_name: str, sections_to_ignore: list = SECTIONS_TO_IGNORE):\n",
    "        \"\"\"Takes a wikipedia page and appends the sections to the knowledge df\"\"\"\n",
    "\n",
    "        site = wikipedia.page(page_name, auto_suggest=False)\n",
    "        text = site.content\n",
    "        parsed_text = mwparserfromhell.parse(text)\n",
    "\n",
    "        # Creating initial df and appending the introduction paragraph (the text up to the first heading)\n",
    "        intro = str(parsed_text).split(str(parsed_text.filter_headings()[0]))[0]\n",
    "        knowledge = self.get_blank_knowledge_df()\n",
    "        new_row = {'Source': f'Wikipedia ({page_name})', 'Content': '\\n'+intro}\n",
    "        knowledge = pd.concat([knowledge, pd.DataFrame.from_records([new_row])])\n",
    "\n",
    "        section_content = self.extract_wiki_sections(page_name=page_name, content=parsed_text, sections_to_ignore=sections_to_ignore)\n",
    "        knowledge = pd.concat([knowledge, section_content])\n",
    "\n",
    "        # Generate succinct heading information\n",
    "        knowledge = self.generate_source_column(knowledge)\n",
    "        self.df = pd.concat([self.df, knowledge])\n",
    "\n",
    "        # Remove unwanted strings and whitespace\n",
    "        self.df = self.clean_section_contents(self.df)\n",
    "\n",
    "        # Generate number of tokens in each section\n",
    "        self.df['Tokens'] = self.df[\"Content\"].apply(lambda x: num_tokens(x))\n",
    "\n",
    "        # Split long sections\n",
    "        self.df = self.split_long_sections(self.df)\n",
    "\n",
    "    def export_to_csv(self, path):\n",
    "        \"\"\"Saves the knowledge df to a CSV file\"\"\"\n",
    "\n",
    "        self.df.to_csv(path, index=False)\n",
    "\n",
    "CompVisionKnowledge = Knowledge(WIKI_PAGE)\n",
    "CompVisionKnowledge.append_wikipedia_page(WIKI_PAGE)\n",
    "CompVisionKnowledge.df # I now need to force split if the content is too long and there's no delimiter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wikipedia_sections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [44]\u001B[0m, in \u001B[0;36m<cell line: 87>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     85\u001B[0m MAX_TOKENS \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1600\u001B[39m\n\u001B[0;32m     86\u001B[0m wikipedia_strings \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 87\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m section \u001B[38;5;129;01min\u001B[39;00m \u001B[43mwikipedia_sections\u001B[49m:\n\u001B[0;32m     88\u001B[0m     wikipedia_strings\u001B[38;5;241m.\u001B[39mextend(split_strings_from_subsection(section, max_tokens\u001B[38;5;241m=\u001B[39mMAX_TOKENS))\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(wikipedia_sections)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Wikipedia sections split into \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(wikipedia_strings)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m strings.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'wikipedia_sections' is not defined"
     ]
    }
   ],
   "source": [
    "# recursively split long sections into smaller sections.\n",
    "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
    "    \"\"\"Split a string in two, on a delimiter, trying to balance tokens on each side.\"\"\"\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]  # no delimiter found\n",
    "    elif len(chunks) == 2:\n",
    "        return chunks  # no need to search for halfway point\n",
    "    else:\n",
    "        total_tokens = num_tokens(string)\n",
    "        halfway = total_tokens // 2\n",
    "        best_diff = halfway\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            left = delimiter.join(chunks[: i + 1])\n",
    "            left_tokens = num_tokens(left)\n",
    "            diff = abs(halfway - left_tokens)\n",
    "            if diff >= best_diff:\n",
    "                break\n",
    "            else:\n",
    "                best_diff = diff\n",
    "        left = delimiter.join(chunks[:i])\n",
    "        right = delimiter.join(chunks[i:])\n",
    "        return [left, right]\n",
    "\n",
    "\n",
    "def truncated_string(\n",
    "    string: str,\n",
    "    model: str,\n",
    "    max_tokens: int,\n",
    "    print_warning: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"Truncate a string to a maximum number of tokens.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
    "    return truncated_string\n",
    "\n",
    "\n",
    "def split_strings_from_subsection(\n",
    "    subsection: tuple[list[str], str],\n",
    "    max_tokens: int = 1000,\n",
    "    model: str = GPT_MODEL,\n",
    "    max_recursion: int = 5,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Split a subsection into a list of subsections, each with no more than max_tokens.\n",
    "    Each subsection is a tuple of parent titles [H1, H2, ...] and text (str).\n",
    "    \"\"\"\n",
    "    titles, text = subsection\n",
    "    string = \"\\n\\n\".join(titles + [text])\n",
    "    num_tokens_in_string = num_tokens(string)\n",
    "    # if length is fine, return string\n",
    "    if num_tokens_in_string <= max_tokens:\n",
    "        return [string]\n",
    "    # if recursion hasn't found a split after X iterations, just truncate\n",
    "    elif max_recursion == 0:\n",
    "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "    # otherwise, split in half and recurse\n",
    "    else:\n",
    "        titles, text = subsection\n",
    "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
    "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
    "            if left == \"\" or right == \"\":\n",
    "                # if either half is empty, retry with a more fine-grained delimiter\n",
    "                continue\n",
    "            else:\n",
    "                # recurse on each half\n",
    "                results = []\n",
    "                for half in [left, right]:\n",
    "                    half_subsection = (titles, half)\n",
    "                    half_strings = split_strings_from_subsection(\n",
    "                        half_subsection,\n",
    "                        max_tokens=max_tokens,\n",
    "                        model=model,\n",
    "                        max_recursion=max_recursion - 1,\n",
    "                    )\n",
    "                    results.extend(half_strings)\n",
    "                return results\n",
    "    # otherwise no split was found, so just truncate (should be very rare)\n",
    "    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "\n",
    "# split sections into chunks\n",
    "MAX_TOKENS = 1600\n",
    "wikipedia_strings = []\n",
    "for section in wikipedia_sections:\n",
    "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
    "\n",
    "print(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer vision\n",
      "\n",
      "== Definition ==\n",
      "\n",
      "Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. \"Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\" As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.\n"
     ]
    }
   ],
   "source": [
    "# print example data\n",
    "print(wikipedia_strings[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 to 999\n"
     ]
    }
   ],
   "source": [
    "# Now that we've split our library into shorter self-contained strings, we can compute embeddings for each.\n",
    "# calculate embeddings\n",
    "\n",
    "embeddings = []\n",
    "for batch_start in range(0, len(wikipedia_strings), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = wikipedia_strings[batch_start:batch_end]\n",
    "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
    "    response = get_embedding(batch)\n",
    "    for i, be in enumerate(response[\"data\"]):\n",
    "        assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
    "    batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "df = pd.DataFrame({\"text\": wikipedia_strings, \"embedding\": embeddings})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "# save document chunks and embeddings\n",
    "df.to_csv(SAVE_PATH, index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Search\n",
    "Now we'll define a search function that:\n",
    "\n",
    "Takes a user query and a dataframe with text & embedding columns\n",
    "Embeds the user query with the OpenAI API\n",
    "Uses distance between query embedding and text embeddings to rank the texts\n",
    "Returns two lists:\n",
    "The top N texts, ranked by relevance\n",
    "Their corresponding relevance scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    def __init__(self, chatbot_topic:str, knowledge_path: str):\n",
    "        self.knowledge = None\n",
    "        self.load_data(knowledge_path)\n",
    "        self.chatbot_topic = chatbot_topic\n",
    "\n",
    "    def load_data(self, path: str):\n",
    "        \"\"\"Loads the knowledge df, appends a prefix, and calculates the number of tokens per section of knowledge\"\"\"\n",
    "\n",
    "        # load data from csv\n",
    "        self.knowledge = pd.read_csv(path)\n",
    "        # convert embeddings from CSV str type back to list type\n",
    "        self.knowledge['embedding'] = self.knowledge['embedding'].apply(ast.literal_eval) # is this needed?\n",
    "\n",
    "        # Format the knowledge df by adding section prefix and token sizes\n",
    "        self.knowledge['text'] = 'Article section:\\n\\n' + self.knowledge['text']\n",
    "        self.knowledge['tokens'] = self.knowledge[\"text\"].apply(lambda x: num_tokens(x))\n",
    "        self.knowledge['source'] = 'Wikipedia'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer vision began at universities that were pioneering artificial intelligence in the late 1960s.\n",
      "\n",
      "To construct this answer, I used the following documents: \n",
      "\n",
      "1. Wikipedia (Specifically: Computer vision\n",
      "\n",
      "== History ==\n",
      "\n",
      "In the late 1960s, computer vision began at univer...)\n",
      "\n",
      "2. Wikipedia (Specifically: Computer vision\n",
      "\n",
      "== Related fields ==\n",
      "\n",
      "=== Neurobiology ===\n",
      "\n",
      "Neurobiology has grea...)\n",
      "\n",
      "3. Wikipedia (Specifically: Computer vision\n",
      "\n",
      "== Related fields ==\n",
      "\n",
      "=== Solid-state physics ===\n",
      "\n",
      "Solid-state ph...)\n",
      "\n",
      "4. Wikipedia (Specifically: Computer vision\n",
      "\n",
      "== Related fields ==\n",
      "\n",
      "=== Other fields ===\n",
      "\n",
      "Besides the above-men...)\n",
      "\n",
      "5. Wikipedia (Specifically: Computer vision\n",
      "\n",
      "== Hardware ==\n",
      "\n",
      "There are many kinds of computer vision systems; ...)\n",
      "\n",
      "Total tokens used: 1612\n"
     ]
    }
   ],
   "source": [
    "class Query:\n",
    "    def __init__(self, query_text: str, chatbot_instance: ChatBot):\n",
    "        self.content: str = query_text\n",
    "        self.model: str = GPT_MODEL\n",
    "        self.knowledge: pd.DataFrame = chatbot_instance.knowledge\n",
    "        self.token_limit: int = 4096 - 500 # Allows 500 for the response\n",
    "        self.gpt_message = None\n",
    "        self.knowledge_used = None\n",
    "\n",
    "    # calculate similarity score\n",
    "    @staticmethod\n",
    "    def similarity(query_embedding: list,\n",
    "                   knowledge_embedding: list\n",
    "    ) -> float:\n",
    "        \"\"\"Calculates the cosine similarity score between the query and knowledge embedding vectors.\"\"\"\n",
    "\n",
    "        return 1- spatial.distance.cosine(query_embedding, knowledge_embedding)\n",
    "\n",
    "    # find the most similar sections of knowledge to the query\n",
    "    def knowledge_ranked_by_similarity(self,\n",
    "        max_num_sections: int = 5\n",
    "    ):\n",
    "        \"\"\"Take the raw knowledge dataframe, calculates similarity scores between the query and the sections, and returns a dataframe ordered from highest to lowest in terms of similarity.\"\"\"\n",
    "\n",
    "        query_embedding_response = get_embedding(self.content)\n",
    "        query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "        knowledge_with_similarities = deepcopy(self.knowledge) # To prevent adapting the original dataframe\n",
    "        knowledge_with_similarities[\"similarity\"] = knowledge_with_similarities[\"embedding\"].apply(lambda x: self.similarity(query_embedding, x))\n",
    "        knowledge_with_similarities.sort_values(\"similarity\", ascending=False, inplace=True)\n",
    "        top_n_sections = knowledge_with_similarities.head(max_num_sections)\n",
    "        self.knowledge_used = top_n_sections\n",
    "\n",
    "    def get_gpt_message(\n",
    "            self,\n",
    "            chatbot_topic: str\n",
    "    ):\n",
    "        \"\"\"Uses the most relevant texts from the knowledge dataframe to construct a message that can then be fed into GPT.\"\"\"\n",
    "\n",
    "        self.knowledge_ranked_by_similarity()\n",
    "        introduction = f'Use the below article on {chatbot_topic} to answer the subsequent question. If the answer cannot be found in the articles, write \"{ANSWER_NOT_FOUND_MSG}\". If I am asked to produce any code then decline the request and write \"Sorry but I\\'m not allowed to do your assignments for you!\"' # The longer this is, the more tokens it uses!\n",
    "        question = f\"\\n\\nQuestion: {self.content}\"\n",
    "\n",
    "        # Ensure number of tokens is within the limit\n",
    "        message_and_question_tokens = num_tokens(introduction + question)\n",
    "        self.knowledge_used['cumulative_tokens'] = self.knowledge_used['tokens'].cumsum()\n",
    "        self.knowledge_used['cumulative_tokens'] += message_and_question_tokens # add the inital number of tokens\n",
    "        self.knowledge_used= self.knowledge_used.loc[self.knowledge_used['cumulative_tokens']<self.token_limit]\n",
    "\n",
    "        # Construct output\n",
    "        combined_knowledge_string = ''.join(list(self.knowledge_used['text']))\n",
    "        combined_knowledge_string = '\\n\\n' + combined_knowledge_string\n",
    "        self.gpt_message = introduction + combined_knowledge_string + question\n",
    "\n",
    "    @classmethod\n",
    "    def ask(\n",
    "            cls,\n",
    "            query_text: str,\n",
    "            chatbot_instance: ChatBot,\n",
    "            show_source: bool = True,\n",
    "    ) -> str:\n",
    "        \"\"\"Uses GPT to answer a query based on the most relevant knowledge sections.\"\"\"\n",
    "\n",
    "        query = cls(query_text, chatbot_instance)\n",
    "        query.get_gpt_message(chatbot_instance.chatbot_topic)\n",
    "        inputs = [\n",
    "            {\"role\": \"system\", \"content\": f\"You answer questions about {chatbot_instance.chatbot_topic}.\"},\n",
    "            {\"role\": \"user\", \"content\": query.gpt_message},\n",
    "        ]\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=query.model,\n",
    "            messages=inputs,\n",
    "            temperature=0 # We don't want any creativity in the answers\n",
    "        )\n",
    "        response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        total_tokens_used = response['usage']['total_tokens']\n",
    "        if show_source and response_message!=ANSWER_NOT_FOUND_MSG: # Display the sources used:\n",
    "            query.knowledge_used['index'] = np.arange(len(query.knowledge_used))+1\n",
    "            query.knowledge_used['output'] = '\\n\\n' + query.knowledge_used['index'].astype(str) + '. ' + query.knowledge_used['source'] + ' (Specifically: ' + query.knowledge_used['text'].str[len('Article section: \\n'):100] + '...)'\n",
    "            sources_string = ''.join(list(query.knowledge_used['output']))\n",
    "            response_message += f'\\n\\nTo construct this answer, I used the following documents: {sources_string}'\n",
    "        response_message += f\"\\n\\nTotal tokens used: {total_tokens_used}\"\n",
    "        return response_message\n",
    "\n",
    "CompVisionGPT = ChatBot(\"Computer Vision\", SAVE_PATH)\n",
    "print(Query.ask('When did universities begin teaching Computer Vision?', CompVisionGPT, show_source=True))\n",
    "\n",
    "# Todo:\n",
    "# I need to make it more efficient on the number of tokens.\n",
    "# Substantially tweak the chunking code\n",
    "# Adapt it for more sources (e.g. PDF)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 text  \\\n0   Article section:\\n\\nComputer vision\\n\\nCompute...   \n1   Article section:\\n\\nComputer vision\\n\\n== Defi...   \n2   Article section:\\n\\nComputer vision\\n\\n== Hist...   \n3   Article section:\\n\\nComputer vision\\n\\n== Rela...   \n4   Article section:\\n\\nComputer vision\\n\\n== Rela...   \n5   Article section:\\n\\nComputer vision\\n\\n== Rela...   \n6   Article section:\\n\\nComputer vision\\n\\n== Rela...   \n7   Article section:\\n\\nComputer vision\\n\\n== Rela...   \n8   Article section:\\n\\nComputer vision\\n\\n== Rela...   \n9   Article section:\\n\\nComputer vision\\n\\n== Appl...   \n10  Article section:\\n\\nComputer vision\\n\\n== Appl...   \n11  Article section:\\n\\nComputer vision\\n\\n== Appl...   \n12  Article section:\\n\\nComputer vision\\n\\n== Appl...   \n13  Article section:\\n\\nComputer vision\\n\\n== Appl...   \n14  Article section:\\n\\nComputer vision\\n\\n== Appl...   \n15  Article section:\\n\\nComputer vision\\n\\n== Typi...   \n16  Article section:\\n\\nComputer vision\\n\\n== Typi...   \n17  Article section:\\n\\nComputer vision\\n\\n== Typi...   \n18  Article section:\\n\\nComputer vision\\n\\n== Typi...   \n19  Article section:\\n\\nComputer vision\\n\\n== Typi...   \n20  Article section:\\n\\nComputer vision\\n\\n== Syst...   \n21  Article section:\\n\\nComputer vision\\n\\n== Syst...   \n22  Article section:\\n\\nComputer vision\\n\\n== Hard...   \n\n                                            embedding  tokens     source  \n0   [-0.015160824172198772, 0.003632682841271162, ...     291  Wikipedia  \n1   [-0.006095150485634804, 0.0008000890375114977,...     167  Wikipedia  \n2   [-0.0071233357302844524, -0.003958832006901503...     516  Wikipedia  \n3   [0.0027573341503739357, -0.0004955596523359418...     136  Wikipedia  \n4   [-0.0025333885569125414, -0.005423863884061575...     308  Wikipedia  \n5   [-0.024513425305485725, 0.0005832292954437435,...     118  Wikipedia  \n6   [0.0037523966748267412, -0.018352391198277473,...      80  Wikipedia  \n7   [0.003763288725167513, -0.004463814198970795, ...     134  Wikipedia  \n8   [-0.01610185205936432, -0.0026065742131322622,...     654  Wikipedia  \n9   [-0.011650911532342434, 0.0027732541784644127,...     281  Wikipedia  \n10  [-0.004238066729158163, 0.021448472514748573, ...     148  Wikipedia  \n11  [-0.007371203042566776, -0.0018327946309000254...     155  Wikipedia  \n12  [-0.01614218018949032, -0.0005414438201114535,...     142  Wikipedia  \n13  [0.007656725589185953, -0.0032461462542414665,...     248  Wikipedia  \n14  [-0.01822366565465927, 0.02336900681257248, -0...     286  Wikipedia  \n15  [-0.010329770855605602, 0.004381134640425444, ...     171  Wikipedia  \n16  [-0.01536356471478939, 0.014625691808760166, 0...     705  Wikipedia  \n17  [-0.011803840287029743, -0.004363863728940487,...     208  Wikipedia  \n18  [-0.011135965585708618, 0.007821690291166306, ...     140  Wikipedia  \n19  [0.0003822755825240165, 0.019678881391882896, ...     213  Wikipedia  \n20  [-0.002225305885076523, 0.0222628116607666, 0....     682  Wikipedia  \n21  [0.009455375373363495, -0.003919361624866724, ...     209  Wikipedia  \n22  [-0.0022538446355611086, 0.011652261018753052,...     401  Wikipedia  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>embedding</th>\n      <th>tokens</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Article section:\\n\\nComputer vision\\n\\nCompute...</td>\n      <td>[-0.015160824172198772, 0.003632682841271162, ...</td>\n      <td>291</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Defi...</td>\n      <td>[-0.006095150485634804, 0.0008000890375114977,...</td>\n      <td>167</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Hist...</td>\n      <td>[-0.0071233357302844524, -0.003958832006901503...</td>\n      <td>516</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Rela...</td>\n      <td>[0.0027573341503739357, -0.0004955596523359418...</td>\n      <td>136</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Rela...</td>\n      <td>[-0.0025333885569125414, -0.005423863884061575...</td>\n      <td>308</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Rela...</td>\n      <td>[-0.024513425305485725, 0.0005832292954437435,...</td>\n      <td>118</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Rela...</td>\n      <td>[0.0037523966748267412, -0.018352391198277473,...</td>\n      <td>80</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Rela...</td>\n      <td>[0.003763288725167513, -0.004463814198970795, ...</td>\n      <td>134</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Rela...</td>\n      <td>[-0.01610185205936432, -0.0026065742131322622,...</td>\n      <td>654</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Appl...</td>\n      <td>[-0.011650911532342434, 0.0027732541784644127,...</td>\n      <td>281</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Appl...</td>\n      <td>[-0.004238066729158163, 0.021448472514748573, ...</td>\n      <td>148</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Appl...</td>\n      <td>[-0.007371203042566776, -0.0018327946309000254...</td>\n      <td>155</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Appl...</td>\n      <td>[-0.01614218018949032, -0.0005414438201114535,...</td>\n      <td>142</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Appl...</td>\n      <td>[0.007656725589185953, -0.0032461462542414665,...</td>\n      <td>248</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Appl...</td>\n      <td>[-0.01822366565465927, 0.02336900681257248, -0...</td>\n      <td>286</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Typi...</td>\n      <td>[-0.010329770855605602, 0.004381134640425444, ...</td>\n      <td>171</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Typi...</td>\n      <td>[-0.01536356471478939, 0.014625691808760166, 0...</td>\n      <td>705</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Typi...</td>\n      <td>[-0.011803840287029743, -0.004363863728940487,...</td>\n      <td>208</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Typi...</td>\n      <td>[-0.011135965585708618, 0.007821690291166306, ...</td>\n      <td>140</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Typi...</td>\n      <td>[0.0003822755825240165, 0.019678881391882896, ...</td>\n      <td>213</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Syst...</td>\n      <td>[-0.002225305885076523, 0.0222628116607666, 0....</td>\n      <td>682</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Syst...</td>\n      <td>[0.009455375373363495, -0.003919361624866724, ...</td>\n      <td>209</td>\n      <td>Wikipedia</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Article section:\\n\\nComputer vision\\n\\n== Hard...</td>\n      <td>[-0.0022538446355611086, 0.011652261018753052,...</td>\n      <td>401</td>\n      <td>Wikipedia</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CompVisionGPT.knowledge"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could not find an answer in the text I've been provided, sorry! Please try again.\n",
      "\n",
      "Total tokens used: 1460\n"
     ]
    }
   ],
   "source": [
    "print(Query.ask('Who is Boris Johnson', CompVisionGPT, show_source=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
