{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GPT-3.5-Turbo Model\n",
    "Creating a question answering chatbot using GPT-3.5. Adapted from: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_tokens() IS OFTEN USED IN KNOWLEDGE.PY (and other files??) WITHOUT SPECIFYING THE EMBEDDING MODEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using a CPU\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e863f3ec9ed84c679e2c4ca9dadf1d67"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/6.81k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30d46b59ef064829b31fb8a0527def93"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\point\\anaconda3\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using a CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets, sentencepiece, transformers, accelerate, tiktoken, rouge_score, evaluate rouge_score bleu_score\n",
    "import sys\n",
    "\n",
    "import openai.error\n",
    "\n",
    "sys.path.append(\"modules\")\n",
    "from modules.config import *\n",
    "from modules.knowledge import *\n",
    "from modules.chatbot import *\n",
    "from modules.embedding_functions import *\n",
    "from modules.data_extraction import *\n",
    "from modules.data_preprocessing import *\n",
    "from modules.gpt_ans_extraction import *\n",
    "from modules.query import *\n",
    "sys.path.remove(\"modules\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NQ Data Extraction and Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Data extraction\n",
    "training = AllData(cache_dir='/content/drive/MyDrive/Diss/Datasets', default='dev') # NEED TO CORRECT THIS (train/test separation for efficiency)\n",
    "training.export_simplified_dataset(path=\"/content/drive/MyDrive/Diss/Output/simplified_dataset_validation_new.csv\")\n",
    "\n",
    "### Data preprocessing\n",
    "training_data = TrainingData(save_dir=f'{OUTPUT_DIR}/all_data_{short_model_name}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/16433 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a4beddc229240dfb8fe5d31b3a3acda"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/4109 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0bb0bd2096984256968222401a847768"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'url', 'content', 'question', 'answer', 'start_token', 'end_token', 'length', 'input_ids', 'attention_mask', 'labels', 'num_tokens'],\n        num_rows: 11504\n    })\n    test: Dataset({\n        features: ['id', 'title', 'url', 'content', 'question', 'answer', 'start_token', 'end_token', 'length', 'input_ids', 'attention_mask', 'labels', 'num_tokens'],\n        num_rows: 2876\n    })\n})"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset and extract only the answerable questions\n",
    "# output_dir = 'G:\\My Drive\\Diss\\Output'\n",
    "# all_data = load_from_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}').shuffle(seed=9)\n",
    "all_ans_data = training_data.training_data.filter(lambda row: (row[\"answer\"] != NO_ANS_TOKEN))\n",
    "all_ans_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 45/11504 [00:56<3:53:04,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Mon, 24 Jul 2023 13:14:26 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7ebc65811d5535da-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 54/11504 [06:27<23:50:21,  7.50s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 71/11504 [07:27<3:23:18,  1.07s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 124/11504 [09:15<3:46:21,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 134/11504 [10:06<4:59:17,  1.58s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred: Request failed due to server shutdown {\n",
      "  \"error\": {\n",
      "    \"message\": \"Request failed due to server shutdown\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'Request failed due to server shutdown', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Mon, 24 Jul 2023 13:18:35 GMT', 'Content-Type': 'application/json', 'Content-Length': '141', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-0613', 'openai-organization': 'user-uqizbohgnk1fzwijnlgc59zd', 'openai-processing-ms': '10433', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89827', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '114ms', 'x-request-id': '72bcbd69624d233cb3a4b34e38e2fb5d', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7ebc72ecbfbd48ad-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 182/11504 [11:10<3:14:40,  1.03s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 195/11504 [12:07<4:09:54,  1.33s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 394/11504 [16:48<3:00:41,  1.02it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 590/11504 [21:21<4:55:01,  1.62s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 608/11504 [22:18<2:41:31,  1.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 713/11504 [25:04<2:55:07,  1.03it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 813/11504 [27:45<2:49:33,  1.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 845/11504 [29:00<2:15:06,  1.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Mon, 24 Jul 2023 13:42:29 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7ebc8e9b8bffbc9d-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 884/11504 [35:01<2:54:02,  1.02it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 911/11504 [36:09<2:56:11,  1.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 933/11504 [37:16<4:50:42,  1.65s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 999/11504 [39:25<3:03:53,  1.05s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 1004/11504 [40:12<12:39:55,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1017/11504 [41:06<3:14:03,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1169/11504 [44:32<2:00:56,  1.42it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Mon, 24 Jul 2023 13:58:01 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7ebca55e1fba888f-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1175/11504 [49:50<48:03:34, 16.75s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1230/11504 [51:33<3:42:07,  1.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1263/11504 [52:47<3:04:03,  1.08s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1350/11504 [55:07<3:37:31,  1.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1457/11504 [57:51<3:18:28,  1.19s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1480/11504 [58:59<2:19:13,  1.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 1567/11504 [1:01:18<2:48:46,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1669/11504 [1:03:54<3:10:17,  1.16s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1684/11504 [1:04:57<4:11:52,  1.54s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1712/11504 [1:06:15<3:22:00,  1.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Mon, 24 Jul 2023 14:19:44 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7ebcc52bcbd2dd86-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1713/11504 [1:11:27<257:16:40, 94.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1722/11504 [1:12:16<14:24:08,  5.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1744/11504 [1:13:21<2:42:55,  1.00s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major error The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1798/11504 [1:15:02<2:29:11,  1.08it/s] "
     ]
    }
   ],
   "source": [
    "# Iterate through the dataset and query GPT to extract a natural-language answer\n",
    "splits = ['train', 'test']\n",
    "for s in splits:\n",
    "    all_timestamps = []\n",
    "    responses = []\n",
    "    # Obtain GPT answers/responses\n",
    "    for i in tqdm(range(len(all_ans_data[s]))): # Due to the RPM\n",
    "        all_timestamps = pause_if_needed(all_timestamps)\n",
    "\n",
    "        formatted_text = format_request(all_ans_data[s][i])\n",
    "        inputs = [\n",
    "                {\"role\": \"system\", \"content\": f\"You answer questions by only using a provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": formatted_text},\n",
    "            ]\n",
    "        response, all_timestamps = query_gpt(inputs, all_timestamps)\n",
    "        responses.append(response.choices[0].message.content)\n",
    "\n",
    "    # Export to txt file\n",
    "    file_name = f\"{s}_all.txt\"\n",
    "    # Open the file in write mode and save the list to the file\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        for item in responses:\n",
    "            file.write(item + \"\\n\")\n",
    "\n",
    "    # Export as HF dataset\n",
    "    df_pandas = all_ans_data[s].to_pandas()\n",
    "    df_pandas['gpt_ans'] = responses\n",
    "    new_dataset = Dataset.from_pandas(df_pandas)\n",
    "    new_dataset.save_to_disk(f\"{OUTPUT_DIR}/{short_model_name}_{s}_split\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now the new (GPT) answers can be merged with the original dataset\n",
    "# Load the original data and GPT data generated above\n",
    "train_dataset_pandas = training_data.training_data['train'].to_pandas()\n",
    "test_dataset_pandas = training_data.training_data['test'].to_pandas()\n",
    "# updated_train_dataset_pandas = load_from_disk(f\"assets/{short_model_name}_train_split\").to_pandas()\n",
    "# updated_test_dataset_pandas = load_from_disk(f\"assets/{short_model_name}_test_split\").to_pandas()\n",
    "updated_train_dataset_pandas = load_from_disk(f\"{OUTPUT_DIR}/{short_model_name}_train_split\").to_pandas()\n",
    "updated_test_dataset_pandas = load_from_disk(f\"{OUTPUT_DIR}/{short_model_name}_test_split\").to_pandas()\n",
    "\n",
    "# Merge on id and keep all rows\n",
    "train_merged = pd.merge(train_dataset_pandas, updated_train_dataset_pandas[['id', 'gpt_ans']], on='id', how='outer')\n",
    "test_merged = pd.merge(test_dataset_pandas, updated_test_dataset_pandas[['id', 'gpt_ans']], on='id', how='outer')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove any examples where it apparently had an answer but GPT couldn't extract oen\n",
    "train_merged = train_merged[train_merged['gpt_ans'] != NO_ANS_TOKEN]\n",
    "test_merged = test_merged[test_merged['gpt_ans'] !=  NO_ANS_TOKEN]\n",
    "\n",
    "# Update the answers to match the GPT ones\n",
    "train_merged.loc[train_merged['answer'] != NO_ANS_TOKEN, 'answer'] = train_merged.loc[train_merged['answer'] != NO_ANS_TOKEN, 'gpt_ans']\n",
    "test_merged.loc[test_merged['answer'] != NO_ANS_TOKEN, 'answer'] = test_merged.loc[test_merged['answer'] != NO_ANS_TOKEN, 'gpt_ans']\n",
    "\n",
    "merged_dataset = DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(train_merged),\n",
    "        \"test\": Dataset.from_pandas(test_merged),\n",
    "    })\n",
    "\n",
    "# Maintain the answerable/non-answerable balance\n",
    "def ensure_ans_non_ans_balance(dataset, dataset_splits=('training', 'validation'), proportion=0.3, seed=SEED):\n",
    "    # Extracting the unanswerable examples\n",
    "    no_ans = dataset.filter(lambda row: (row[\"answer\"] == NO_ANS_TOKEN))\n",
    "    good_ans = dataset.filter(lambda row: (row[\"answer\"] != NO_ANS_TOKEN))\n",
    "\n",
    "    # Discarding some unanswerable examples so the answer-no_ans ratio is favourable in each split\n",
    "    processed_datasets_dict = {}\n",
    "    for split in dataset_splits:\n",
    "      num_no_ans = proportion*len(good_ans[split])/(1-proportion)\n",
    "      no_ans_keep = no_ans[split].train_test_split(train_size=num_no_ans/len(no_ans[split]), seed=seed)['train']\n",
    "      processed_datasets_dict[split] = concatenate_datasets([no_ans_keep, good_ans[split]])\n",
    "\n",
    "    processed_dataset = DatasetDict({\n",
    "            dataset_splits[0]: processed_datasets_dict[dataset_splits[0]],\n",
    "            dataset_splits[1]: processed_datasets_dict[dataset_splits[1]],\n",
    "        })\n",
    "    shuffled_dataset = processed_dataset.shuffle(seed=seed)\n",
    "    return shuffled_dataset\n",
    "\n",
    "final_dataset = ensure_ans_non_ans_balance(merged_dataset, dataset_splits=['train', 'test'])\n",
    "\n",
    "# As can be seen, the 80/20 train-test split has been maintained\n",
    "len(final_dataset['train'])/(len(final_dataset['train'])+len(final_dataset['test']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finally, the questions/contexts and answers need to be tokenised to override previous values\n",
    "def tokenise(data):\n",
    "    # tokenize the inputs (questions and contexts)\n",
    "    additional_cols = tokeniser(data['content'], data['question'], truncation=False)\n",
    "\n",
    "    # tokenize the answers\n",
    "    targets = tokeniser(text_target=data['answer'], truncation=False)\n",
    "\n",
    "    #set labels\n",
    "    additional_cols['labels'] = targets['input_ids']\n",
    "    additional_cols['num_tokens'] = [len(row) for row in additional_cols[\"input_ids\"]]\n",
    "    return additional_cols\n",
    "\n",
    "final_dataset = final_dataset.map(tokenise, batched = True)\n",
    "final_dataset.save_to_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}_gpt_updated')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NQ Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "all_data = load_from_disk(f'{OUTPUT_DIR}/all_data_{short_model_name}_gpt_updated') # add or remove the suffix a required\n",
    "\n",
    "# Connect to HuggingFace\n",
    "HfFolder.save_token(\"ADD_TOKEN_HERE\")\n",
    "!git config --global user.email \"pointon.joel@gmail.com\"\n",
    "!git config --global user.name \"Joel Pointon\"\n",
    "\n",
    "# Training arguments/config\n",
    "batch_size = 8 # 64\n",
    "num_train_epochs = 8\n",
    "logging_steps = len(all_data[\"training\"]) // batch_size # Show the training loss with every epoch\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_name}_updated\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    push_to_hub=True,\n",
    "    seed=9,\n",
    "    # optim=\"adafactor\"\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokeniser.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokeniser.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokeniser.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores\n",
    "    # result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokeniser, model=model)\n",
    "\n",
    "def model_init():\n",
    "  return AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=all_data[\"train\"],\n",
    "    eval_dataset=all_data[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokeniser,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "path = f\"/content/drive/MyDrive/Diss/Output/{short_model_name}-finetuned-natural-questions\"\n",
    "trainer.train()\n",
    "trainer.push_to_hub()\n",
    "trainer.save_model(path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# NEED TO FIX THIS\n",
    "filename = 'assets/knowledge/Fundamentals_of_Digital_Image_Processing_Textbook.pdf'\n",
    "doc = fitz.open(filename)\n",
    "page_limit = None\n",
    "all_text = ''\n",
    "# Iterate through the content\n",
    "for page in doc:\n",
    "    page_limit = doc.page_count if not page_limit else page_limit\n",
    "    if page.number <= page_limit:\n",
    "        block_content = page.get_text(\"blocks\") #.encode(\"utf8\") # \"blocks\"\n",
    "        for block in block_content:\n",
    "            if block[6] == 0:  # I.e. only extract text\n",
    "                plain_text = unidecode(block[4])  # .decode('latin1') #.decode('utf-8')\n",
    "                all_text += plain_text\n",
    "    else:\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Rate limit reached for default-text-embedding-ada-002 in organization org-fD9aTSHfvzCKrinOV5h9G0Xm on tokens per min. Limit: 150000 / min. Current: 0 / min. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 8\u001B[0m\n\u001B[0;32m      4\u001B[0m CompVisionKnowledge \u001B[38;5;241m=\u001B[39m Knowledge(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCompVisionPDF\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGPT\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# for page in WIKI_PAGES:\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# CompVisionKnowledge.append_wikipedia_page(WIKI_PAGE) # page\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# CompVisionKnowledge.append_pdf(filename, 'CompVisionPDF')\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[43mCompVisionKnowledge\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappend_pdf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mCompVisionPDF\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# CompVisionKnowledge.export_to_csv(GPT_KNOWLEDGE_FILENAME)\u001B[39;00m\n\u001B[0;32m     10\u001B[0m CompVisionKnowledge\u001B[38;5;241m.\u001B[39mdf\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\Standard Files\\University\\Masters\\Diss\\MSc-Diss\\code\\modules\\knowledge.py:353\u001B[0m, in \u001B[0;36mKnowledge.append_pdf\u001B[1;34m(self, filename_path, document_name)\u001B[0m\n\u001B[0;32m    350\u001B[0m knowledge \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_source_column(knowledge, docType\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPDF\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    352\u001B[0m \u001B[38;5;66;03m# conduct final formatting and getting embeddings\u001B[39;00m\n\u001B[1;32m--> 353\u001B[0m knowledge \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat_and_get_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mknowledge\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[38;5;66;03m# Concat with main self.df\u001B[39;00m\n\u001B[0;32m    356\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf, knowledge])\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\Standard Files\\University\\Masters\\Diss\\MSc-Diss\\code\\modules\\knowledge.py:219\u001B[0m, in \u001B[0;36mKnowledge.format_and_get_embeddings\u001B[1;34m(self, knowledge)\u001B[0m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;66;03m# Get embeddings\u001B[39;00m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGPT\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m--> 219\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mget_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mknowledge\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mContent\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, be \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(response[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m]):\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m i \u001B[38;5;241m==\u001B[39m be[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m]  \u001B[38;5;66;03m# double check embeddings are in same order as input\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\Standard Files\\University\\Masters\\Diss\\MSc-Diss\\code\\modules\\embedding_functions.py:23\u001B[0m, in \u001B[0;36mget_embedding\u001B[1;34m(content, embedding_model)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;124;03m    Returns the embedding of a string given an embedding model.\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m embedding_model \u001B[38;5;241m==\u001B[39m GPT_EMBEDDING_MODEL:\n\u001B[1;32m---> 23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEmbedding\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membedding_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     25\u001B[0m     similarity_model \u001B[38;5;241m=\u001B[39m SentenceTransformer(embedding_model)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\embedding.py:33\u001B[0m, in \u001B[0;36mEmbedding.create\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 33\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     35\u001B[0m         \u001B[38;5;66;03m# If a user specifies base64, we'll just return the encoded string.\u001B[39;00m\n\u001B[0;32m     36\u001B[0m         \u001B[38;5;66;03m# This is only for the default case.\u001B[39;00m\n\u001B[0;32m     37\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m user_provided_encoding_format:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001B[0m, in \u001B[0;36mEngineAPIResource.create\u001B[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    136\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[0;32m    137\u001B[0m ):\n\u001B[0;32m    138\u001B[0m     (\n\u001B[0;32m    139\u001B[0m         deployment_id,\n\u001B[0;32m    140\u001B[0m         engine,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    150\u001B[0m         api_key, api_base, api_type, api_version, organization, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[0;32m    151\u001B[0m     )\n\u001B[1;32m--> 153\u001B[0m     response, _, api_key \u001B[38;5;241m=\u001B[39m \u001B[43mrequestor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[0;32m    164\u001B[0m         \u001B[38;5;66;03m# must be an iterator\u001B[39;00m\n\u001B[0;32m    165\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, OpenAIResponse)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:298\u001B[0m, in \u001B[0;36mAPIRequestor.request\u001B[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[0;32m    277\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    278\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    279\u001B[0m     method,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    286\u001B[0m     request_timeout: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    287\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m    288\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_raw(\n\u001B[0;32m    289\u001B[0m         method\u001B[38;5;241m.\u001B[39mlower(),\n\u001B[0;32m    290\u001B[0m         url,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    296\u001B[0m         request_timeout\u001B[38;5;241m=\u001B[39mrequest_timeout,\n\u001B[0;32m    297\u001B[0m     )\n\u001B[1;32m--> 298\u001B[0m     resp, got_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    299\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m resp, got_stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:700\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response\u001B[1;34m(self, result, stream)\u001B[0m\n\u001B[0;32m    692\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m    693\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interpret_response_line(\n\u001B[0;32m    694\u001B[0m             line, result\u001B[38;5;241m.\u001B[39mstatus_code, result\u001B[38;5;241m.\u001B[39mheaders, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    695\u001B[0m         )\n\u001B[0;32m    696\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m parse_stream(result\u001B[38;5;241m.\u001B[39miter_lines())\n\u001B[0;32m    697\u001B[0m     ), \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m--> 700\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response_line\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    701\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    702\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstatus_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    703\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    704\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    705\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    706\u001B[0m         \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    707\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:763\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response_line\u001B[1;34m(self, rbody, rcode, rheaders, stream)\u001B[0m\n\u001B[0;32m    761\u001B[0m stream_error \u001B[38;5;241m=\u001B[39m stream \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mdata\n\u001B[0;32m    762\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream_error \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m rcode \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m300\u001B[39m:\n\u001B[1;32m--> 763\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_error_response(\n\u001B[0;32m    764\u001B[0m         rbody, rcode, resp\u001B[38;5;241m.\u001B[39mdata, rheaders, stream_error\u001B[38;5;241m=\u001B[39mstream_error\n\u001B[0;32m    765\u001B[0m     )\n\u001B[0;32m    766\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "\u001B[1;31mRateLimitError\u001B[0m: Rate limit reached for default-text-embedding-ada-002 in organization org-fD9aTSHfvzCKrinOV5h9G0Xm on tokens per min. Limit: 150000 / min. Current: 0 / min. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method."
     ]
    }
   ],
   "source": [
    "# get_embedding(\"Joel\", embedding_model=self.embedding_model)\n",
    "# filename = 'assets/knowledge/Fundamentals_of_Digital_Image_Processing_Textbook.pdf'\n",
    "filename = 'assets/knowledge/Fundamentals_of_Digital_Image_Processing_Textbook.pdf'\n",
    "CompVisionKnowledge = Knowledge('CompVisionPDF', 'GPT')\n",
    "# for page in WIKI_PAGES:\n",
    "# CompVisionKnowledge.append_wikipedia_page(WIKI_PAGE) # page\n",
    "# CompVisionKnowledge.append_pdf(filename, 'CompVisionPDF')\n",
    "CompVisionKnowledge.append_pdf(filename, 'CompVisionPDF')\n",
    "# CompVisionKnowledge.export_to_csv(GPT_KNOWLEDGE_FILENAME)\n",
    "CompVisionKnowledge.df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MLM Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Extracting training content\n",
    "all_text = ''.join(CompVisionKnowledge.df['Content'].tolist())\n",
    "with open(f'assets/knowledge/Digital_Image_Processing_Textbook.txt', \"w\") as f:\n",
    "    f.write(all_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CompVisionKnowledgeBERT = Knowledge(WIKI_PAGE, 'BERT')\n",
    "CompVisionKnowledgeBERT.append_wikipedia_page(WIKI_PAGE)\n",
    "# save document chunks and embeddings\n",
    "CompVisionKnowledgeBERT.export_to_csv(BERT_KNOWLEDGE_FILENAME)\n",
    "CompVisionKnowledgeBERT.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Search\n",
    "Now we'll define a search function that:\n",
    "\n",
    "Takes a user query and a dataframe with text & embedding columns\n",
    "Embeds the user query with the OpenAI API\n",
    "Uses distance between query embedding and text embeddings to rank the texts\n",
    "Returns two lists:\n",
    "The top N texts, ranked by relevance\n",
    "Their corresponding relevance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Todo:\n",
    "# I need to make it more efficient on the number of tokens\n",
    "# Check num_tokens usage and specifying the encoding model\n",
    "# Fix the potential issue fo GPT sections being longer than 1024 tokens when using BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\point\\OneDrive\\Documents\\Standard Files\\University\\Masters\\Diss\\MSc-Diss\\code\\modules\\query.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Index'] = np.arange(len(self.knowledge_used)) + 1\n",
      "C:\\Users\\point\\OneDrive\\Documents\\Standard Files\\University\\Masters\\Diss\\MSc-Diss\\code\\modules\\query.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Cumulative_tokens'] = self.knowledge_used['Tokens'].cumsum()\n",
      "C:\\Users\\point\\OneDrive\\Documents\\Standard Files\\University\\Masters\\Diss\\MSc-Diss\\code\\modules\\query.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Cumulative_tokens'] += message_and_question_tokens  # add the initial number of tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA stands for Principal Component Analysis, which is a statistical technique used for dimensionality reduction of correlated data. It involves finding a new set of uncorrelated variables (or axes) that are linear combinations of the original variables, and which capture the maximum amount of variance in the data. PCA is commonly used in digital imaging for tasks such as data compression, automated facial recognition, and determination of the orientation of basic shapes.\n",
      "\n",
      "To construct this answer, I used the following documents : \n",
      "\n",
      "1. CompVisionPDF-> p.265/266/267:\n",
      "The reader is probably familiar with the common saying that goes something along the\n",
      "lines of 'Why ...\n",
      "\n",
      "2. CompVisionPDF-> p.268/269/270:\n",
      "For a 2-D space, we must stop here (there are no more dimensions left). Figure 9.9\n",
      "shows our exampl...\n",
      "\n",
      "Total tokens used: 3227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\point\\OneDrive\\Documents\\Standard Files\\University\\Masters\\Diss\\MSc-Diss\\code\\modules\\query.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Output'] = '\\n\\n' + self.knowledge_used['Index'].astype(str) + '. ' + self.knowledge_used[\n"
     ]
    }
   ],
   "source": [
    "CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\n",
    "print(Query.ask('What is PCA?', CompVisionGPT, show_source=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d07a56b31944c4bb14a999b57fd3c4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (768,) (1536,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# CompVisionBERT = ChatBot(\"Computer Vision\", 'assets/' + BERT_KNOWLEDGE_FILENAME)\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mQuery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mask_bert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mWhen did universities begin teaching Computer Vision?\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCompVisionGPT\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.ask_bert\u001B[1;34m(cls, query_text, chatbot_instance, embedding_model, encoding_model, bert_model, show_source)\u001B[0m\n\u001B[0;32m    126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQuestion is too long, please try again with a shorter question.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    127\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(query_text, chatbot_instance)\n\u001B[1;32m--> 128\u001B[0m response_message, answer_index \u001B[38;5;241m=\u001B[39m \u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_bert_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membedding_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbert_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbert_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m show_source \u001B[38;5;129;01mand\u001B[39;00m response_message\u001B[38;5;241m!=\u001B[39mANSWER_NOT_FOUND_MSG: \u001B[38;5;66;03m# Display the sources used:\u001B[39;00m\n\u001B[0;32m    131\u001B[0m     response_message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m query\u001B[38;5;241m.\u001B[39mshow_source_message(answer_index\u001B[38;5;241m=\u001B[39manswer_index)\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.get_bert_output\u001B[1;34m(self, embedding_model, encoding_model, bert_model)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_bert_output\u001B[39m(\n\u001B[0;32m     77\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     78\u001B[0m         embedding_model: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m     79\u001B[0m         encoding_model: BertTokenizer \u001B[38;5;241m=\u001B[39m BERT_ENCODING,\n\u001B[0;32m     80\u001B[0m         bert_model: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m BERT_MODEL\n\u001B[0;32m     81\u001B[0m ):\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;124;03m\"\"\"Uses the most relevant texts from the knowledge dataframe to construct a message that can then be fed into GPT.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 83\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mknowledge_ranked_by_similarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membedding_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m     answer_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     86\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.knowledge_ranked_by_similarity\u001B[1;34m(self, max_num_sections, confidence_level, embedding_model)\u001B[0m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# knowledge_with_similarities[\"similarity\"] = knowledge_with_similarities[\"Embedding\"].apply(lambda x: self.similarity(query_embedding, x))\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(query_embedding_response)\n\u001B[1;32m---> 34\u001B[0m knowledge_with_similarities[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mknowledge_with_similarities\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEmbedding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_embedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     36\u001B[0m knowledge_with_similarities\u001B[38;5;241m.\u001B[39msort_values(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     37\u001B[0m top_n_sections \u001B[38;5;241m=\u001B[39m knowledge_with_similarities\u001B[38;5;241m.\u001B[39mhead(max_num_sections)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[0;32m   4323\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4324\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4325\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4328\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4329\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4330\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4331\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4332\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4431\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4432\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4433\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1078\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m   1079\u001B[0m     \u001B[38;5;66;03m# if we are a string, try to dispatch\u001B[39;00m\n\u001B[0;32m   1080\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[1;32m-> 1082\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1131\u001B[0m         values \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values\n\u001B[0;32m   1132\u001B[0m         \u001B[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001B[39;00m\n\u001B[0;32m   1133\u001B[0m         \u001B[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001B[39;00m\n\u001B[0;32m   1134\u001B[0m         \u001B[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001B[39;00m\n\u001B[0;32m   1135\u001B[0m         \u001B[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001B[39;00m\n\u001B[0;32m   1136\u001B[0m         \u001B[38;5;66;03m# \"Callable[[Any], Any]\"\u001B[39;00m\n\u001B[1;32m-> 1137\u001B[0m         mapped \u001B[38;5;241m=\u001B[39m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1138\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1139\u001B[0m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m   1140\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1141\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1144\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1145\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1146\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.knowledge_ranked_by_similarity.<locals>.<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# knowledge_with_similarities[\"similarity\"] = knowledge_with_similarities[\"Embedding\"].apply(lambda x: self.similarity(query_embedding, x))\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(query_embedding_response)\n\u001B[1;32m---> 34\u001B[0m knowledge_with_similarities[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m knowledge_with_similarities[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmbedding\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_embedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     36\u001B[0m knowledge_with_similarities\u001B[38;5;241m.\u001B[39msort_values(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     37\u001B[0m top_n_sections \u001B[38;5;241m=\u001B[39m knowledge_with_similarities\u001B[38;5;241m.\u001B[39mhead(max_num_sections)\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.similarity\u001B[1;34m(query_embedding, knowledge_embedding)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msimilarity\u001B[39m(query_embedding: \u001B[38;5;28mlist\u001B[39m,\n\u001B[0;32m     13\u001B[0m                knowledge_embedding: \u001B[38;5;28mlist\u001B[39m\n\u001B[0;32m     14\u001B[0m                ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;124;03m\"\"\"Calculates the cosine similarity score between the query and knowledge embedding vectors.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m1\u001B[39m\u001B[38;5;241m-\u001B[39m \u001B[43mspatial\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcosine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_embedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mknowledge_embedding\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:670\u001B[0m, in \u001B[0;36mcosine\u001B[1;34m(u, v, w)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;124;03mCompute the Cosine distance between 1-D arrays.\u001B[39;00m\n\u001B[0;32m    630\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    665\u001B[0m \n\u001B[0;32m    666\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    667\u001B[0m \u001B[38;5;66;03m# cosine distance is also referred to as 'uncentered correlation',\u001B[39;00m\n\u001B[0;32m    668\u001B[0m \u001B[38;5;66;03m#   or 'reflective correlation'\u001B[39;00m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;66;03m# clamp the result to 0-2\u001B[39;00m\n\u001B[1;32m--> 670\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mmin\u001B[39m(\u001B[43mcorrelation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mu\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcentered\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m, \u001B[38;5;241m2.0\u001B[39m))\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:619\u001B[0m, in \u001B[0;36mcorrelation\u001B[1;34m(u, v, w, centered)\u001B[0m\n\u001B[0;32m    617\u001B[0m     u \u001B[38;5;241m=\u001B[39m u \u001B[38;5;241m-\u001B[39m umu\n\u001B[0;32m    618\u001B[0m     v \u001B[38;5;241m=\u001B[39m v \u001B[38;5;241m-\u001B[39m vmu\n\u001B[1;32m--> 619\u001B[0m uv \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39maverage(\u001B[43mu\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m, weights\u001B[38;5;241m=\u001B[39mw)\n\u001B[0;32m    620\u001B[0m uu \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39maverage(np\u001B[38;5;241m.\u001B[39msquare(u), weights\u001B[38;5;241m=\u001B[39mw)\n\u001B[0;32m    621\u001B[0m vv \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39maverage(np\u001B[38;5;241m.\u001B[39msquare(v), weights\u001B[38;5;241m=\u001B[39mw)\n",
      "\u001B[1;31mValueError\u001B[0m: operands could not be broadcast together with shapes (768,) (1536,) "
     ]
    }
   ],
   "source": [
    "CompVisionBERT = ChatBot(\"Computer Vision\", 'assets/' + BERT_KNOWLEDGE_FILENAME)\n",
    "print(Query.ask_bert('When did universities begin teaching Computer Vision?', CompVisionBERT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96c7a743564c4afda124d34911ac58d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [20]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mQuery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mask_bart\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mWhen did Universities begin teaching Computer Vision?\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCompVisionBERT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_source\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.ask_bart\u001B[1;34m(cls, query_text, chatbot_instance, show_source, confidence_level)\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQuestion is too long, please try again with a shorter question.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    214\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(query_text, chatbot_instance)\n\u001B[1;32m--> 215\u001B[0m response_message \u001B[38;5;241m=\u001B[39m \u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_bart_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfidence_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfidence_level\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m show_source \u001B[38;5;129;01mand\u001B[39;00m response_message\u001B[38;5;241m!=\u001B[39mANSWER_NOT_FOUND_MSG: \u001B[38;5;66;03m# Display the sources used:\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     response_message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m query\u001B[38;5;241m.\u001B[39mshow_source_message()\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.get_bart_output\u001B[1;34m(self, encoding_model, bert_model, confidence_level)\u001B[0m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_bart_output\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    171\u001B[0m                     \u001B[38;5;66;03m# chatbot_instance: ChatBot,\u001B[39;00m\n\u001B[0;32m    172\u001B[0m                     \u001B[38;5;66;03m# embedding_model: str = BART_EMBEDDING_MODEL,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    175\u001B[0m                     confidence_level: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m,\n\u001B[0;32m    176\u001B[0m                     ):\n\u001B[1;32m--> 177\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mknowledge_ranked_by_similarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfidence_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfidence_level\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mknowledge_used)\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m ANSWER_NOT_FOUND_MSG\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.knowledge_ranked_by_similarity\u001B[1;34m(self, max_num_sections, confidence_level, embedding_model)\u001B[0m\n\u001B[0;32m     28\u001B[0m query_embedding_response \u001B[38;5;241m=\u001B[39m get_embedding(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontent, embedding_model\u001B[38;5;241m=\u001B[39mBERT_EMBEDDING_MODEL)\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m embedding_model \u001B[38;5;241m==\u001B[39m GPT_EMBEDDING_MODEL:\n\u001B[1;32m---> 30\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[43mquery_embedding_response\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# knowledge_with_similarities[\"similarity\"] = knowledge_with_similarities[\"Embedding\"].apply(lambda x: self.similarity(query_embedding, x))\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(query_embedding_response)\n",
      "\u001B[1;31mIndexError\u001B[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\n",
    "print(Query.ask_bart('When did Universities begin teaching Computer Vision?', CompVisionGPT, show_source=True))  # What if the GPT knowledge sections are longer than 1024 tokens?? Need to account for this!"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
