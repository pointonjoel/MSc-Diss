{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GPT-3.5-Turbo Model\n",
    "Creating a question answering chatbot using GPT-3.5. Adapted from: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_tokens() IS OFTEN USED IN KNOWLEDGE.PY (and other files??) WITHOUT SPECIFYING THE EMBEDDING MODEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"modules\")\n",
    "from modules.config import *\n",
    "from modules.knowledge import *\n",
    "from modules.chatbot import *\n",
    "from modules.embedding_functions import *\n",
    "from modules.query import *\n",
    "sys.path.remove(\"modules\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filename = 'assets/knowledge/Fundamentals_of_Digital_Image_Processing_Textbook.pdf'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CompVisionKnowledge = Knowledge('CompVisionPDF', 'GPT')\n",
    "CompVisionKnowledge.append_pdf(filename, 'CompVisionPDF') # Need to debug this!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "               Source Heading Subheading  \\\n0   Comp Vision Stuff       0        NaN   \n0   Comp Vision Stuff       0        NaN   \n0   Comp Vision Stuff       0        NaN   \n0   Comp Vision Stuff       0        NaN   \n0   Comp Vision Stuff       1        NaN   \n..                ...     ...        ...   \n0   Comp Vision Stuff      31        NaN   \n0   Comp Vision Stuff      31        NaN   \n0   Comp Vision Stuff      31        NaN   \n0   Comp Vision Stuff      31        NaN   \n0   Comp Vision Stuff      31        NaN   \n\n                                              Content  \n0                                  GLOBAL \\nEDITION\\n  \n0                          Digital Image Processing\\n  \n0                                    FOURTH EDITION\\n  \n0             Rafael C. Gonzalez * Richard E. Woods\\n  \n0    Support Package for Digital \\nImage Processing\\n  \n..                                                ...  \n0   7\\nShort-wave infrared\\n2.09-2.35\\nMineral map...  \n0   TABLE 1.1\\nThematic bands \\nof NASA's \\nLANDSA...  \n0                                           1\\n2\\n3\\n  \n0                                        4\\n5\\n6\\n7\\n  \n0   FIGURE 1.10 LANDSAT satellite images of the Wa...  \n\n[206 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Source</th>\n      <th>Heading</th>\n      <th>Subheading</th>\n      <th>Content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>GLOBAL \\nEDITION\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Digital Image Processing\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>FOURTH EDITION\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Rafael C. Gonzalez * Richard E. Woods\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>Support Package for Digital \\nImage Processing\\n</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>31</td>\n      <td>NaN</td>\n      <td>7\\nShort-wave infrared\\n2.09-2.35\\nMineral map...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>31</td>\n      <td>NaN</td>\n      <td>TABLE 1.1\\nThematic bands \\nof NASA's \\nLANDSA...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>31</td>\n      <td>NaN</td>\n      <td>1\\n2\\n3\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>31</td>\n      <td>NaN</td>\n      <td>4\\n5\\n6\\n7\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>31</td>\n      <td>NaN</td>\n      <td>FIGURE 1.10 LANDSAT satellite images of the Wa...</td>\n    </tr>\n  </tbody>\n</table>\n<p>206 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = fitz.open(filename)\n",
    "output = []\n",
    "def get_blank_knowledge_df() -> pd.DataFrame:\n",
    "    return pd.DataFrame(columns=['Source', 'Heading', 'Subheading', 'Content'])\n",
    "document_name = 'Comp Vision Stuff'\n",
    "page_limit = 31\n",
    "content = get_blank_knowledge_df()\n",
    "for page in doc:\n",
    "    page_limit = doc.page_count if not page_limit else page_limit\n",
    "    if page.number <= page_limit:\n",
    "        block_content = page.get_text(\"blocks\")\n",
    "        for block in block_content:\n",
    "                if block[6] == 0: # I.e. only extract text\n",
    "                    plain_text = unidecode(block[4])\n",
    "                    new_row = {'Source': document_name, 'Heading': page.number, 'Content': plain_text}\n",
    "                    content = pd.concat([content, pd.DataFrame.from_records([new_row])])\n",
    "    else:\n",
    "        pass\n",
    "import re\n",
    "# cleaned_text = re.sub(r'http\\S+', '', all_text)\n",
    "# cleaned_text = re.sub(r'www.+', '', cleaned_text)\n",
    "content['Content'] = content['Content'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "content['Content'] = content['Content'].apply(lambda x: re.sub(r'www.+', '', x))\n",
    "content = content.loc[content['Content']!='\\n']\n",
    "content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "               Source Heading Subheading  \\\n0   Comp Vision Stuff     {0}        NaN   \n0   Comp Vision Stuff     {0}        NaN   \n0   Comp Vision Stuff     {0}        NaN   \n0   Comp Vision Stuff     {0}        NaN   \n0   Comp Vision Stuff     {0}        NaN   \n..                ...     ...        ...   \n0   Comp Vision Stuff  {1021}        NaN   \n0   Comp Vision Stuff  {1021}        NaN   \n0   Comp Vision Stuff  {1021}        NaN   \n0   Comp Vision Stuff  {1021}        NaN   \n0   Comp Vision Stuff  {1021}        NaN   \n\n                                              Content  \n0                                \\nGLOBAL \\nEDITION\\n  \n0                        \\nDigital Image Processing\\n  \n0                                 \\n FOURTH EDITION\\n  \n0          \\n Rafael C. Gonzalez * Richard E. Woods\\n  \n0                              \\nwww.EBooksWorld.ir\\n  \n..                                                ...  \n0                   \\n18    Chapter 1  Introduction\\n  \n0         \\n1.1 WHAT IS DIGITAL IMAGE PROCESSING?  \\n  \n0   \\nAn image may be defined as a two-dimensional...  \n0                                             \\n1.1\\n  \n0                              \\nwww.EBooksWorld.ir\\n  \n\n[5168 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Source</th>\n      <th>Heading</th>\n      <th>Subheading</th>\n      <th>Content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{0}</td>\n      <td>NaN</td>\n      <td>\\nGLOBAL \\nEDITION\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{0}</td>\n      <td>NaN</td>\n      <td>\\nDigital Image Processing\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{0}</td>\n      <td>NaN</td>\n      <td>\\n FOURTH EDITION\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{0}</td>\n      <td>NaN</td>\n      <td>\\n Rafael C. Gonzalez * Richard E. Woods\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{0}</td>\n      <td>NaN</td>\n      <td>\\nwww.EBooksWorld.ir\\n</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{1021}</td>\n      <td>NaN</td>\n      <td>\\n18    Chapter 1  Introduction\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{1021}</td>\n      <td>NaN</td>\n      <td>\\n1.1 WHAT IS DIGITAL IMAGE PROCESSING?  \\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{1021}</td>\n      <td>NaN</td>\n      <td>\\nAn image may be defined as a two-dimensional...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{1021}</td>\n      <td>NaN</td>\n      <td>\\n1.1\\n</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Comp Vision Stuff</td>\n      <td>{1021}</td>\n      <td>NaN</td>\n      <td>\\nwww.EBooksWorld.ir\\n</td>\n    </tr>\n  </tbody>\n</table>\n<p>5168 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "'GLOBAL \\nEDITION\\nDigital Image Processing\\n FOURTH EDITION\\n Rafael C. Gonzalez * Richard E. Woods\\n\\nYour new textbook provides access to support packages that may include reviews in areas \\nlike probability and vectors, tutorials on topics relevant to the material in the book, an image \\ndatabase, and more. Refer to the Preface in the textbook for a detailed list of resources.\\nFollow the instructions below to register for the Companion Website for Rafael C. Gonzalez and \\nRichard E. Woods\\' Digital Image Processing, Fourth Edition, Global Edition.\\n1. Go to \\n2. Find the title of your textbook.\\n3.  Click Support Materials and follow the on-screen instructions to create a login name and \\npassword.\\nUse the login name and password you created during registration to start using the \\ndigital resources that accompany your textbook.\\nIMPORTANT:\\nThis serial code can only be used once. This subscription is not transferrable.\\n\\nProcessing\\nigital Image\\n4\\nD\\nF O U R T H\\nE D I T I O N\\nRafael C. Gonzalez\\nUniversity of Tennessee\\nRichard E. Woods\\nInterapptics\\n330 Hudson Street, New York, NY 10013\\nGlobal Edition\\n\\nMATLAB is a registered trademark of The MathWorks, Inc., 1 Apple Hill Drive, Natick, MA 01760-2098.\\nPearson Education Limited\\nEdinburgh Gate\\nHarlow\\nEssex CM20 2JE\\nEngland\\nand Associated Companies throughout the world\\nVisit us on the World Wide Web at: \\n\\n(c) Pearson Education Limited 2018\\nThe rights of Rafael C. Gonzalez and Richard E. Woods to be identified as the authors of this work have been asserted by them \\nin accordance with the Copyright, Designs and Patents Act 1988.\\nAuthorized adaptation from the United States edition, entitled Digital Image Processing, Fourth Edition, ISBN 978-0-13-335672-4, \\nby Rafael C. Gonzalez and Richard E. Woods, published by Pearson Education (c) 2018.\\nAll rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by \\nany means, electronic, mechanical, photocopying, recording or otherwise, without either the prior written permission of the pub-\\nlisher or a license permitting restricted copying in the United Kingdom issued by the Copyright Licensing Agency Ltd, Saffron \\nHouse, 6-10 Kirby Street, London EC1N 8TS.\\nAll trademarks used herein are the property of their respective owners. The use of any trademark in this text does not vest in the \\nauthor or publisher any trademark ownership rights in such trademarks, nor does the use of such trademarks imply any affiliation \\nwith or endorsement of this book by such owners.\\nBritish Library Cataloguing-in-Publication Data\\nA catalogue record for this book is available from the British Library\\n10 9 8 7 6 5 4 3 2 1\\nISBN 10: 1-292-22304-9\\nISBN 13: 978-1-292-22304-9\\nTypeset by Richard E. Woods\\nPrinted and bound in Malaysia\\n\\n\\n\\nPreface  9\\nAcknowledgments  12\\nThe Book Website  13\\nThe DIP4E Support Packages  13\\nAbout the Authors  14\\n1 Introduction  17\\nWhat is Digital Image Processing?  18\\nThe Origins of Digital Image Processing  19\\nExamples of Fields that Use Digital Image Processing  23\\nFundamental Steps in Digital Image Processing  41\\nComponents of an Image Processing System  44\\n2 Digital Image Fundamentals  47\\nElements of Visual Perception  48\\nLight and the Electromagnetic Spectrum  54\\nImage Sensing and Acquisition  57\\nImage Sampling and Quantization  63\\nSome Basic Relationships Between Pixels  79\\nIntroduction to the Basic Mathematical Tools Used in Digital Image \\nProcessing  83\\n3 Intensity Transformations and Spatial  \\nFiltering  119\\nBackground  120\\nSome Basic Intensity Transformation Functions  122\\nHistogram Processing  133\\nFundamentals of Spatial Filtering  153\\nSmoothing (Lowpass) Spatial Filters  164\\nSharpening (Highpass) Spatial Filters  175\\nHighpass, Bandreject, and Bandpass Filters from Lowpass Filters  188\\nCombining Spatial Enhancement Methods  191\\n\\nBackground  204\\nPreliminary Concepts  207\\nSampling and the Fourier Transform of Sampled  \\nFunctions  215\\nThe Discrete Fourier Transform of One Variable  225\\nExtensions to Functions of Two Variables  230\\nSome Properties of the 2-D DFT and IDFT  240\\nThe Basics of Filtering in the Frequency Domain  260\\nImage Smoothing Using Lowpass Frequency Domain  \\nFilters  272\\nImage Sharpening Using Highpass Filters  284\\nSelective Filtering  296\\nThe Fast Fourier Transform  303\\n5 Image Restoration \\nand Reconstruction  317\\nA Model of the Image Degradation/Restoration  \\nprocess  318\\nNoise Models  318\\nRestoration in the Presence of Noise Only--Spatial Filtering  327\\nPeriodic Noise Reduction Using Frequency Domain Filtering  340\\nLinear, Position-Invariant Degradations  348\\nEstimating the Degradation Function  352\\nInverse Filtering  356\\nMinimum Mean Square Error (Wiener) Filtering  358\\nConstrained Least Squares Filtering  363\\nGeometric Mean Filter  367\\nImage Reconstruction from Projections  368\\n6 Color Image Processing  399\\nColor Fundamentals  400\\nColor Models  405\\nPseudocolor Image Processing  420\\nBasics of Full-Color Image Processing  429\\nColor Transformations  430\\n\\nColor Image Smoothing and Sharpening  442\\nUsing Color in Image Segmentation  445\\nNoise in Color Images  452\\nColor Image Compression  455\\n7 Wavelet and Other Image Transforms  463\\nPreliminaries  464\\nMatrix-based Transforms  466\\nCorrelation  478\\nBasis Functions in the Time-Frequency Plane  479\\nBasis Images  483\\nFourier-Related Transforms  484\\nWalsh-Hadamard Transforms  496\\nSlant Transform  500\\nHaar Transform  502\\nWavelet Transforms  504\\n8 Image Compression and  \\nWatermarking  539\\nFundamentals  540\\nHuffman Coding  553\\nGolomb Coding  556\\nArithmetic Coding  561\\nLZW Coding  564\\nRun-length Coding  566\\nSymbol-based Coding  572\\nBit-plane Coding  575\\nBlock Transform Coding  576\\nPredictive Coding  594\\nWavelet Coding  614\\nDigital Image Watermarking  624\\n9 Morphological Image Processing  635\\nPreliminaries  636\\nErosion and Dilation  638\\nOpening and Closing  644\\nThe Hit-or-Miss Transform  648\\n\\nSome Basic Morphological Algorithms  652\\nMorphological Reconstruction  667\\nSummary of Morphological Operations on Binary Images  673\\nGrayscale Morphology  674\\n10 Image Segmentation  699\\nFundamentals  700\\nPoint, Line, and Edge Detection  701\\nThresholding  742\\nSegmentation by Region Growing and by Region Splitting and \\nMerging  764\\nRegion Segmentation Using Clustering and  \\nSuperpixels  770\\nRegion Segmentation Using Graph Cuts  777\\nSegmentation Using Morphological Watersheds  786\\nThe Use of Motion in Segmentation  796\\n11 Feature Extraction   811\\nBackground  812\\nBoundary Preprocessing  814\\nBoundary Feature Descriptors  831\\nRegion Feature Descriptors  840\\nPrincipal Components as Feature Descriptors  859\\nWhole-Image Features  868\\nScale-Invariant Feature Transform (SIFT)  881\\n12 Image Pattern Classification  903\\nBackground  904\\nPatterns and Pattern Classes  906\\nPattern Classification by Prototype Matching  910\\nOptimum (Bayes) Statistical Classifiers  923\\nNeural Networks and Deep Learning  931\\nDeep Convolutional Neural Networks  964\\nSome Additional Details of Implementation  987\\nBibliography  995\\nIndex  1009\\n\\nWhen something can be read without effort, great effort has gone into its writing.\\nEnrique Jardiel Poncela\\nThis edition of Digital Image Processing is a major revision of the book. As in \\nthe 1977 and 1987 editions by Gonzalez and Wintz, and the 1992, 2002, and 2008  \\neditions by Gonzalez and Woods, this sixth-generation edition was prepared \\nwith students and instructors in mind. The principal objectives of the book \\ncontinue to be to provide an introduction to basic concepts and methodologies \\napplicable to digital image processing, and to develop a foundation that can \\nbe used as the basis for further study and research in this field. To achieve \\nthese objectives, we focused again on material that we believe is fundamental \\nand whose scope of application is not limited to the solution of specialized \\nproblems. The mathematical complexity of the book remains at a level well \\nwithin the grasp of college seniors and first-year graduate students who have \\nintroductory preparation in mathematical analysis, vectors, matrices, probability, \\nstatistics, linear systems, and computer programming. The book website pro-\\nvides tutorials to support readers needing a review of this background material. \\nOne of the principal reasons this book has been the world leader in its field for \\n40 years is the level of attention we pay to the changing educational needs of our \\nreaders. The present edition is based on an extensive survey that involved faculty, \\nstudents, and independent readers of the book in 150 institutions from 30 countries. \\nThe survey revealed a need for coverage of new material that has matured since the \\nlast edition of the book. The principal findings of the survey indicated a need for: \\n* Expanded coverage of the fundamentals of spatial filtering.\\n* A more comprehensive and cohesive coverage of image transforms.\\n* A more complete presentation of finite differences, with a focus on edge detec-\\ntion.\\n* A discussion of clustering, superpixels, and their use in region segmentation. \\n* Coverage of maximally stable extremal regions.\\n* Expanded coverage of feature extraction to include the Scale Invariant Feature \\nTransform (SIFT).\\n* Expanded coverage of neural networks to include deep neural networks, back-\\npropagation, deep learning, and, especially, deep convolutional neural networks. \\n* More homework exercises at the end of the chapters.\\nThe new and reorganized material that resulted in the present edition is our \\nattempt at providing a reasonable balance between rigor, clarity of presentation, \\nand the findings of the survey. In addition to new material, earlier portions of the \\ntext were updated and clarified. This edition contains 241 new images, 72 new draw-\\nings, and 135 new exercises.\\n\\nNew to This Edition\\nThe highlights of this edition are as follows.\\nChapter 1: Some figures were updated, and parts of the text were rewritten to cor-\\nrespond to changes in later chapters.\\nChapter 2: Many of the sections and examples were rewritten for clarity. We \\nadded 14 new exercises. \\nChapter 3: Fundamental concepts of spatial filtering were rewritten to include a \\ndiscussion on separable filter kernels, expanded coverage of the properties of low-\\npass Gaussian kernels, and expanded coverage of highpass, bandreject, and band-\\npass filters, including numerous new examples that illustrate their use. In addition to \\nrevisions in the text, including 6 new examples, the chapter has 59 new images, 2 new \\nline drawings, and 15 new exercises.\\nChapter 4: Several of the sections of this chapter were revised to improve the clar-\\nity of presentation. We replaced dated graphical material with 35 new images and 4 \\nnew line drawings. We added 21 new exercises. \\nChapter 5: Revisions to this chapter were limited to clarifications and a few cor-\\nrections in notation. We added 6 new images and 14 new exercises, \\nChapter 6: Several sections were clarified, and the explanation of the CMY and \\nCMYK color models was expanded, including 2 new images.\\nChapter 7: This is a new chapter that brings together wavelets, several new trans-\\nforms, and many of the image transforms that were scattered throughout the book. \\nThe emphasis of this new chapter is on the presentation of these transforms from a \\nunified point of view.  We added 24 new images, 20 new drawings, and 25 new exer-\\ncises. \\nChapter 8: The material was revised with numerous clarifications and several \\nimprovements to the presentation.\\nChapter 9: Revisions of this chapter included a complete rewrite of several sec-\\ntions, including redrafting of several line drawings. We added 16 new exercises\\nChapter 10: Several of the sections were rewritten for clarity. We updated the \\nchapter by adding coverage of finite differences, K-means clustering, superpixels, \\nand graph cuts. The new topics are illustrated with 4 new examples. In total, we \\nadded 29 new images, 3 new drawings, and 6 new exercises.\\nChapter 11: The chapter was updated with numerous topics, beginning with a more \\ndetailed classification of feature types and their uses. In addition to improvements in \\nthe clarity of presentation, we added coverage of slope change codes, expanded the \\nexplanation of skeletons, medial axes, and the distance transform, and added sev-\\neral new basic descriptors of compactness, circularity, and eccentricity. New mate-\\nrial includes coverage of the Harris-Stephens corner detector, and a presentation of \\nmaximally stable extremal regions. A major addition to the chapter is a comprehen-\\nsive discussion dealing with the Scale-Invariant Feature Transform (SIFT). The new \\nmaterial is complemented by 65 new images, 15 new drawings, and 12 new exercises.\\n\\nChapter 12: This chapter underwent a major revision to include an extensive \\nrewrite of neural networks and deep learning, an area that has grown significantly \\nsince the last edition of the book. We added a comprehensive discussion on fully \\nconnected, deep neural networks that includes derivation of backpropagation start-\\ning from basic principles. The equations of backpropagation were expressed in \"tra-\\nditional\" scalar terms, and then generalized into a compact set of matrix equations \\nideally suited for implementation of deep neural nets. The effectiveness of fully con-\\nnected networks was demonstrated with several examples that included a compari-\\nson with the Bayes classifier. One of the most-requested topics in the survey was \\ncoverage of deep convolutional neural networks. We added an extensive section on \\nthis, following the same blueprint we used for deep, fully connected nets. That is, we \\nderived the equations of backpropagation for convolutional nets, and showed how \\nthey are different from \"traditional\" backpropagation. We then illustrated the use of \\nconvolutional networks with simple images, and applied them to large image data-\\nbases of numerals and natural scenes.  The written material is complemented by 23 \\nnew images, 28 new drawings, and 12 new exercises.\\nAlso for the first time, we have created student and faculty support packages that \\ncan be downloaded from the book website. The Student Support Package contains \\nmany of the original images in the book and answers to selected exercises The Fac-\\nulty Support Package contains solutions to all exercises, teaching suggestions, and all \\nthe art in the book in the form of modifiable PowerPoint slides. One support pack-\\nage is made available with every new book, free of charge. \\nThe book website, established during the launch of the 2002 edition, continues to \\nbe a success, attracting more than 25,000 visitors each month. The site was upgraded \\nfor the launch of this edition. For more details on site features and content, see The \\nBook Website, following the Acknowledgments section.\\nThis edition of Digital Image Processing is a reflection of how the educational \\nneeds of our readers have changed since 2008. As is usual in an endeavor such as \\nthis, progress in the field continues after work on the manuscript stops. One of the \\nreasons why this book has been so well accepted since it first appeared in 1977 is its \\ncontinued emphasis on fundamental concepts that retain their relevance over time. \\nThis approach, among other things, attempts to provide a measure of stability in a \\nrapidly evolving body of knowledge. We have tried to follow the same principle in \\npreparing this edition of the book.\\nR.C.G.\\nR.E.W.\\n\\nWe are indebted to a number of individuals in academic circles, industry, and gov-\\nernment who have contributed to this edition of the book. In particular, we wish \\nto extend our appreciation to Hairong Qi and her students, Zhifei Zhang and \\nChengcheng Li, for their valuable review of the material on neural networks, and for \\ntheir help in generating examples for that material. We also want to thank Ernesto \\nBribiesca Correa for providing and reviewing material on slope chain codes, and \\nDirk Padfield for his many suggestions and review of several chapters in the book. \\nWe appreciate Michel Kocher\\'s many thoughtful comments and suggestions over \\nthe years on how to improve the book. Thanks also to Steve Eddins for his sugges-\\ntions on MATLAB and related software issues.\\nNumerous individuals have contributed to material carried over from the previ-\\nous to the current edition of the book. Their contributions have been important in so \\nmany different ways that we find it difficult to acknowledge them in any other way \\nbut alphabetically. We thank Mongi A. Abidi, Yongmin Kim, Bryan Morse, Andrew \\nOldroyd, Ali M. Reza, Edgardo Felipe Riveron, Jose Ruiz Shulcloper, and Cameron \\nH.G. Wright for their many suggestions on how to improve the presentation and/or \\nthe scope of coverage in the book. We are also indebted to Naomi Fernandes at the \\nMathWorks for providing us with MATLAB software and support that were impor-\\ntant in our ability to create many of the examples and experimental results included \\nin this edition of the book.\\nA significant percentage of the new images used in this edition (and in some \\ncases their history and interpretation) were obtained through the efforts of indi-\\nviduals whose contributions are sincerely appreciated. In particular, we wish to \\nacknowledge the efforts of Serge Beucher, Uwe Boos, Michael E. Casey, Michael \\nW. Davidson, Susan L. Forsburg, Thomas R. Gest, Daniel A. Hammer, Zhong He, \\nRoger Heady, Juan A. Herrera, John M. Hudak, Michael Hurwitz, Chris J. Johannsen, \\nRhonda Knighton, Don P. Mitchell, A. Morris, Curtis C. Ober, David. R. Pickens, \\nMichael Robinson, Michael Shaffer, Pete Sites, Sally Stowe, Craig Watson, David \\nK. Wehe, and Robert A. West. We also wish to acknowledge other individuals and \\norganizations cited in the captions of numerous figures throughout the book for \\ntheir permission to use that material. \\nWe also thank Scott Disanno, Michelle Bayman, Rose Kernan, and Julie Bai for \\ntheir support and significant patience during the production of the book.\\nR.C.G.\\nR.E.W.\\n\\n\\nDigital Image Processing is a completely self-contained book. However, the compan-\\nion website offers additional support in a number of important areas.\\nFor the Student or Independent Reader the site contains\\n* Reviews in areas such as probability, statistics, vectors, and matrices.\\n* A Tutorials section containing dozens of tutorials on topics relevant to the mate-\\nrial in the book.\\n* An image database containing all the images in the book, as well as many other \\nimage databases.\\nFor the Instructor the site contains\\n* An Instructor\\'s Manual with complete solutions to all the problems.\\n* Classroom presentation materials in modifiable PowerPoint format.\\n* Material removed from previous editions, downloadable in convenient PDF \\nformat.\\n* Numerous links to other educational resources.\\nFor the Practitioner the site contains additional specialized topics such as\\n* Links to commercial sites.\\n* Selected new references.\\n* Links to commercial image databases.\\nThe website is an ideal tool for keeping the book current between editions by includ-\\ning new topics, digital images, and other relevant material that has appeared after \\nthe book was published. Although considerable care was taken in the production \\nof the book, the website is also a convenient repository for any errors discovered \\nbetween printings. \\nThe DIP4E Support Packages\\nIn this edition, we created support packages for students and faculty to organize \\nall the classroom support materials available for the new edition of the book into \\none easy download. The Student Support Package contains many of the original \\nimages in the book, and answers to selected exercises, The Faculty Support Package \\ncontains solutions to all exercises, teaching suggestions, and all the art in the book \\nin modifiable PowerPoint slides. One support package is made available with every \\nnew book, free of charge. Applications for the support packages are submitted at \\nthe book website.\\n\\nRAFAEL C. GONZALEZ\\nR. C. Gonzalez received the B.S.E.E. degree from the University of Miami in 1965 \\nand the M.E. and Ph.D. degrees in electrical engineering from the University of \\nFlorida, Gainesville, in 1967 and 1970, respectively. He joined the Electrical and \\nComputer Science Department at the University of Tennessee, Knoxville (UTK) in \\n1970, where he became Associate Professor in 1973, Professor in 1978, and Distin-\\nguished Service Professor in 1984. He served as Chairman of the department from \\n1994 through 1997. He is currently a Professor Emeritus at UTK.\\nGonzalez is the founder of the Image & Pattern Analysis Laboratory and the \\nRobotics & Computer Vision Laboratory at the University of Tennessee. He also \\nfounded Perceptics Corporation in 1982 and was its president until 1992. The last \\nthree years of this period were spent under a full-time employment contract with \\nWestinghouse Corporation, who acquired the company in 1989. \\nUnder his direction, Perceptics became highly successful in image processing, \\ncomputer vision, and laser disk storage technology. In its initial ten years, Perceptics \\nintroduced a series of innovative products, including: The world\\'s first commercially \\navailable computer vision system for automatically reading license plates on moving \\nvehicles; a series of large-scale image processing and archiving systems used by the \\nU.S. Navy at six different manufacturing sites throughout the country to inspect the \\nrocket motors of missiles in the Trident II Submarine Program; the market-leading \\nfamily of imaging boards for advanced Macintosh computers; and a line of trillion-\\nbyte laser disk products.\\nHe is a frequent consultant to industry and government in the areas of pattern \\nrecognition, image processing, and machine learning. His academic honors for work \\nin these fields include the 1977 UTK College of Engineering Faculty Achievement \\nAward; the 1978 UTK Chancellor\\'s Research Scholar Award; the 1980 Magnavox \\nEngineering Professor Award; and the 1980 M.E. Brooks Distinguished Professor \\nAward. In 1981 he became an IBM Professor at the University of Tennessee and \\nin 1984 he was named a Distinguished Service Professor there. He was awarded a \\nDistinguished Alumnus Award by the University of Miami in 1985, the Phi Kappa \\nPhi Scholar Award in 1986, and the University of Tennessee\\'s Nathan W. Dougherty \\nAward for Excellence in Engineering in 1992.\\nHonors for industrial accomplishment include the 1987 IEEE Outstanding Engi-\\nneer Award for Commercial Development in Tennessee; the 1988 Albert Rose \\nNational Award for Excellence in Commercial Image Processing; the 1989 B. Otto \\nWheeley Award for Excellence in Technology Transfer; the 1989 Coopers and \\nLybrand Entrepreneur of the Year Award; the 1992 IEEE Region 3 Outstanding \\nEngineer Award; and the 1993 Automated Imaging Association National Award for \\nTechnology Development.\\nGonzalez is author or co-author of over 100 technical articles, two edited books, \\nand four textbooks in the fields of pattern recognition, image processing, and robot-\\nics. His books are used in over 1000 universities and research institutions throughout \\n\\nRICHARD E. WOODS\\nR. E. Woods earned his B.S., M.S., and Ph.D. degrees in Electrical Engineering from \\nthe University of Tennessee, Knoxville in 1975, 1977, and 1980, respectively. He \\nbecame an Assistant Professor of Electrical Engineering and Computer Science in \\n1981 and was recognized as a Distinguished Engineering Alumnus in 1986.\\nA veteran hardware and software developer, Dr. Woods has been involved in \\nthe founding of several high-technology startups, including Perceptics Corporation, \\nwhere he was responsible for the development of the company\\'s quantitative image \\nanalysis and autonomous decision-making products; MedData Interactive, a high-\\ntechnology company specializing in the development of handheld computer systems \\nfor medical applications; and Interapptics, an internet-based company that designs \\ndesktop and handheld computer applications.\\nDr. Woods currently serves on several nonprofit educational and media-related \\nboards, including Johnson University, and was recently a summer English instructor \\nat the Beijing Institute of Technology. He is the holder of a U.S. Patent in the area \\nof digital image processing and has published two textbooks, as well as numerous \\narticles related to digital signal processing. Dr. Woods is a member of several profes-\\nsional societies, including Tau Beta Pi, Phi Kappa Phi, and the IEEE.\\n\\n\\n17\\n1\\nIntroduction\\nOne picture is worth more than ten thousand words.\\nAnonymous\\nPreview\\nInterest in digital image processing methods stems from two principal application areas: improvement \\nof pictorial information for human interpretation, and processing of image data for tasks such as storage, \\ntransmission, and extraction of pictorial information. This chapter has several objectives: (1) to define \\nthe scope of the field that we call image processing; (2) to give a historical perspective of the origins of \\nthis field; (3) to present an overview of the state of the art in image processing by examining some of \\nthe principal areas in which it is applied; (4) to discuss briefly the principal approaches used in digital \\nimage processing; (5) to give an overview of the components contained in a typical, general-purpose \\nimage processing system; and (6) to provide direction to the literature where image processing work is \\nreported. The material in this chapter is extensively illustrated with a range of images that are represen-\\ntative of the images we will be using throughout the book.\\nUpon completion of this chapter, readers should:\\n Understand the concept of a digital image.\\n Have a broad overview of the historical under-\\npinnings of the field of digital image process-\\ning.\\n Understand the definition and scope of digi-\\ntal image processing.\\n Know the fundamentals of the electromag-\\nnetic spectrum and its relationship to image \\ngeneration.\\n Be aware of the different fields in which digi-\\ntal image processing methods are applied.\\n Be familiar with the basic processes involved \\nin image processing.\\n Be familiar with the components that make \\nup a general-purpose digital image process-\\ning system.\\n Be familiar with the scope of the literature \\nwhere image processing work is reported.\\n\\n1.1 WHAT IS DIGITAL IMAGE PROCESSING?  \\nAn image may be defined as a two-dimensional function, f x y\\n( , ), where x and y are \\nspatial (plane) coordinates, and the amplitude of f at any pair of coordinates ( , )\\nx y  \\nis called the intensity or gray level of the image at that point. When x, y, and the \\nintensity values of f are all finite, discrete quantities, we call the image a digital image. \\nThe field of digital image processing refers to processing digital images by means of \\na digital computer. Note that a digital image is composed of a finite number of ele-\\nments, each of which has a particular location and value. These elements are called \\npicture elements, image elements, pels, and pixels. Pixel is the term used most widely \\nto denote the elements of a digital image. We will consider these definitions in more \\nformal terms in Chapter 2.\\nVision is the most advanced of our senses, so it is not surprising that images \\nplay the single most important role in human perception. However, unlike humans, \\nwho are limited to the visual band of the electromagnetic (EM) spectrum, imaging \\nmachines cover almost the entire EM spectrum, ranging from gamma to radio waves. \\nThey can operate on images generated by sources that humans are not accustomed \\nto associating with images. These include ultrasound, electron microscopy, and com-\\nputer-generated images. Thus, digital image processing encompasses a wide and var-\\nied field of applications.\\nThere is no general agreement among authors regarding where image process-\\ning stops and other related areas, such as image analysis and computer vision, start. \\nSometimes, a distinction is made by defining image processing as a discipline in \\nwhich both the input and output of a process are images. We believe this to be a \\nlimiting and somewhat artificial boundary. For example, under this definition, even \\nthe trivial task of computing the average intensity of an image (which yields a sin-\\ngle number) would not be considered an image processing operation. On the other \\nhand, there are fields such as computer vision whose ultimate goal is to use comput-\\ners to emulate human vision, including learning and being able to make inferences \\nand take actions based on visual inputs. This area itself is a branch of artificial intel-\\nligence (AI) whose objective is to emulate human intelligence. The field of AI is in its \\nearliest stages of infancy in terms of development, with progress having been much \\nslower than originally anticipated. The area of image analysis (also called image \\nunderstanding) is in between image processing and computer vision.\\nThere are no clear-cut boundaries in the continuum from image processing at \\none end to computer vision at the other. However, one useful paradigm is to con-\\nsider three types of computerized processes in this continuum: low-, mid-, and high-\\nlevel processes. Low-level processes involve primitive operations such as image \\npreprocessing to reduce noise, contrast enhancement, and image sharpening. A low-\\nlevel process is characterized by the fact that both its inputs and outputs are images. \\nMid-level processing of images involves tasks such as segmentation (partitioning \\nan image into regions or objects), description of those objects to reduce them to a \\nform suitable for computer processing, and classification (recognition) of individual \\nobjects. A mid-level process is characterized by the fact that its inputs generally \\nare images, but its outputs are attributes extracted from those images (e.g., edges, \\ncontours, and the identity of individual objects). Finally, higher-level processing \\n1.1\\n\\ninvolves \"making sense\" of an ensemble of recognized objects, as in image analysis, \\nand, at the far end of the continuum, performing the cognitive functions normally \\nassociated with human vision.\\nBased on the preceding comments, we see that a logical place of overlap between \\nimage processing and image analysis is the area of recognition of individual regions \\nor objects in an image. Thus, what we call in this book digital image processing encom-\\npasses processes whose inputs and outputs are images and, in addition, includes pro-\\ncesses that extract attributes from images up to, and including, the recognition of \\nindividual objects. As an illustration to clarify these concepts, consider the area of \\nautomated analysis of text. The processes of acquiring an image of the area con-\\ntaining the text, preprocessing that image, extracting (segmenting) the individual \\ncharacters, describing the characters in a form suitable for computer processing, and \\nrecognizing those individual characters are in the scope of what we call digital image \\nprocessing in this book. Making sense of the content of the page may be viewed as \\nbeing in the domain of image analysis and even computer vision, depending on the \\nlevel of complexity implied by the statement \"making sense of.\" As will become \\nevident shortly, digital image processing, as we have defined it, is used routinely in a \\nbroad range of areas of exceptional social and economic value. The concepts devel-\\noped in the following chapters are the foundation for the methods used in those \\napplication areas.\\n1.2 THE ORIGINS OF DIGITAL IMAGE PROCESSING  \\nOne of the earliest applications of digital images was in the newspaper industry, \\nwhen pictures were first sent by submarine cable between London and New York. \\nIntroduction of the Bartlane cable picture transmission system in the early 1920s \\nreduced the time required to transport a picture across the Atlantic from more than \\na week to less than three hours. Specialized printing equipment coded pictures for \\ncable transmission, then reconstructed them at the receiving end. Figure 1.1 was \\ntransmitted in this way and reproduced on a telegraph printer fitted with typefaces \\nsimulating a halftone pattern. \\nSome of the initial problems in improving the visual quality of these early digital \\npictures were related to the selection of printing procedures and the distribution of \\n1.2\\nFIGURE 1.1  A digital picture produced in 1921 from a coded tape by a telegraph printer with \\nspecial typefaces. (McFarlane.) [References in the bibliography at the end of the book are \\nlisted in alphabetical order by authors\\' last names.]\\n\\nintensity levels. The printing method used to obtain Fig. 1.1 was abandoned toward \\nthe end of 1921 in favor of a technique based on photographic reproduction made \\nfrom tapes perforated at the telegraph receiving terminal. Figure 1.2 shows an image \\nobtained using this method. The improvements over Fig. 1.1 are evident, both in \\ntonal quality and in resolution.\\nThe early Bartlane systems were capable of coding images in five distinct levels \\nof gray. This capability was increased to 15 levels in 1929. Figure 1.3 is typical of the \\ntype of images that could be obtained using the 15-tone equipment. During this \\nperiod, introduction of a system for developing a film plate via light beams that were \\nmodulated by the coded picture tape improved the reproduction process consider-\\nably.\\nAlthough the examples just cited involve digital images, they are not considered \\ndigital image processing results in the context of our definition, because digital com-\\nputers were not used in their creation. Thus, the history of digital image processing \\nis intimately tied to the development of the digital computer. In fact, digital images \\nrequire so much storage and computational power that progress in the field of digi-\\ntal image processing has been dependent on the development of digital computers \\nand of supporting technologies that include data storage, display, and transmission.\\nFIGURE 1.2\\nA digital picture \\nmade in 1922 \\nfrom a tape \\npunched after \\nthe signals had \\ncrossed the  \\nAtlantic twice. \\n(McFarlane.)\\nFIGURE 1.3\\nUnretouched \\ncable picture of \\nGenerals Pershing \\n(right) and Foch,  \\ntransmitted in \\n1929 from  \\nLondon to New \\nYork by 15-tone \\nequipment. \\n(McFarlane.)\\n\\nThe concept of a computer dates back to the invention of the abacus in Asia \\nMinor, more than 5000 years ago. More recently, there have been developments \\nin the past two centuries that are the foundation of what we call a computer today. \\nHowever, the basis for what we call a modern digital computer dates back to only \\nthe 1940s, with the introduction by John von Neumann of two key concepts: (1) a \\nmemory to hold a stored program and data, and (2) conditional branching. These \\ntwo ideas are the foundation of a central processing unit (CPU), which is at the heart \\nof computers today. Starting with von Neumann, there were a series of key advanc-\\nes that led to computers powerful enough to be used for digital image processing. \\nBriefly, these advances may be summarized as follows: (1) the invention of the tran-\\nsistor at Bell Laboratories in 1948; (2) the development in the 1950s and 1960s of \\nthe high-level programming languages COBOL (Common Business-Oriented Lan-\\nguage) and FORTRAN (Formula Translator); (3) the invention of the integrated \\ncircuit (IC) at Texas Instruments in 1958; (4) the development of operating systems \\nin the early 1960s; (5) the development of the microprocessor (a single chip consist-\\ning of a CPU, memory, and input and output controls) by Intel in the early 1970s; \\n(6) the introduction by IBM of the personal computer in 1981; and (7) progressive \\nminiaturization of components, starting with large-scale integration (LI) in the late \\n1970s, then very-large-scale integration (VLSI) in the 1980s, to the present use of \\nultra-large-scale integration (ULSI) and experimental nonotechnologies. Concur-\\nrent with these advances were developments in the areas of mass storage and display \\nsystems, both of which are fundamental requirements for digital image processing.\\nThe first computers powerful enough to carry out meaningful image processing \\ntasks appeared in the early 1960s. The birth of what we call digital image processing \\ntoday can be traced to the availability of those machines, and to the onset of the \\nspace program during that period. It took the combination of those two develop-\\nments to bring into focus the potential of digital image processing for solving prob-\\nlems of practical significance. Work on using computer techniques for improving \\nimages from a space probe began at the Jet Propulsion Laboratory (Pasadena, Cali-\\nfornia) in 1964, when pictures of the moon transmitted by Ranger 7 were processed \\nby a computer to correct various types of image distortion inherent in the on-board \\ntelevision camera. Figure 1.4 shows the first image of the moon taken by Ranger \\n7 on July 31, 1964 at 9:09 A.M. Eastern Daylight Time (EDT), about 17 minutes \\nbefore impacting the lunar surface (the markers, called reseau marks, are used for \\ngeometric corrections, as discussed in Chapter 2).This also is the first image of the \\nmoon taken by a U.S. spacecraft. The imaging lessons learned with Ranger 7 served \\nas the basis for improved methods used to enhance and restore images from the Sur-\\nveyor missions to the moon, the Mariner series of flyby missions to Mars, the Apollo \\nmanned flights to the moon, and others.\\nIn parallel with space applications, digital image processing techniques began in \\nthe late 1960s and early 1970s to be used in medical imaging, remote Earth resourc-\\nes observations, and astronomy. The invention in the early 1970s of computerized \\naxial tomography (CAT), also called computerized tomography (CT) for short, is \\none of the most important events in the application of image processing in medical \\ndiagnosis. Computerized axial tomography is a process in which a ring of detectors \\n\\nencircles an object (or patient) and an X-ray source, concentric with the detector \\nring, rotates about the object. The X-rays pass through the object and are collected \\nat the opposite end by the corresponding detectors in the ring. This procedure is \\nrepeated the source rotates. Tomography consists of algorithms that use the sensed \\ndata to construct an image that represents a \"slice\" through the object. Motion of \\nthe object in a direction perpendicular to the ring of detectors produces a set of \\nsuch slices, which constitute a three-dimensional (3-D) rendition of the inside of the \\nobject. Tomography was invented independently by Sir Godfrey N. Hounsfield and \\nProfessor Allan M. Cormack, who shared the 1979 Nobel Prize in Medicine for their \\ninvention. It is interesting to note that X-rays were discovered in 1895 by Wilhelm \\nConrad Roentgen, for which he received the 1901 Nobel Prize for Physics. These two \\ninventions, nearly 100 years apart, led to some of the most important applications of \\nimage processing today.\\nFrom the 1960s until the present, the field of image processing has grown vigor-\\nously. In addition to applications in medicine and the space program, digital image \\nprocessing techniques are now used in a broad range of applications. Computer pro-\\ncedures are used to enhance the contrast or code the intensity levels into color for \\neasier interpretation of X-rays and other images used in industry, medicine, and the \\nbiological sciences. Geographers use the same or similar techniques to study pollu-\\ntion patterns from aerial and satellite imagery. Image enhancement and restoration \\nprocedures are used to process degraded images of unrecoverable objects, or experi-\\nmental results too expensive to duplicate. In archeology, image processing meth-\\nods have successfully restored blurred pictures that were the only available records \\nof rare artifacts lost or damaged after being photographed. In physics and related \\nfields, computer techniques routinely enhance images of experiments in areas such \\nas high-energy plasmas and electron microscopy. Similarly successful applications \\nof image processing concepts can be found in astronomy, biology, nuclear medicine, \\nlaw enforcement, defense, and industry.\\nFIGURE 1.4\\nThe first picture \\nof the moon by \\na U.S. spacecraft. \\nRanger 7 took \\nthis image on \\nJuly 31, 1964 at \\n9:09 A.M. EDT, \\nabout 17 minutes \\nbefore impacting \\nthe lunar surface. \\n(Courtesy of \\nNASA.) \\n\\nThese examples illustrate processing results intended for human interpretation. \\nThe second major area of application of digital image processing techniques men-\\ntioned at the beginning of this chapter is in solving problems dealing with machine \\nperception. In this case, interest is on procedures for extracting information from \\nan image, in a form suitable for computer processing. Often, this information bears \\nlittle resemblance to visual features that humans use in interpreting the content \\nof an image. Examples of the type of information used in machine perception are \\nstatistical moments, Fourier transform coefficients, and multidimensional distance \\nmeasures. Typical problems in machine perception that routinely utilize image pro-\\ncessing techniques are automatic character recognition, industrial machine vision \\nfor product assembly and inspection, military recognizance, automatic processing of \\nfingerprints, screening of X-rays and blood samples, and machine processing of aer-\\nial and satellite imagery for weather prediction and environmental assessment. The \\ncontinuing decline in the ratio of computer price to performance, and the expansion \\nof networking and communication bandwidth via the internet, have created unprec-\\nedented opportunities for continued growth of digital image processing. Some of \\nthese application areas will be illustrated in the following section.\\n1.3 EXAMPLES OF FIELDS THAT USE DIGITAL IMAGE PROCESSING  \\nToday, there is almost no area of technical endeavor that is not impacted in some \\nway by digital image processing. We can cover only a few of these applications in the \\ncontext and space of the current discussion. However, limited as it is, the material \\npresented in this section will leave no doubt in your mind regarding the breadth and \\nimportance of digital image processing. We show in this section numerous areas of \\napplication, each of which routinely utilizes the digital image processing techniques \\ndeveloped in the following chapters. Many of the images shown in this section are \\nused later in one or more of the examples given in the book. Most images shown are \\ndigital images. \\nThe areas of application of digital image processing are so varied that some form \\nof organization is desirable in attempting to capture the breadth of this field. One \\nof the simplest ways to develop a basic understanding of the extent of image pro-\\ncessing applications is to categorize images according to their source (e.g., X-ray, \\nvisual, infrared, and so on).The principal energy source for images in use today is \\nthe electromagnetic energy spectrum. Other important sources of energy include \\nacoustic, ultrasonic, and electronic (in the form of electron beams used in electron \\nmicroscopy). Synthetic images, used for modeling and visualization, are generated \\nby computer. In this section we will discuss briefly how images are generated in \\nthese various categories, and the areas in which they are applied. Methods for con-\\nverting images into digital form will be discussed in the next chapter.\\nImages based on radiation from the EM spectrum are the most familiar, espe-\\ncially images in the X-ray and visual bands of the spectrum. Electromagnetic waves \\ncan be conceptualized as propagating sinusoidal waves of varying wavelengths, or \\nthey can be thought of as a stream of massless particles, each traveling in a wavelike \\npattern and moving at the speed of light. Each massless particle contains a certain \\namount (or bundle) of energy. Each bundle of energy is called a photon. If spectral \\n1.3\\n\\nbands are grouped according to energy per photon, we obtain the spectrum shown \\nin Fig. 1.5, ranging from gamma rays (highest energy) at one end to radio waves \\n(lowest energy) at the other. The bands are shown shaded to convey the fact that \\nbands of the EM spectrum are not distinct, but rather transition smoothly from one \\nto the other.\\nGAMMA-RAY IMAGING\\nMajor uses of imaging based on gamma rays include nuclear medicine and astro-\\nnomical observations. In nuclear medicine, the approach is to inject a patient with a \\nradioactive isotope that emits gamma rays as it decays. Images are produced from \\nthe emissions collected by gamma-ray detectors. Figure 1.6(a) shows an image of a \\ncomplete bone scan obtained by using gamma-ray imaging. Images of this sort are \\nused to locate sites of bone pathology, such as infections or tumors. Figure 1.6(b) \\nshows another major modality of nuclear imaging called positron emission tomogra-\\nphy (PET). The principle is the same as with X-ray tomography, mentioned briefly \\nin Section 1.2. However, instead of using an external source of X-ray energy, the \\npatient is given a radioactive isotope that emits positrons as it decays. When a pos-\\nitron meets an electron, both are annihilated and two gamma rays are given off. \\nThese are detected and a tomographic image is created using the basic principles of \\ntomography. The image shown in Fig. 1.6(b) is one sample of a sequence that con-\\nstitutes a 3-D rendition of the patient. This image shows a tumor in the brain and \\nanother in the lung, easily visible as small white masses.\\nA star in the constellation of Cygnus exploded about 15,000 years ago, generat-\\ning a superheated, stationary gas cloud (known as the Cygnus Loop) that glows in \\na spectacular array of colors. Figure 1.6(c) shows an image of the Cygnus Loop in \\nthe gamma-ray band. Unlike the two examples in Figs. 1.6(a) and (b), this image was \\nobtained using the natural radiation of the object being imaged. Finally, Fig. 1.6(d) \\nshows an image of gamma radiation from a valve in a nuclear reactor. An area of \\nstrong radiation is seen in the lower left side of the image.\\nX-RAY IMAGING\\nX-rays are among the oldest sources of EM radiation used for imaging. The best \\nknown use of X-rays is medical diagnostics, but they are also used extensively in \\nindustry and other areas, such as astronomy. X-rays for medical and industrial imag-\\ning are generated using an X-ray tube, which is a vacuum tube with a cathode and \\nanode. The cathode is heated, causing free electrons to be released. These electrons \\nflow at high speed to the positively charged anode. When the electrons strike a \\n109\\n108\\n107\\n106\\n105\\n104\\n103\\n102\\n100\\n101\\n101\\n102\\n103\\n104\\n105\\n106\\nEnergy of one photon (electron volts)\\nGamma rays\\nX-rays\\nUltraviolet Visible\\nInfrared\\nMicrowaves\\nRadio waves\\nFIGURE 1.5  The electromagnetic spectrum arranged according to energy per photon.\\n\\nnucleus, energy is released in the form of X-ray radiation. The energy (penetrat-\\ning power) of X-rays is controlled by a voltage applied across the anode, and by a \\ncurrent applied to the filament in the cathode. Figure 1.7(a) shows a familiar chest \\nX-ray generated simply by placing the patient between an X-ray source and a film \\nsensitive to X-ray energy. The intensity of the X-rays is modified by absorption as \\nthey pass through the patient, and the resulting energy falling on the film develops it, \\nmuch in the same way that light develops photographic film. In digital radiography, \\nb\\na\\nd\\nc\\nFIGURE 1.6\\nExamples of \\ngamma-ray  \\nimaging.  \\n(a) Bone scan.  \\n(b) PET image. \\n(c) Cygnus Loop. \\n(d) Gamma radia-\\ntion (bright spot) \\nfrom a reactor \\nvalve.  \\n(Images  \\ncourtesy of  \\n(a) G.E. Medical \\nSystems; (b) Dr. \\nMichael E. Casey, \\nCTI PET Systems; \\n(c) NASA;  \\n(d) Professors \\nZhong He and \\nDavid K. Wehe,  \\nUniversity of \\nMichigan.) \\n\\ndigital images are obtained by one of two methods: (1) by digitizing X-ray films; or; \\n(2) by having the X-rays that pass through the patient fall directly onto devices (such \\nas a phosphor screen) that convert X-rays to light. The light signal in turn is captured \\nby a light-sensitive digitizing system. We will discuss digitization in more detail in \\nChapters 2 and 4.\\nb\\na d\\nc\\ne\\nFIGURE 1.7\\nExamples of \\nX-ray imaging.  \\n(a) Chest X-ray. \\n(b) Aortic  \\nangiogram.  \\n(c) Head CT.  \\n(d) Circuit boards. \\n(e) Cygnus Loop. \\n(Images courtesy \\nof (a) and (c) Dr. \\nDavid R. Pickens, \\nDept. of  \\nRadiology & \\nRadiological  \\nSciences,  \\nVanderbilt  \\nUniversity  \\nMedical Center; \\n(b) Dr. Thomas \\nR. Gest, Division \\nof Anatomical \\nSciences, Univ. of \\nMichigan Medical \\nSchool;  \\n(d) Mr. Joseph \\nE. Pascente, Lixi, \\nInc.; and  \\n(e) NASA.) \\n\\nAngiography is another major application in an area called contrast enhancement \\nradiography. This procedure is used to obtain images of blood vessels, called angio-\\ngrams. A catheter (a small, flexible, hollow tube) is inserted, for example, into an \\nartery or vein in the groin. The catheter is threaded into the blood vessel and guided \\nto the area to be studied. When the catheter reaches the site under investigation, \\nan X-ray contrast medium is injected through the tube. This enhances the contrast \\nof the blood vessels and enables a radiologist to see any irregularities or blockages. \\nFigure 1.7(b) shows an example of an aortic angiogram. The catheter can be seen \\nbeing inserted into the large blood vessel on the lower left of the picture. Note the \\nhigh contrast of the large vessel as the contrast medium flows up in the direction of \\nthe kidneys, which are also visible in the image. As we will discuss further in Chapter 2, \\nangiography is a major area of digital image processing, where image subtraction is \\nused to further enhance the blood vessels being studied.\\nAnother important use of X-rays in medical imaging is computerized axial tomog-\\nraphy (CAT). Due to their resolution and 3-D capabilities, CAT scans revolution-\\nized medicine from the moment they first became available in the early 1970s. As \\nnoted in Section 1.2, each CAT image is a \"slice\" taken perpendicularly through \\nthe patient. Numerous slices are generated as the patient is moved in a longitudinal \\ndirection. The ensemble of such images constitutes a 3-D rendition of the inside of \\nthe body, with the longitudinal resolution being proportional to the number of slice \\nimages taken. Figure 1.7(c) shows a typical CAT slice image of a human head.\\nTechniques similar to the ones just discussed, but generally involving higher \\nenergy X-rays, are applicable in industrial processes. Figure 1.7(d) shows an X-ray \\nimage of an electronic circuit board. Such images, representative of literally hundreds \\nof industrial applications of X-rays, are used to examine circuit boards for flaws in \\nmanufacturing, such as missing components or broken traces. Industrial CAT scans \\nare useful when the parts can be penetrated by X-rays, such as in plastic assemblies, \\nand even large bodies, such as solid-propellant rocket motors. Figure 1.7(e) shows an \\nexample of X-ray imaging in astronomy. This image is the Cygnus Loop of Fig. 1.6(c), \\nbut imaged in the X-ray band.\\nIMAGING IN THE ULTRAVIOLET BAND\\nApplications of ultraviolet \"light\" are varied. They include lithography, industrial \\ninspection, microscopy, lasers, biological imaging, and astronomical observations. \\nWe illustrate imaging in this band with examples from microscopy and astronomy.\\nUltraviolet light is used in fluorescence microscopy, one of the fastest growing \\nareas of microscopy. Fluorescence is a phenomenon discovered in the middle of the \\nnineteenth century, when it was first observed that the mineral fluorspar fluoresces \\nwhen ultraviolet light is directed upon it. The ultraviolet light itself is not visible, but \\nwhen a photon of ultraviolet radiation collides with an electron in an atom of a fluo-\\nrescent material, it elevates the electron to a higher energy level. Subsequently, the \\nexcited electron relaxes to a lower level and emits light in the form of a lower-energy \\nphoton in the visible (red) light region. Important tasks performed with a fluores-\\ncence microscope are to use an excitation light to irradiate a prepared specimen, \\nand then to separate the much weaker radiating fluorescent light from the brighter \\n\\nexcitation light. Thus, only the emission light reaches the eye or other detector. The \\nresulting fluorescing areas shine against a dark background with sufficient contrast \\nto permit detection. The darker the background of the nonfluorescing material, the \\nmore efficient the instrument.\\nFluorescence microscopy is an excellent method for studying materials that can be \\nmade to fluoresce, either in their natural form (primary fluorescence) or when treat-\\ned with chemicals capable of fluorescing (secondary fluorescence). Figures 1.8(a) \\nand (b) show results typical of the capability of fluorescence microscopy. Figure \\n1.8(a) shows a fluorescence microscope image of normal corn, and Fig. 1.8(b) shows \\ncorn infected by \"smut,\" a disease of cereals, corn, grasses, onions, and sorghum that \\ncan be caused by any one of more than 700 species of parasitic fungi. Corn smut is \\nparticularly harmful because corn is one of the principal food sources in the world. \\nAs another illustration, Fig. 1.8(c) shows the Cygnus Loop imaged in the high-energy \\nregion of the ultraviolet band.\\nIMAGING IN THE VISIBLE AND INFRARED BANDS\\nConsidering that the visual band of the electromagnetic spectrum is the most famil-\\niar in all our activities, it is not surprising that imaging in this band outweighs by far \\nall the others in terms of breadth of application. The infrared band often is used in \\nconjunction with visual imaging, so we have grouped the visible and infrared bands \\nin this section for the purpose of illustration. We consider in the following discus-\\nsion applications in light microscopy, astronomy, remote sensing, industry, and law \\nenforcement.\\nFigure 1.9 shows several examples of images obtained with a light microscope. \\nThe examples range from pharmaceuticals and microinspection to materials char-\\nacterization. Even in microscopy alone, the application areas are too numerous to \\ndetail here. It is not difficult to conceptualize the types of processes one might apply \\nto these images, ranging from enhancement to measurements.\\nab\\nc\\nFIGURE 1.8  Examples of ultraviolet imaging. (a) Normal corn. (b) Corn infected by smut. (c) Cygnus Loop. (Images \\n(a) and (b) courtesy of Dr. Michael W. Davidson, Florida State University, (c) NASA.) \\n\\nAnother major area of visual processing is remote sensing, which usually includes \\nseveral bands in the visual and infrared regions of the spectrum. Table 1.1 shows the \\nso-called thematic bands in NASA\\'s LANDSAT satellites. The primary function of \\nLANDSAT is to obtain and transmit images of the Earth from space, for purposes \\nof monitoring environmental conditions on the planet. The bands are expressed in \\nterms of wavelength, with 1mm  being equal to 10 6\\n-  m (we will discuss the wave-\\nlength regions of the electromagnetic spectrum in more detail in Chapter 2). Note \\nthe characteristics and uses of each band in Table 1.1.\\nIn order to develop a basic appreciation for the power of this type of multispec-\\ntral imaging, consider Fig. 1.10, which shows one image for each of the spectral bands \\nin Table 1.1. The area imaged is Washington D.C., which includes features such as \\nbuildings, roads, vegetation, and a major river (the Potomac) going though the city. \\nab\\nc\\nde\\nf\\nFIGURE 1.9\\nExamples of light  \\nmicroscopy images.  \\n(a) Taxol (antican-\\ncer agent), magni-\\nfied 250 x. \\n(b) Cholesterol--\\n40 x.  \\n(c) Microproces-\\nsor--60 x.  \\n(d) Nickel oxide \\nthin film--600 x. \\n(e) Surface of audio \\nCD--1750 x.  \\n(f) Organic super-\\nconductor-- 450 x. \\n(Images courtesy of \\nDr. Michael W.  \\nDavidson, Florida \\nState University.) \\n\\n'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = 'My name is Joel and my website is www.joelpointon.ir'\n",
    "cleaned_text = re.sub(r'http\\S+', '', all_text)\n",
    "cleaned_text = re.sub(r'www.+', '', cleaned_text)\n",
    "cleaned_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "class Article:\n",
    "    def count_tokens(self, text):\n",
    "        return num_tokens(text)\n",
    "\n",
    "    def split_document(self, content):\n",
    "        CHUNK_SIZE = 20\n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "        # text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0, encoding_name=GPT_EMBEDDING_MODEL) # , separators=['\\n\\n', '\\n', '.', ' ', '']\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE,\n",
    "                                                       length_function=num_tokens,\n",
    "                                                       separators=['\\n\\n', '\\n', '.', ' ', ''], # ['', ' ', '.', '\\n', '\\n\\n'],\n",
    "                                                       chunk_overlap=5)\n",
    "        return text_splitter.split_text(content)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "inst = Article()\n",
    "chunks = inst.split_document(all_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Knowledge' object has no attribute 'append_pdf'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[47], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m CompVisionKnowledge \u001B[38;5;241m=\u001B[39m Knowledge(WIKI_PAGE, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGPT\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mCompVisionKnowledge\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappend_pdf\u001B[49m(filename, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCompVisionPDF\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Knowledge' object has no attribute 'append_pdf'"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_file_folder = 'assets/'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       Page Numbers                                            Content  \\\n0                 1                                 GLOBAL \\nEDITION\\n   \n1                 1                         Digital Image Processing\\n   \n2                 1                                   FOURTH EDITION\\n   \n3                 1            Rafael C. Gonzalez * Richard E. Woods\\n   \n4                 1                               www.EBooksWorld.ir\\n   \n...             ...                                                ...   \n19376          1022  This is a special edition of an established \\n...   \n19377          1022                           Pearson Global Edition\\n   \n19378          1022                                 GLOBAL \\nEDITION\\n   \n19379          1022  For these Global Editions, the editorial team ...   \n19380          1022                               www.EBooksWorld.ir\\n   \n\n                                             no_newlines  \n0                                         GLOBAL EDITION  \n1                               Digital Image Processing  \n2                                         FOURTH EDITION  \n3                  Rafael C. Gonzalez * Richard E. Woods  \n4                                     www.EBooksWorld.ir  \n...                                                  ...  \n19376  This is a special edition of an established ti...  \n19377                             Pearson Global Edition  \n19378                                     GLOBAL EDITION  \n19379  For these Global Editions, the editorial team ...  \n19380                                 www.EBooksWorld.ir  \n\n[19381 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Page Numbers</th>\n      <th>Content</th>\n      <th>no_newlines</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>GLOBAL \\nEDITION\\n</td>\n      <td>GLOBAL EDITION</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Digital Image Processing\\n</td>\n      <td>Digital Image Processing</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>FOURTH EDITION\\n</td>\n      <td>FOURTH EDITION</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>Rafael C. Gonzalez * Richard E. Woods\\n</td>\n      <td>Rafael C. Gonzalez * Richard E. Woods</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>www.EBooksWorld.ir\\n</td>\n      <td>www.EBooksWorld.ir</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19376</th>\n      <td>1022</td>\n      <td>This is a special edition of an established \\n...</td>\n      <td>This is a special edition of an established ti...</td>\n    </tr>\n    <tr>\n      <th>19377</th>\n      <td>1022</td>\n      <td>Pearson Global Edition\\n</td>\n      <td>Pearson Global Edition</td>\n    </tr>\n    <tr>\n      <th>19378</th>\n      <td>1022</td>\n      <td>GLOBAL \\nEDITION\\n</td>\n      <td>GLOBAL EDITION</td>\n    </tr>\n    <tr>\n      <th>19379</th>\n      <td>1022</td>\n      <td>For these Global Editions, the editorial team ...</td>\n      <td>For these Global Editions, the editorial team ...</td>\n    </tr>\n    <tr>\n      <th>19380</th>\n      <td>1022</td>\n      <td>www.EBooksWorld.ir\\n</td>\n      <td>www.EBooksWorld.ir</td>\n    </tr>\n  </tbody>\n</table>\n<p>19381 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unidecode import unidecode # parsing symbols\n",
    "import fitz\n",
    "doc = fitz.open(filename)\n",
    "page_num = 0\n",
    "page_numbers = []\n",
    "content = []\n",
    "output = []\n",
    "for page in doc:\n",
    "    page_num += 1\n",
    "    blocks = page.get_text(\"blocks\")\n",
    "    for block in blocks:\n",
    "        if block[6]==0:\n",
    "            block_content = unidecode(block[4])\n",
    "            stripped_block_content = block_content.replace('\\n', '')\n",
    "            if not stripped_block_content.isdigit() and not is_float(stripped_block_content):\n",
    "                content.append(block_content)\n",
    "                page_numbers.append(page_num)\n",
    "            else:\n",
    "                pass\n",
    "content_df = pd.DataFrame(\n",
    "    {'Page Numbers': page_numbers,\n",
    "     'Content': content,\n",
    "    })\n",
    "content_df['no_newlines'] = content_df['Content'].str.replace(r'\\n', '', regex=True)\n",
    "content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "block_dict = {}\n",
    "page_num = 1\n",
    "for page in doc: # Iterate all pages in the document\n",
    "      file_dict = page.get_text('dict') # Get the page dictionary\n",
    "      block = file_dict['blocks'] # Get the block information\n",
    "      block_dict[page_num] = block # Store in block dictionary\n",
    "      page_num += 1 # Increase the page value by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mSSLCertVerificationError\u001B[0m                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:1346\u001B[0m, in \u001B[0;36mAbstractHTTPHandler.do_open\u001B[1;34m(self, http_class, req, **http_conn_args)\u001B[0m\n\u001B[0;32m   1345\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1346\u001B[0m     \u001B[43mh\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1347\u001B[0m \u001B[43m              \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhas_header\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTransfer-encoding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1348\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:1285\u001B[0m, in \u001B[0;36mHTTPConnection.request\u001B[1;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[0;32m   1284\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1285\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:1331\u001B[0m, in \u001B[0;36mHTTPConnection._send_request\u001B[1;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[0;32m   1330\u001B[0m     body \u001B[38;5;241m=\u001B[39m _encode(body, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m-> 1331\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mendheaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:1280\u001B[0m, in \u001B[0;36mHTTPConnection.endheaders\u001B[1;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[0;32m   1279\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CannotSendHeader()\n\u001B[1;32m-> 1280\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:1040\u001B[0m, in \u001B[0;36mHTTPConnection._send_output\u001B[1;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[0;32m   1039\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer[:]\n\u001B[1;32m-> 1040\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1042\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m message_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1043\u001B[0m \n\u001B[0;32m   1044\u001B[0m     \u001B[38;5;66;03m# create a consistent interface to message_body\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:980\u001B[0m, in \u001B[0;36mHTTPConnection.send\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m    979\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_open:\n\u001B[1;32m--> 980\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    981\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\http\\client.py:1454\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1452\u001B[0m     server_hostname \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost\n\u001B[1;32m-> 1454\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrap_socket\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1455\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mserver_hostname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\ssl.py:501\u001B[0m, in \u001B[0;36mSSLContext.wrap_socket\u001B[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001B[0m\n\u001B[0;32m    495\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrap_socket\u001B[39m(\u001B[38;5;28mself\u001B[39m, sock, server_side\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    496\u001B[0m                 do_handshake_on_connect\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    497\u001B[0m                 suppress_ragged_eofs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    498\u001B[0m                 server_hostname\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    499\u001B[0m     \u001B[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001B[39;00m\n\u001B[0;32m    500\u001B[0m     \u001B[38;5;66;03m# ctx._wrap_socket()\u001B[39;00m\n\u001B[1;32m--> 501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msslsocket_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43msock\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mserver_side\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mserver_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdo_handshake_on_connect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_handshake_on_connect\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    505\u001B[0m \u001B[43m        \u001B[49m\u001B[43msuppress_ragged_eofs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msuppress_ragged_eofs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    506\u001B[0m \u001B[43m        \u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mserver_hostname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    507\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    508\u001B[0m \u001B[43m        \u001B[49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msession\u001B[49m\n\u001B[0;32m    509\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\ssl.py:1041\u001B[0m, in \u001B[0;36mSSLSocket._create\u001B[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001B[0m\n\u001B[0;32m   1040\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1041\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_handshake\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1042\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mOSError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\ssl.py:1310\u001B[0m, in \u001B[0;36mSSLSocket.do_handshake\u001B[1;34m(self, block)\u001B[0m\n\u001B[0;32m   1309\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msettimeout(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m-> 1310\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_handshake\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1311\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[1;31mSSLCertVerificationError\u001B[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mURLError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m target_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://homepages.inf.ed.ac.uk/rbf/HIPR2/median.htm\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01murllib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrequest\u001B[39;00m  \u001B[38;5;66;03m# the lib that handles the url stuff\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m \u001B[43murllib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget_url\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28mprint\u001B[39m(line\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:214\u001B[0m, in \u001B[0;36murlopen\u001B[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[0m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    213\u001B[0m     opener \u001B[38;5;241m=\u001B[39m _opener\n\u001B[1;32m--> 214\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:517\u001B[0m, in \u001B[0;36mOpenerDirector.open\u001B[1;34m(self, fullurl, data, timeout)\u001B[0m\n\u001B[0;32m    514\u001B[0m     req \u001B[38;5;241m=\u001B[39m meth(req)\n\u001B[0;32m    516\u001B[0m sys\u001B[38;5;241m.\u001B[39maudit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124murllib.Request\u001B[39m\u001B[38;5;124m'\u001B[39m, req\u001B[38;5;241m.\u001B[39mfull_url, req\u001B[38;5;241m.\u001B[39mdata, req\u001B[38;5;241m.\u001B[39mheaders, req\u001B[38;5;241m.\u001B[39mget_method())\n\u001B[1;32m--> 517\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    519\u001B[0m \u001B[38;5;66;03m# post-process response\u001B[39;00m\n\u001B[0;32m    520\u001B[0m meth_name \u001B[38;5;241m=\u001B[39m protocol\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_response\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:534\u001B[0m, in \u001B[0;36mOpenerDirector._open\u001B[1;34m(self, req, data)\u001B[0m\n\u001B[0;32m    531\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[0;32m    533\u001B[0m protocol \u001B[38;5;241m=\u001B[39m req\u001B[38;5;241m.\u001B[39mtype\n\u001B[1;32m--> 534\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_open\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\n\u001B[0;32m    535\u001B[0m \u001B[43m                          \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_open\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result:\n\u001B[0;32m    537\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:494\u001B[0m, in \u001B[0;36mOpenerDirector._call_chain\u001B[1;34m(self, chain, kind, meth_name, *args)\u001B[0m\n\u001B[0;32m    492\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m handler \u001B[38;5;129;01min\u001B[39;00m handlers:\n\u001B[0;32m    493\u001B[0m     func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[1;32m--> 494\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    495\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    496\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:1389\u001B[0m, in \u001B[0;36mHTTPSHandler.https_open\u001B[1;34m(self, req)\u001B[0m\n\u001B[0;32m   1388\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhttps_open\u001B[39m(\u001B[38;5;28mself\u001B[39m, req):\n\u001B[1;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhttp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mHTTPSConnection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1390\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_hostname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_hostname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\urllib\\request.py:1349\u001B[0m, in \u001B[0;36mAbstractHTTPHandler.do_open\u001B[1;34m(self, http_class, req, **http_conn_args)\u001B[0m\n\u001B[0;32m   1346\u001B[0m         h\u001B[38;5;241m.\u001B[39mrequest(req\u001B[38;5;241m.\u001B[39mget_method(), req\u001B[38;5;241m.\u001B[39mselector, req\u001B[38;5;241m.\u001B[39mdata, headers,\n\u001B[0;32m   1347\u001B[0m                   encode_chunked\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39mhas_header(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTransfer-encoding\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m   1348\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n\u001B[1;32m-> 1349\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m URLError(err)\n\u001B[0;32m   1350\u001B[0m     r \u001B[38;5;241m=\u001B[39m h\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[0;32m   1351\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "\u001B[1;31mURLError\u001B[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>"
     ]
    }
   ],
   "source": [
    "target_url = 'https://homepages.inf.ed.ac.uk/rbf/HIPR2/median.htm'\n",
    "import urllib.request  # the lib that handles the url stuff\n",
    "\n",
    "for line in urllib.request.urlopen(target_url):\n",
    "    print(line.decode('utf-8')) #utf-8 or iso8859-1 or whatever the page encoding scheme is"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Subheading</th>\n",
       "      <th>Content</th>\n",
       "      <th>Section</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\nComputer vision tasks include methods for ac...</td>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>286</td>\n",
       "      <td>[-0.01913553662598133, 0.002932898933067918, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Definition</td>\n",
       "      <td></td>\n",
       "      <td>\\nComputer vision is an interdisciplinary fiel...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Definition</td>\n",
       "      <td>158</td>\n",
       "      <td>[-0.021093836054205894, 0.0049119978211820126,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>History</td>\n",
       "      <td></td>\n",
       "      <td>\\nIn the late 1960s, computer vision began at ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;History</td>\n",
       "      <td>507</td>\n",
       "      <td>[-0.011549791321158409, -0.004044382367283106,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Solid-state physics</td>\n",
       "      <td>\\nSolid-state physics is another field that is...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;S...</td>\n",
       "      <td>120</td>\n",
       "      <td>[0.0018743288237601519, 0.011324070394039154, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Neurobiology</td>\n",
       "      <td>\\nNeurobiology has greatly influenced the deve...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;N...</td>\n",
       "      <td>293</td>\n",
       "      <td>[-0.009132628329098225, 0.0011366719845682383,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Signal processing</td>\n",
       "      <td>\\nYet another field related to computer vision...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;S...</td>\n",
       "      <td>103</td>\n",
       "      <td>[-0.027298789471387863, 0.007510432507842779, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Robotic navigation</td>\n",
       "      <td>\\nRobot navigation sometimes deals with autono...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;R...</td>\n",
       "      <td>64</td>\n",
       "      <td>[0.0034529592376202345, -0.014102335087954998,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Other fields</td>\n",
       "      <td>\\nBesides the above-mentioned views on compute...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;O...</td>\n",
       "      <td>119</td>\n",
       "      <td>[0.002435609931126237, -0.003915637265890837, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Distinctions</td>\n",
       "      <td>\\nThe fields most closely related to computer ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;D...</td>\n",
       "      <td>639</td>\n",
       "      <td>[-0.017207426950335503, 0.005905073136091232, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td></td>\n",
       "      <td>\\nApplications range from tasks such as indust...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications</td>\n",
       "      <td>272</td>\n",
       "      <td>[-0.022458024322986603, 0.005672922823578119, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>\\nOne of the most prominent application fields...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Med...</td>\n",
       "      <td>135</td>\n",
       "      <td>[-0.016155855730175972, 0.02248280495405197, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Machine vision</td>\n",
       "      <td>\\nA second application area in computer vision...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Mac...</td>\n",
       "      <td>141</td>\n",
       "      <td>[-0.016629502177238464, 0.002632115501910448, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Military</td>\n",
       "      <td>\\nMilitary applications are probably one of th...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Mil...</td>\n",
       "      <td>129</td>\n",
       "      <td>[-0.02624369040131569, 0.001608214108273387, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Autonomous vehicles</td>\n",
       "      <td>\\nOne of the newer application areas is autono...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Aut...</td>\n",
       "      <td>234</td>\n",
       "      <td>[0.002211927669122815, -0.004606796428561211, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Tactile feedback</td>\n",
       "      <td>\\nMaterials such as rubber and silicon are bei...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Tac...</td>\n",
       "      <td>270</td>\n",
       "      <td>[-0.015194285660982132, 0.023810898885130882, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td></td>\n",
       "      <td>\\nEach of the application areas described abov...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks</td>\n",
       "      <td>161</td>\n",
       "      <td>[-0.018167616799473763, 0.007240524981170893, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Recognition</td>\n",
       "      <td>\\nThe classical problem in computer vision, im...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Re...</td>\n",
       "      <td>691</td>\n",
       "      <td>[-0.018989920616149902, 0.02043752372264862, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Motion analysis</td>\n",
       "      <td>\\nSeveral tasks relate to motion estimation wh...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Mo...</td>\n",
       "      <td>193</td>\n",
       "      <td>[-0.02092411182820797, 0.00222062598913908, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Scene reconstruction</td>\n",
       "      <td>\\nGiven one or (typically) more images of a sc...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Sc...</td>\n",
       "      <td>125</td>\n",
       "      <td>[-0.02498997002840042, 0.015953006222844124, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Image restoration</td>\n",
       "      <td>\\nImage restoration comes into picture when th...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Im...</td>\n",
       "      <td>198</td>\n",
       "      <td>[0.0009744223789311945, 0.03190232068300247, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>System methods</td>\n",
       "      <td></td>\n",
       "      <td>\\nThe organization of a computer vision system...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;System methods</td>\n",
       "      <td>672</td>\n",
       "      <td>[-0.0014013586333021522, 0.026699397712945938,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>System methods</td>\n",
       "      <td>Image-understanding systems</td>\n",
       "      <td>\\nImage-understanding systems (IUS) include th...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;System methods-&gt;I...</td>\n",
       "      <td>192</td>\n",
       "      <td>[0.006758322473615408, -0.004905234090983868, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Hardware</td>\n",
       "      <td></td>\n",
       "      <td>\\nThere are many kinds of computer vision syst...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Hardware</td>\n",
       "      <td>392</td>\n",
       "      <td>[-0.006029689218848944, 0.016216637566685677, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Source         Heading                   Subheading  \\\n",
       "0   Wikipedia (Computer vision)                                                \n",
       "1   Wikipedia (Computer vision)      Definition                                \n",
       "2   Wikipedia (Computer vision)         History                                \n",
       "4   Wikipedia (Computer vision)  Related fields          Solid-state physics   \n",
       "5   Wikipedia (Computer vision)  Related fields                 Neurobiology   \n",
       "6   Wikipedia (Computer vision)  Related fields            Signal processing   \n",
       "7   Wikipedia (Computer vision)  Related fields           Robotic navigation   \n",
       "8   Wikipedia (Computer vision)  Related fields                 Other fields   \n",
       "9   Wikipedia (Computer vision)  Related fields                 Distinctions   \n",
       "10  Wikipedia (Computer vision)    Applications                                \n",
       "11  Wikipedia (Computer vision)    Applications                     Medicine   \n",
       "12  Wikipedia (Computer vision)    Applications               Machine vision   \n",
       "13  Wikipedia (Computer vision)    Applications                     Military   \n",
       "14  Wikipedia (Computer vision)    Applications          Autonomous vehicles   \n",
       "15  Wikipedia (Computer vision)    Applications             Tactile feedback   \n",
       "16  Wikipedia (Computer vision)   Typical tasks                                \n",
       "17  Wikipedia (Computer vision)   Typical tasks                  Recognition   \n",
       "18  Wikipedia (Computer vision)   Typical tasks              Motion analysis   \n",
       "19  Wikipedia (Computer vision)   Typical tasks         Scene reconstruction   \n",
       "20  Wikipedia (Computer vision)   Typical tasks            Image restoration   \n",
       "21  Wikipedia (Computer vision)  System methods                                \n",
       "22  Wikipedia (Computer vision)  System methods  Image-understanding systems   \n",
       "23  Wikipedia (Computer vision)        Hardware                                \n",
       "\n",
       "                                              Content  \\\n",
       "0   \\nComputer vision tasks include methods for ac...   \n",
       "1   \\nComputer vision is an interdisciplinary fiel...   \n",
       "2   \\nIn the late 1960s, computer vision began at ...   \n",
       "4   \\nSolid-state physics is another field that is...   \n",
       "5   \\nNeurobiology has greatly influenced the deve...   \n",
       "6   \\nYet another field related to computer vision...   \n",
       "7   \\nRobot navigation sometimes deals with autono...   \n",
       "8   \\nBesides the above-mentioned views on compute...   \n",
       "9   \\nThe fields most closely related to computer ...   \n",
       "10  \\nApplications range from tasks such as indust...   \n",
       "11  \\nOne of the most prominent application fields...   \n",
       "12  \\nA second application area in computer vision...   \n",
       "13  \\nMilitary applications are probably one of th...   \n",
       "14  \\nOne of the newer application areas is autono...   \n",
       "15  \\nMaterials such as rubber and silicon are bei...   \n",
       "16  \\nEach of the application areas described abov...   \n",
       "17  \\nThe classical problem in computer vision, im...   \n",
       "18  \\nSeveral tasks relate to motion estimation wh...   \n",
       "19  \\nGiven one or (typically) more images of a sc...   \n",
       "20  \\nImage restoration comes into picture when th...   \n",
       "21  \\nThe organization of a computer vision system...   \n",
       "22  \\nImage-understanding systems (IUS) include th...   \n",
       "23  \\nThere are many kinds of computer vision syst...   \n",
       "\n",
       "                                              Section  Tokens  \\\n",
       "0                         Wikipedia (Computer vision)     286   \n",
       "1             Wikipedia (Computer vision)->Definition     158   \n",
       "2                Wikipedia (Computer vision)->History     507   \n",
       "4   Wikipedia (Computer vision)->Related fields->S...     120   \n",
       "5   Wikipedia (Computer vision)->Related fields->N...     293   \n",
       "6   Wikipedia (Computer vision)->Related fields->S...     103   \n",
       "7   Wikipedia (Computer vision)->Related fields->R...      64   \n",
       "8   Wikipedia (Computer vision)->Related fields->O...     119   \n",
       "9   Wikipedia (Computer vision)->Related fields->D...     639   \n",
       "10          Wikipedia (Computer vision)->Applications     272   \n",
       "11  Wikipedia (Computer vision)->Applications->Med...     135   \n",
       "12  Wikipedia (Computer vision)->Applications->Mac...     141   \n",
       "13  Wikipedia (Computer vision)->Applications->Mil...     129   \n",
       "14  Wikipedia (Computer vision)->Applications->Aut...     234   \n",
       "15  Wikipedia (Computer vision)->Applications->Tac...     270   \n",
       "16         Wikipedia (Computer vision)->Typical tasks     161   \n",
       "17  Wikipedia (Computer vision)->Typical tasks->Re...     691   \n",
       "18  Wikipedia (Computer vision)->Typical tasks->Mo...     193   \n",
       "19  Wikipedia (Computer vision)->Typical tasks->Sc...     125   \n",
       "20  Wikipedia (Computer vision)->Typical tasks->Im...     198   \n",
       "21        Wikipedia (Computer vision)->System methods     672   \n",
       "22  Wikipedia (Computer vision)->System methods->I...     192   \n",
       "23              Wikipedia (Computer vision)->Hardware     392   \n",
       "\n",
       "                                            Embedding  \n",
       "0   [-0.01913553662598133, 0.002932898933067918, 0...  \n",
       "1   [-0.021093836054205894, 0.0049119978211820126,...  \n",
       "2   [-0.011549791321158409, -0.004044382367283106,...  \n",
       "4   [0.0018743288237601519, 0.011324070394039154, ...  \n",
       "5   [-0.009132628329098225, 0.0011366719845682383,...  \n",
       "6   [-0.027298789471387863, 0.007510432507842779, ...  \n",
       "7   [0.0034529592376202345, -0.014102335087954998,...  \n",
       "8   [0.002435609931126237, -0.003915637265890837, ...  \n",
       "9   [-0.017207426950335503, 0.005905073136091232, ...  \n",
       "10  [-0.022458024322986603, 0.005672922823578119, ...  \n",
       "11  [-0.016155855730175972, 0.02248280495405197, 0...  \n",
       "12  [-0.016629502177238464, 0.002632115501910448, ...  \n",
       "13  [-0.02624369040131569, 0.001608214108273387, 0...  \n",
       "14  [0.002211927669122815, -0.004606796428561211, ...  \n",
       "15  [-0.015194285660982132, 0.023810898885130882, ...  \n",
       "16  [-0.018167616799473763, 0.007240524981170893, ...  \n",
       "17  [-0.018989920616149902, 0.02043752372264862, 0...  \n",
       "18  [-0.02092411182820797, 0.00222062598913908, -0...  \n",
       "19  [-0.02498997002840042, 0.015953006222844124, 0...  \n",
       "20  [0.0009744223789311945, 0.03190232068300247, 0...  \n",
       "21  [-0.0014013586333021522, 0.026699397712945938,...  \n",
       "22  [0.006758322473615408, -0.004905234090983868, ...  \n",
       "23  [-0.006029689218848944, 0.016216637566685677, ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CompVisionKnowledge = Knowledge(WIKI_PAGE, 'GPT')\n",
    "for page in WIKI_PAGES:\n",
    "    CompVisionKnowledge.append_wikipedia_page(WIKI_PAGE)\n",
    "# save document chunks and embeddings\n",
    "CompVisionKnowledge.export_to_csv(GPT_KNOWLEDGE_FILENAME)\n",
    "CompVisionKnowledge.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Subheading</th>\n",
       "      <th>Content</th>\n",
       "      <th>Section</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\nComputer vision tasks include methods for ac...</td>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>290</td>\n",
       "      <td>[-0.5566069483757019, 0.6151323318481445, 0.70...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Definition</td>\n",
       "      <td></td>\n",
       "      <td>\\nComputer vision is an interdisciplinary fiel...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Definition</td>\n",
       "      <td>162</td>\n",
       "      <td>[-0.18940897285938263, 0.5564344525337219, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>History</td>\n",
       "      <td></td>\n",
       "      <td>\\nIn the late 1960s, computer vision began at ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;History</td>\n",
       "      <td>243</td>\n",
       "      <td>[-0.5624328255653381, 0.35494446754455566, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>History</td>\n",
       "      <td></td>\n",
       "      <td>\\nBy the 1990s, some of the previous research ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;History</td>\n",
       "      <td>264</td>\n",
       "      <td>[-0.8420819044113159, 0.011862404644489288, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Solid-state physics</td>\n",
       "      <td>\\nSolid-state physics is another field that is...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;S...</td>\n",
       "      <td>123</td>\n",
       "      <td>[-0.1766018569469452, 0.5509802103042603, 0.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Neurobiology</td>\n",
       "      <td>\\nNeurobiology has greatly influenced the deve...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;N...</td>\n",
       "      <td>296</td>\n",
       "      <td>[0.07454751431941986, 0.42800015211105347, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Signal processing</td>\n",
       "      <td>\\nYet another field related to computer vision...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;S...</td>\n",
       "      <td>106</td>\n",
       "      <td>[-0.1562240570783615, 0.18830031156539917, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Robotic navigation</td>\n",
       "      <td>\\nRobot navigation sometimes deals with autono...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;R...</td>\n",
       "      <td>65</td>\n",
       "      <td>[0.09209920465946198, 0.5207788944244385, 1.26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Other fields</td>\n",
       "      <td>\\nBesides the above-mentioned views on compute...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;O...</td>\n",
       "      <td>122</td>\n",
       "      <td>[-0.34635502099990845, 0.12120083719491959, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Distinctions</td>\n",
       "      <td>\\nThe fields most closely related to computer ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;D...</td>\n",
       "      <td>244</td>\n",
       "      <td>[-0.011814514175057411, 0.9892112612724304, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Related fields</td>\n",
       "      <td>Distinctions</td>\n",
       "      <td>\\nImage processing and image analysis tend to ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Related fields-&gt;D...</td>\n",
       "      <td>395</td>\n",
       "      <td>[-0.25189733505249023, 0.7090330123901367, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td></td>\n",
       "      <td>\\nApplications range from tasks such as indust...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications</td>\n",
       "      <td>294</td>\n",
       "      <td>[0.054026536643505096, 0.536332368850708, 0.80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>\\nOne of the most prominent application fields...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Med...</td>\n",
       "      <td>145</td>\n",
       "      <td>[0.07362960278987885, 0.8047475218772888, 1.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Machine vision</td>\n",
       "      <td>\\nA second application area in computer vision...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Mac...</td>\n",
       "      <td>148</td>\n",
       "      <td>[0.23051360249519348, 0.6428527235984802, 0.69...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Military</td>\n",
       "      <td>\\nMilitary applications are probably one of th...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Mil...</td>\n",
       "      <td>129</td>\n",
       "      <td>[-0.45685380697250366, 0.40300095081329346, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Autonomous vehicles</td>\n",
       "      <td>\\nOne of the newer application areas is autono...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Aut...</td>\n",
       "      <td>258</td>\n",
       "      <td>[-0.5068467855453491, 0.4222138524055481, 1.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Tactile feedback</td>\n",
       "      <td>\\nMaterials such as rubber and silicon are bei...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Applications-&gt;Tac...</td>\n",
       "      <td>286</td>\n",
       "      <td>[-0.006287522614002228, 0.3353821039199829, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td></td>\n",
       "      <td>\\nEach of the application areas described abov...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks</td>\n",
       "      <td>166</td>\n",
       "      <td>[-0.3646470308303833, 0.5144392848014832, 1.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Recognition</td>\n",
       "      <td>\\nThe classical problem in computer vision, im...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Re...</td>\n",
       "      <td>427</td>\n",
       "      <td>[-0.11414705961942673, 0.8190616965293884, 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Recognition</td>\n",
       "      <td>\\nContent-based image retrieval – finding all ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Re...</td>\n",
       "      <td>264</td>\n",
       "      <td>[-0.3213890492916107, 1.116416335105896, 0.800...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Motion analysis</td>\n",
       "      <td>\\nSeveral tasks relate to motion estimation wh...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Mo...</td>\n",
       "      <td>194</td>\n",
       "      <td>[-0.08507562428712845, 0.058049630373716354, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Scene reconstruction</td>\n",
       "      <td>\\nGiven one or (typically) more images of a sc...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Sc...</td>\n",
       "      <td>115</td>\n",
       "      <td>[-0.6162410378456116, 0.3802846670150757, 1.66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Typical tasks</td>\n",
       "      <td>Image restoration</td>\n",
       "      <td>\\nImage restoration comes into picture when th...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Typical tasks-&gt;Im...</td>\n",
       "      <td>204</td>\n",
       "      <td>[-0.25118744373321533, 0.52935791015625, 1.085...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>System methods</td>\n",
       "      <td></td>\n",
       "      <td>\\nThe organization of a computer vision system...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;System methods</td>\n",
       "      <td>122</td>\n",
       "      <td>[0.13426706194877625, 0.5260908603668213, 1.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>System methods</td>\n",
       "      <td></td>\n",
       "      <td>\\nImage acquisition – A digital image is produ...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;System methods</td>\n",
       "      <td>253</td>\n",
       "      <td>[0.13420583307743073, 0.20847204327583313, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>System methods</td>\n",
       "      <td></td>\n",
       "      <td>\\nLocalized interest points such as corners, b...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;System methods</td>\n",
       "      <td>297</td>\n",
       "      <td>[-0.2144298553466797, -0.15774108469486237, 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>System methods</td>\n",
       "      <td>Image-understanding systems</td>\n",
       "      <td>\\nImage-understanding systems (IUS) include th...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;System methods-&gt;I...</td>\n",
       "      <td>202</td>\n",
       "      <td>[-0.3703722059726715, 0.7370752692222595, 0.51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Wikipedia (Computer vision)</td>\n",
       "      <td>Hardware</td>\n",
       "      <td></td>\n",
       "      <td>\\nThere are many kinds of computer vision syst...</td>\n",
       "      <td>Wikipedia (Computer vision)-&gt;Hardware</td>\n",
       "      <td>409</td>\n",
       "      <td>[0.02258257381618023, 0.39461076259613037, 1.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Source         Heading                   Subheading  \\\n",
       "0   Wikipedia (Computer vision)                                                \n",
       "1   Wikipedia (Computer vision)      Definition                                \n",
       "2   Wikipedia (Computer vision)         History                                \n",
       "3   Wikipedia (Computer vision)         History                                \n",
       "5   Wikipedia (Computer vision)  Related fields          Solid-state physics   \n",
       "6   Wikipedia (Computer vision)  Related fields                 Neurobiology   \n",
       "7   Wikipedia (Computer vision)  Related fields            Signal processing   \n",
       "8   Wikipedia (Computer vision)  Related fields           Robotic navigation   \n",
       "9   Wikipedia (Computer vision)  Related fields                 Other fields   \n",
       "10  Wikipedia (Computer vision)  Related fields                 Distinctions   \n",
       "11  Wikipedia (Computer vision)  Related fields                 Distinctions   \n",
       "12  Wikipedia (Computer vision)    Applications                                \n",
       "13  Wikipedia (Computer vision)    Applications                     Medicine   \n",
       "14  Wikipedia (Computer vision)    Applications               Machine vision   \n",
       "15  Wikipedia (Computer vision)    Applications                     Military   \n",
       "16  Wikipedia (Computer vision)    Applications          Autonomous vehicles   \n",
       "17  Wikipedia (Computer vision)    Applications             Tactile feedback   \n",
       "18  Wikipedia (Computer vision)   Typical tasks                                \n",
       "19  Wikipedia (Computer vision)   Typical tasks                  Recognition   \n",
       "20  Wikipedia (Computer vision)   Typical tasks                  Recognition   \n",
       "21  Wikipedia (Computer vision)   Typical tasks              Motion analysis   \n",
       "22  Wikipedia (Computer vision)   Typical tasks         Scene reconstruction   \n",
       "23  Wikipedia (Computer vision)   Typical tasks            Image restoration   \n",
       "24  Wikipedia (Computer vision)  System methods                                \n",
       "25  Wikipedia (Computer vision)  System methods                                \n",
       "26  Wikipedia (Computer vision)  System methods                                \n",
       "27  Wikipedia (Computer vision)  System methods  Image-understanding systems   \n",
       "28  Wikipedia (Computer vision)        Hardware                                \n",
       "\n",
       "                                              Content  \\\n",
       "0   \\nComputer vision tasks include methods for ac...   \n",
       "1   \\nComputer vision is an interdisciplinary fiel...   \n",
       "2   \\nIn the late 1960s, computer vision began at ...   \n",
       "3   \\nBy the 1990s, some of the previous research ...   \n",
       "5   \\nSolid-state physics is another field that is...   \n",
       "6   \\nNeurobiology has greatly influenced the deve...   \n",
       "7   \\nYet another field related to computer vision...   \n",
       "8   \\nRobot navigation sometimes deals with autono...   \n",
       "9   \\nBesides the above-mentioned views on compute...   \n",
       "10  \\nThe fields most closely related to computer ...   \n",
       "11  \\nImage processing and image analysis tend to ...   \n",
       "12  \\nApplications range from tasks such as indust...   \n",
       "13  \\nOne of the most prominent application fields...   \n",
       "14  \\nA second application area in computer vision...   \n",
       "15  \\nMilitary applications are probably one of th...   \n",
       "16  \\nOne of the newer application areas is autono...   \n",
       "17  \\nMaterials such as rubber and silicon are bei...   \n",
       "18  \\nEach of the application areas described abov...   \n",
       "19  \\nThe classical problem in computer vision, im...   \n",
       "20  \\nContent-based image retrieval – finding all ...   \n",
       "21  \\nSeveral tasks relate to motion estimation wh...   \n",
       "22  \\nGiven one or (typically) more images of a sc...   \n",
       "23  \\nImage restoration comes into picture when th...   \n",
       "24  \\nThe organization of a computer vision system...   \n",
       "25  \\nImage acquisition – A digital image is produ...   \n",
       "26  \\nLocalized interest points such as corners, b...   \n",
       "27  \\nImage-understanding systems (IUS) include th...   \n",
       "28  \\nThere are many kinds of computer vision syst...   \n",
       "\n",
       "                                              Section  Tokens  \\\n",
       "0                         Wikipedia (Computer vision)     290   \n",
       "1             Wikipedia (Computer vision)->Definition     162   \n",
       "2                Wikipedia (Computer vision)->History     243   \n",
       "3                Wikipedia (Computer vision)->History     264   \n",
       "5   Wikipedia (Computer vision)->Related fields->S...     123   \n",
       "6   Wikipedia (Computer vision)->Related fields->N...     296   \n",
       "7   Wikipedia (Computer vision)->Related fields->S...     106   \n",
       "8   Wikipedia (Computer vision)->Related fields->R...      65   \n",
       "9   Wikipedia (Computer vision)->Related fields->O...     122   \n",
       "10  Wikipedia (Computer vision)->Related fields->D...     244   \n",
       "11  Wikipedia (Computer vision)->Related fields->D...     395   \n",
       "12          Wikipedia (Computer vision)->Applications     294   \n",
       "13  Wikipedia (Computer vision)->Applications->Med...     145   \n",
       "14  Wikipedia (Computer vision)->Applications->Mac...     148   \n",
       "15  Wikipedia (Computer vision)->Applications->Mil...     129   \n",
       "16  Wikipedia (Computer vision)->Applications->Aut...     258   \n",
       "17  Wikipedia (Computer vision)->Applications->Tac...     286   \n",
       "18         Wikipedia (Computer vision)->Typical tasks     166   \n",
       "19  Wikipedia (Computer vision)->Typical tasks->Re...     427   \n",
       "20  Wikipedia (Computer vision)->Typical tasks->Re...     264   \n",
       "21  Wikipedia (Computer vision)->Typical tasks->Mo...     194   \n",
       "22  Wikipedia (Computer vision)->Typical tasks->Sc...     115   \n",
       "23  Wikipedia (Computer vision)->Typical tasks->Im...     204   \n",
       "24        Wikipedia (Computer vision)->System methods     122   \n",
       "25        Wikipedia (Computer vision)->System methods     253   \n",
       "26        Wikipedia (Computer vision)->System methods     297   \n",
       "27  Wikipedia (Computer vision)->System methods->I...     202   \n",
       "28              Wikipedia (Computer vision)->Hardware     409   \n",
       "\n",
       "                                            Embedding  \n",
       "0   [-0.5566069483757019, 0.6151323318481445, 0.70...  \n",
       "1   [-0.18940897285938263, 0.5564344525337219, 0.5...  \n",
       "2   [-0.5624328255653381, 0.35494446754455566, 0.6...  \n",
       "3   [-0.8420819044113159, 0.011862404644489288, 0....  \n",
       "5   [-0.1766018569469452, 0.5509802103042603, 0.11...  \n",
       "6   [0.07454751431941986, 0.42800015211105347, 0.1...  \n",
       "7   [-0.1562240570783615, 0.18830031156539917, 0.4...  \n",
       "8   [0.09209920465946198, 0.5207788944244385, 1.26...  \n",
       "9   [-0.34635502099990845, 0.12120083719491959, 0....  \n",
       "10  [-0.011814514175057411, 0.9892112612724304, 0....  \n",
       "11  [-0.25189733505249023, 0.7090330123901367, 1.0...  \n",
       "12  [0.054026536643505096, 0.536332368850708, 0.80...  \n",
       "13  [0.07362960278987885, 0.8047475218772888, 1.09...  \n",
       "14  [0.23051360249519348, 0.6428527235984802, 0.69...  \n",
       "15  [-0.45685380697250366, 0.40300095081329346, 0....  \n",
       "16  [-0.5068467855453491, 0.4222138524055481, 1.11...  \n",
       "17  [-0.006287522614002228, 0.3353821039199829, 0....  \n",
       "18  [-0.3646470308303833, 0.5144392848014832, 1.08...  \n",
       "19  [-0.11414705961942673, 0.8190616965293884, 0.9...  \n",
       "20  [-0.3213890492916107, 1.116416335105896, 0.800...  \n",
       "21  [-0.08507562428712845, 0.058049630373716354, 1...  \n",
       "22  [-0.6162410378456116, 0.3802846670150757, 1.66...  \n",
       "23  [-0.25118744373321533, 0.52935791015625, 1.085...  \n",
       "24  [0.13426706194877625, 0.5260908603668213, 1.00...  \n",
       "25  [0.13420583307743073, 0.20847204327583313, 0.6...  \n",
       "26  [-0.2144298553466797, -0.15774108469486237, 1....  \n",
       "27  [-0.3703722059726715, 0.7370752692222595, 0.51...  \n",
       "28  [0.02258257381618023, 0.39461076259613037, 1.0...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CompVisionKnowledgeBERT = Knowledge(WIKI_PAGE, 'BERT')\n",
    "CompVisionKnowledgeBERT.append_wikipedia_page(WIKI_PAGE)\n",
    "# save document chunks and embeddings\n",
    "CompVisionKnowledgeBERT.export_to_csv(BERT_KNOWLEDGE_FILENAME)\n",
    "CompVisionKnowledgeBERT.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Search\n",
    "Now we'll define a search function that:\n",
    "\n",
    "Takes a user query and a dataframe with text & embedding columns\n",
    "Embeds the user query with the OpenAI API\n",
    "Uses distance between query embedding and text embeddings to rank the texts\n",
    "Returns two lists:\n",
    "The top N texts, ranked by relevance\n",
    "Their corresponding relevance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "370d7486832f4481859fe77fbfc7a969"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36m<cell line: 256>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# CompVisionBERT = ChatBot(\"Computer Vision\", 'assets/' + BERT_KNOWLEDGE_FILENAME)\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# print(Query.ask_bert('When did universities begin teaching Computer Vision?', CompVisionBERT))\u001B[39;00m\n\u001B[0;32m    251\u001B[0m \n\u001B[0;32m    252\u001B[0m \u001B[38;5;66;03m# CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\u001B[39;00m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;66;03m# print(Query.ask('Who is Boris Johnson', CompVisionGPT, show_source=True))\u001B[39;00m\n\u001B[0;32m    255\u001B[0m CompVisionGPT \u001B[38;5;241m=\u001B[39m ChatBot(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mComputer Vision\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124massets/\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m GPT_KNOWLEDGE_FILENAME)\n\u001B[1;32m--> 256\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mQuery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mask_bart\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mWhen did Universities begin teaching Computer Vision?\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCompVisionGPT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_source\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.ask_bart\u001B[1;34m(cls, query_text, chatbot_instance, show_source, confidence_level)\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQuestion is too long, please try again with a shorter question.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    214\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(query_text, chatbot_instance)\n\u001B[1;32m--> 215\u001B[0m response_message \u001B[38;5;241m=\u001B[39m \u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_bart_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfidence_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfidence_level\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m show_source \u001B[38;5;129;01mand\u001B[39;00m response_message\u001B[38;5;241m!=\u001B[39mANSWER_NOT_FOUND_MSG: \u001B[38;5;66;03m# Display the sources used:\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     response_message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m query\u001B[38;5;241m.\u001B[39mshow_source_message()\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.get_bart_output\u001B[1;34m(self, encoding_model, bert_model, confidence_level)\u001B[0m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_bart_output\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    171\u001B[0m                     \u001B[38;5;66;03m# chatbot_instance: ChatBot,\u001B[39;00m\n\u001B[0;32m    172\u001B[0m                     \u001B[38;5;66;03m# embedding_model: str = BART_EMBEDDING_MODEL,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    175\u001B[0m                     confidence_level: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m,\n\u001B[0;32m    176\u001B[0m                     ):\n\u001B[1;32m--> 177\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mknowledge_ranked_by_similarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfidence_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfidence_level\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mknowledge_used)\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m ANSWER_NOT_FOUND_MSG\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.knowledge_ranked_by_similarity\u001B[1;34m(self, max_num_sections, confidence_level, embedding_model)\u001B[0m\n\u001B[0;32m     28\u001B[0m query_embedding_response \u001B[38;5;241m=\u001B[39m get_embedding(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontent, embedding_model\u001B[38;5;241m=\u001B[39mBERT_EMBEDDING_MODEL)\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m embedding_model \u001B[38;5;241m==\u001B[39m GPT_EMBEDDING_MODEL:\n\u001B[1;32m---> 30\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[43mquery_embedding_response\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# knowledge_with_similarities[\"similarity\"] = knowledge_with_similarities[\"Embedding\"].apply(lambda x: self.similarity(query_embedding, x))\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(query_embedding_response)\n",
      "\u001B[1;31mIndexError\u001B[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# CompVisionBERT = ChatBot(\"Computer Vision\", 'assets/' + BERT_KNOWLEDGE_FILENAME)\n",
    "# print(Query.ask_bert('When did universities begin teaching Computer Vision?', CompVisionBERT))\n",
    "\n",
    "# CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\n",
    "# print(Query.ask('Who is Boris Johnson', CompVisionGPT, show_source=True))\n",
    "\n",
    "CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\n",
    "print(Query.ask_bart('When did Universities begin teaching Computer Vision?', CompVisionGPT, show_source=True)) # What if the GPT knowledge sections are longer than 1024 tokens?? Need to account for this!\n",
    "\n",
    "# Todo:\n",
    "# I need to make it more efficient on the number of tokens.\n",
    "# Adapt it for more sources (e.g. PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\point\\AppData\\Local\\Temp\\ipykernel_23968\\3384959035.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Index'] = np.arange(len(self.knowledge_used))+1\n",
      "C:\\Users\\point\\AppData\\Local\\Temp\\ipykernel_23968\\3384959035.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Cumulative_tokens'] = self.knowledge_used['Tokens'].cumsum()\n",
      "C:\\Users\\point\\AppData\\Local\\Temp\\ipykernel_23968\\3384959035.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.knowledge_used['Cumulative_tokens'] += message_and_question_tokens # add the inital number of tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could not find an answer in the text I've been provided, sorry! Please try again.\n",
      "\n",
      "Total tokens used: 721\n"
     ]
    }
   ],
   "source": [
    "CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\n",
    "print(Query.ask('Who is Boris Johnson', CompVisionGPT, show_source=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d07a56b31944c4bb14a999b57fd3c4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (768,) (1536,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# CompVisionBERT = ChatBot(\"Computer Vision\", 'assets/' + BERT_KNOWLEDGE_FILENAME)\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mQuery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mask_bert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mWhen did universities begin teaching Computer Vision?\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCompVisionGPT\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.ask_bert\u001B[1;34m(cls, query_text, chatbot_instance, embedding_model, encoding_model, bert_model, show_source)\u001B[0m\n\u001B[0;32m    126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQuestion is too long, please try again with a shorter question.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    127\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(query_text, chatbot_instance)\n\u001B[1;32m--> 128\u001B[0m response_message, answer_index \u001B[38;5;241m=\u001B[39m \u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_bert_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membedding_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbert_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbert_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m show_source \u001B[38;5;129;01mand\u001B[39;00m response_message\u001B[38;5;241m!=\u001B[39mANSWER_NOT_FOUND_MSG: \u001B[38;5;66;03m# Display the sources used:\u001B[39;00m\n\u001B[0;32m    131\u001B[0m     response_message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m query\u001B[38;5;241m.\u001B[39mshow_source_message(answer_index\u001B[38;5;241m=\u001B[39manswer_index)\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.get_bert_output\u001B[1;34m(self, embedding_model, encoding_model, bert_model)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_bert_output\u001B[39m(\n\u001B[0;32m     77\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     78\u001B[0m         embedding_model: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m     79\u001B[0m         encoding_model: BertTokenizer \u001B[38;5;241m=\u001B[39m BERT_ENCODING,\n\u001B[0;32m     80\u001B[0m         bert_model: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m BERT_MODEL\n\u001B[0;32m     81\u001B[0m ):\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;124;03m\"\"\"Uses the most relevant texts from the knowledge dataframe to construct a message that can then be fed into GPT.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 83\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mknowledge_ranked_by_similarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membedding_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m     answer_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     86\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.knowledge_ranked_by_similarity\u001B[1;34m(self, max_num_sections, confidence_level, embedding_model)\u001B[0m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# knowledge_with_similarities[\"similarity\"] = knowledge_with_similarities[\"Embedding\"].apply(lambda x: self.similarity(query_embedding, x))\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(query_embedding_response)\n\u001B[1;32m---> 34\u001B[0m knowledge_with_similarities[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mknowledge_with_similarities\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEmbedding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_embedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     36\u001B[0m knowledge_with_similarities\u001B[38;5;241m.\u001B[39msort_values(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     37\u001B[0m top_n_sections \u001B[38;5;241m=\u001B[39m knowledge_with_similarities\u001B[38;5;241m.\u001B[39mhead(max_num_sections)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[0;32m   4323\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4324\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4325\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4328\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4329\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4330\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4331\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4332\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4431\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4432\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4433\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1078\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m   1079\u001B[0m     \u001B[38;5;66;03m# if we are a string, try to dispatch\u001B[39;00m\n\u001B[0;32m   1080\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[1;32m-> 1082\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1131\u001B[0m         values \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values\n\u001B[0;32m   1132\u001B[0m         \u001B[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001B[39;00m\n\u001B[0;32m   1133\u001B[0m         \u001B[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001B[39;00m\n\u001B[0;32m   1134\u001B[0m         \u001B[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001B[39;00m\n\u001B[0;32m   1135\u001B[0m         \u001B[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001B[39;00m\n\u001B[0;32m   1136\u001B[0m         \u001B[38;5;66;03m# \"Callable[[Any], Any]\"\u001B[39;00m\n\u001B[1;32m-> 1137\u001B[0m         mapped \u001B[38;5;241m=\u001B[39m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1138\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1139\u001B[0m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m   1140\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1141\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1144\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1145\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1146\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.knowledge_ranked_by_similarity.<locals>.<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# knowledge_with_similarities[\"similarity\"] = knowledge_with_similarities[\"Embedding\"].apply(lambda x: self.similarity(query_embedding, x))\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(query_embedding_response)\n\u001B[1;32m---> 34\u001B[0m knowledge_with_similarities[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m knowledge_with_similarities[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmbedding\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_embedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     36\u001B[0m knowledge_with_similarities\u001B[38;5;241m.\u001B[39msort_values(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     37\u001B[0m top_n_sections \u001B[38;5;241m=\u001B[39m knowledge_with_similarities\u001B[38;5;241m.\u001B[39mhead(max_num_sections)\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mQuery.similarity\u001B[1;34m(query_embedding, knowledge_embedding)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msimilarity\u001B[39m(query_embedding: \u001B[38;5;28mlist\u001B[39m,\n\u001B[0;32m     13\u001B[0m                knowledge_embedding: \u001B[38;5;28mlist\u001B[39m\n\u001B[0;32m     14\u001B[0m                ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;124;03m\"\"\"Calculates the cosine similarity score between the query and knowledge embedding vectors.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m1\u001B[39m\u001B[38;5;241m-\u001B[39m \u001B[43mspatial\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcosine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_embedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mknowledge_embedding\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:670\u001B[0m, in \u001B[0;36mcosine\u001B[1;34m(u, v, w)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;124;03mCompute the Cosine distance between 1-D arrays.\u001B[39;00m\n\u001B[0;32m    630\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    665\u001B[0m \n\u001B[0;32m    666\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    667\u001B[0m \u001B[38;5;66;03m# cosine distance is also referred to as 'uncentered correlation',\u001B[39;00m\n\u001B[0;32m    668\u001B[0m \u001B[38;5;66;03m#   or 'reflective correlation'\u001B[39;00m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;66;03m# clamp the result to 0-2\u001B[39;00m\n\u001B[1;32m--> 670\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mmin\u001B[39m(\u001B[43mcorrelation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mu\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcentered\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m, \u001B[38;5;241m2.0\u001B[39m))\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:619\u001B[0m, in \u001B[0;36mcorrelation\u001B[1;34m(u, v, w, centered)\u001B[0m\n\u001B[0;32m    617\u001B[0m     u \u001B[38;5;241m=\u001B[39m u \u001B[38;5;241m-\u001B[39m umu\n\u001B[0;32m    618\u001B[0m     v \u001B[38;5;241m=\u001B[39m v \u001B[38;5;241m-\u001B[39m vmu\n\u001B[1;32m--> 619\u001B[0m uv \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39maverage(\u001B[43mu\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m, weights\u001B[38;5;241m=\u001B[39mw)\n\u001B[0;32m    620\u001B[0m uu \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39maverage(np\u001B[38;5;241m.\u001B[39msquare(u), weights\u001B[38;5;241m=\u001B[39mw)\n\u001B[0;32m    621\u001B[0m vv \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39maverage(np\u001B[38;5;241m.\u001B[39msquare(v), weights\u001B[38;5;241m=\u001B[39mw)\n",
      "\u001B[1;31mValueError\u001B[0m: operands could not be broadcast together with shapes (768,) (1536,) "
     ]
    }
   ],
   "source": [
    "CompVisionBERT = ChatBot(\"Computer Vision\", 'assets/' + BERT_KNOWLEDGE_FILENAME)\n",
    "print(Query.ask_bert('When did universities begin teaching Computer Vision?', CompVisionBERT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96c7a743564c4afda124d34911ac58d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [20]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mQuery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mask_bart\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mWhen did Universities begin teaching Computer Vision?\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCompVisionBERT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_source\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.ask_bart\u001B[1;34m(cls, query_text, chatbot_instance, show_source, confidence_level)\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQuestion is too long, please try again with a shorter question.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    214\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(query_text, chatbot_instance)\n\u001B[1;32m--> 215\u001B[0m response_message \u001B[38;5;241m=\u001B[39m \u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_bart_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfidence_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfidence_level\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m show_source \u001B[38;5;129;01mand\u001B[39;00m response_message\u001B[38;5;241m!=\u001B[39mANSWER_NOT_FOUND_MSG: \u001B[38;5;66;03m# Display the sources used:\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     response_message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m query\u001B[38;5;241m.\u001B[39mshow_source_message()\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.get_bart_output\u001B[1;34m(self, encoding_model, bert_model, confidence_level)\u001B[0m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_bart_output\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    171\u001B[0m                     \u001B[38;5;66;03m# chatbot_instance: ChatBot,\u001B[39;00m\n\u001B[0;32m    172\u001B[0m                     \u001B[38;5;66;03m# embedding_model: str = BART_EMBEDDING_MODEL,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    175\u001B[0m                     confidence_level: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m,\n\u001B[0;32m    176\u001B[0m                     ):\n\u001B[1;32m--> 177\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mknowledge_ranked_by_similarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfidence_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfidence_level\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mknowledge_used)\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m ANSWER_NOT_FOUND_MSG\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mQuery.knowledge_ranked_by_similarity\u001B[1;34m(self, max_num_sections, confidence_level, embedding_model)\u001B[0m\n\u001B[0;32m     28\u001B[0m query_embedding_response \u001B[38;5;241m=\u001B[39m get_embedding(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontent, embedding_model\u001B[38;5;241m=\u001B[39mBERT_EMBEDDING_MODEL)\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m embedding_model \u001B[38;5;241m==\u001B[39m GPT_EMBEDDING_MODEL:\n\u001B[1;32m---> 30\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[43mquery_embedding_response\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# knowledge_with_similarities[\"similarity\"] = knowledge_with_similarities[\"Embedding\"].apply(lambda x: self.similarity(query_embedding, x))\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(query_embedding_response)\n",
      "\u001B[1;31mIndexError\u001B[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "CompVisionGPT = ChatBot(\"Computer Vision\", 'assets/' + GPT_KNOWLEDGE_FILENAME)\n",
    "print(Query.ask_bart('When did Universities begin teaching Computer Vision?', CompVisionGPT, show_source=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[6.7410e+22, 2.6729e+23, 5.3689e-05],\n",
      "        [1.3542e-05, 5.2905e-08, 6.7942e-07]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def describe(x):\n",
    "    print(\"Type: {}\".format(x.type()))\n",
    "    print(\"Shape/size: {}\".format(x.shape))\n",
    "    print(\"Values: \\n{}\".format(x))\n",
    "\n",
    "describe(torch.Tensor(2, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tasks\n",
    "Clean the parsed text so we have pure english words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", '’', 'crazy', 'ones', ',', 'misfits', ',', 'rebels', ',', 'troublemakers', ',', 'round', 'pegs', 'square', 'holes', '.', 'ones', 'see', 'things', 'differently', '—', '’', 'fond', 'rules', '.', 'quote', ',', 'disagree', ',', 'glorify', 'vilify', ',', 'thing', '’', 'ignore', 'change', 'things', '.', 'push', 'human', 'race', 'forward', ',', 'may', 'see', 'crazy', 'ones', ',', 'see', 'genius', ',', 'ones', 'crazy', 'enough', 'think', 'change', 'world', ',', 'ones', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"It's Here’s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. The ones who see things differently — they’re not fond of rules. You can quote them, disagree with them, glorify or vilify them, but the only thing you can’t do is ignore them because they change things. They push the human race forward, and while some may see them as the crazy ones, we see genius, because the ones who are crazy enough to think that they can change the world, are the ones who do.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "['It',\n \"'s\",\n 'Here',\n '’',\n 's',\n 'to',\n 'the',\n 'crazy',\n 'ones',\n ',',\n 'the',\n 'misfits',\n ',',\n 'the',\n 'rebels',\n ',',\n 'the',\n 'troublemakers',\n ',',\n 'the',\n 'round',\n 'pegs',\n 'in',\n 'the',\n 'square',\n 'holes',\n '.',\n 'The',\n 'ones',\n 'who',\n 'see',\n 'things',\n 'differently',\n '—',\n 'they',\n '’',\n 're',\n 'not',\n 'fond',\n 'of',\n 'rules',\n '.',\n 'You',\n 'can',\n 'quote',\n 'them',\n ',',\n 'disagree',\n 'with',\n 'them',\n ',',\n 'glorify',\n 'or',\n 'vilify',\n 'them',\n ',',\n 'but',\n 'the',\n 'only',\n 'thing',\n 'you',\n 'can',\n '’',\n 't',\n 'do',\n 'is',\n 'ignore',\n 'them',\n 'because',\n 'they',\n 'change',\n 'things',\n '.',\n 'They',\n 'push',\n 'the',\n 'human',\n 'race',\n 'forward',\n ',',\n 'and',\n 'while',\n 'some',\n 'may',\n 'see',\n 'them',\n 'as',\n 'the',\n 'crazy',\n 'ones',\n ',',\n 'we',\n 'see',\n 'genius',\n ',',\n 'because',\n 'the',\n 'ones',\n 'who',\n 'are',\n 'crazy',\n 'enough',\n 'to',\n 'think',\n 'that',\n 'they',\n 'can',\n 'change',\n 'the',\n 'world',\n ',',\n 'are',\n 'the',\n 'ones',\n 'who',\n 'do',\n '.']"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "sentences = [\n",
    "\"Artificial intelligence (AI) is revolutionising industries and transforming the way we live and work.\",\n",
    "\"Machine learning, a subset of AI, enables computers to learn and make predictions or decisions without being explicitly programmed.\",\n",
    "\"AI algorithms can analyse vast amounts of data to uncover patterns, detect anomalies, and extract valuable insights.\",\n",
    "\"Natural language processing (NLP) allows machines to understand and process human language, facilitating communication between humans and computers.\",\n",
    "\"Computer vision enables machines to interpret and understand visual information, enabling applications such as image recognition and object detection.\",\n",
    "\"AI-powered virtual assistants like Siri, Alexa, and Google Assistant are becoming increasingly common, providing voice-based interactions and personalised experiences.\",\n",
    "\"AI is driving advancements in healthcare, including disease diagnosis, drug discovery, and personalised medicine, leading to improved patient outcomes.\",\n",
    "\"In the field of autonomous vehicles, AI plays a crucial role in enabling self-driving cars to perceive their surroundings and make informed decisions.\",\n",
    "\"AI is transforming the customer service industry by utilising chatbots and automated systems to provide faster and more efficient support to customers.\",\n",
    "\"Ethical considerations, such as transparency, fairness, and privacy, are essential in the development and deployment of AI systems to ensure responsible and accountable use.\",\n",
    "\"Cars have revolutionised transportation, providing a convenient and efficient means of travel for people around the world.\",\n",
    "\"Automobile manufacturing involves a complex process of designing, engineering, and assembling various components to create a functional vehicle.\",\n",
    "\"Safety features such as seat belts, airbags, and anti-lock braking systems have greatly improved the overall safety of cars.\",\n",
    "\"Electric vehicles (EVs) are gaining popularity as eco-friendly alternatives to traditional petrol-powered cars, reducing carbon emissions and dependence on fossil fuels.\",\n",
    "\"Advanced driver-assistance systems (ADAS) enhance car safety by incorporating technologies like adaptive cruise control and lane-keeping assist.\",\n",
    "\"Sports cars are known for their high-performance capabilities, offering speed, agility, and an exhilarating driving experience.\",\n",
    "\"Classic cars hold a special place in automotive history, with their timeless designs and nostalgic appeal capturing the hearts of car enthusiasts.\",\n",
    "\"Car customisation allows owners to personalise their vehicles, from unique paint jobs and body modifications to performance upgrades.\",\n",
    "\"Car-sharing services and ride-hailing apps have transformed the way people access transportation, providing convenient alternatives to car ownership.\",\n",
    "\"The future of cars is expected to bring autonomous vehicles, where cars can navigate and operate without human intervention, promising increased safety and efficiency.\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_text(text: str, remove_stopwords: bool) -> str:\n",
    "    \"\"\"Function that cleans the input text by going to:\n",
    "    - remove links\n",
    "    - remove special characters\n",
    "    - remove numbers\n",
    "    - remove stopwords\n",
    "    - convert to lowercase\n",
    "    - remove excessive white spaces\n",
    "    Arguments:\n",
    "        text (str): text to clean\n",
    "        remove_stopwords (bool): whether to remove stopwords\n",
    "    Returns:\n",
    "        str: cleaned text\n",
    "    \"\"\"\n",
    "    # remove links\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # remove numbers and special characters\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
    "    # remove stopwords\n",
    "    if remove_stopwords:\n",
    "        # 1. create tokens\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # 2. check if it's a stopword\n",
    "        tokens = [w.lower().strip() for w in tokens if not w.lower() in stopwords.words(\"english\")]\n",
    "        # return a list of cleaned tokens\n",
    "        return tokens\n",
    "\n",
    "tokenised_sentences = [preprocess_text(sentence, remove_stopwords=True) for sentence in sentences]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "[['artificial',\n  'intelligence',\n  'ai',\n  'revolutionising',\n  'industries',\n  'transforming',\n  'way',\n  'live',\n  'work'],\n ['machine',\n  'learning',\n  'subset',\n  'ai',\n  'enables',\n  'computers',\n  'learn',\n  'make',\n  'predictions',\n  'decisions',\n  'without',\n  'explicitly',\n  'programmed'],\n ['ai',\n  'algorithms',\n  'analyse',\n  'vast',\n  'amounts',\n  'data',\n  'uncover',\n  'patterns',\n  'detect',\n  'anomalies',\n  'extract',\n  'valuable',\n  'insights'],\n ['natural',\n  'language',\n  'processing',\n  'nlp',\n  'allows',\n  'machines',\n  'understand',\n  'process',\n  'human',\n  'language',\n  'facilitating',\n  'communication',\n  'humans',\n  'computers'],\n ['computer',\n  'vision',\n  'enables',\n  'machines',\n  'interpret',\n  'understand',\n  'visual',\n  'information',\n  'enabling',\n  'applications',\n  'image',\n  'recognition',\n  'object',\n  'detection'],\n ['ai',\n  'powered',\n  'virtual',\n  'assistants',\n  'like',\n  'siri',\n  'alexa',\n  'google',\n  'assistant',\n  'becoming',\n  'increasingly',\n  'common',\n  'providing',\n  'voice',\n  'based',\n  'interactions',\n  'personalised',\n  'experiences'],\n ['ai',\n  'driving',\n  'advancements',\n  'healthcare',\n  'including',\n  'disease',\n  'diagnosis',\n  'drug',\n  'discovery',\n  'personalised',\n  'medicine',\n  'leading',\n  'improved',\n  'patient',\n  'outcomes'],\n ['field',\n  'autonomous',\n  'vehicles',\n  'ai',\n  'plays',\n  'crucial',\n  'role',\n  'enabling',\n  'self',\n  'driving',\n  'cars',\n  'perceive',\n  'surroundings',\n  'make',\n  'informed',\n  'decisions'],\n ['ai',\n  'transforming',\n  'customer',\n  'service',\n  'industry',\n  'utilising',\n  'chatbots',\n  'automated',\n  'systems',\n  'provide',\n  'faster',\n  'efficient',\n  'support',\n  'customers'],\n ['ethical',\n  'considerations',\n  'transparency',\n  'fairness',\n  'privacy',\n  'essential',\n  'development',\n  'deployment',\n  'ai',\n  'systems',\n  'ensure',\n  'responsible',\n  'accountable',\n  'use'],\n ['cars',\n  'revolutionised',\n  'transportation',\n  'providing',\n  'convenient',\n  'efficient',\n  'means',\n  'travel',\n  'people',\n  'around',\n  'world'],\n ['automobile',\n  'manufacturing',\n  'involves',\n  'complex',\n  'process',\n  'designing',\n  'engineering',\n  'assembling',\n  'various',\n  'components',\n  'create',\n  'functional',\n  'vehicle'],\n ['safety',\n  'features',\n  'seat',\n  'belts',\n  'airbags',\n  'anti',\n  'lock',\n  'braking',\n  'systems',\n  'greatly',\n  'improved',\n  'overall',\n  'safety',\n  'cars'],\n ['electric',\n  'vehicles',\n  'evs',\n  'gaining',\n  'popularity',\n  'eco',\n  'friendly',\n  'alternatives',\n  'traditional',\n  'petrol',\n  'powered',\n  'cars',\n  'reducing',\n  'carbon',\n  'emissions',\n  'dependence',\n  'fossil',\n  'fuels'],\n ['advanced',\n  'driver',\n  'assistance',\n  'systems',\n  'adas',\n  'enhance',\n  'car',\n  'safety',\n  'incorporating',\n  'technologies',\n  'like',\n  'adaptive',\n  'cruise',\n  'control',\n  'lane',\n  'keeping',\n  'assist'],\n ['sports',\n  'cars',\n  'known',\n  'high',\n  'performance',\n  'capabilities',\n  'offering',\n  'speed',\n  'agility',\n  'exhilarating',\n  'driving',\n  'experience'],\n ['classic',\n  'cars',\n  'hold',\n  'special',\n  'place',\n  'automotive',\n  'history',\n  'timeless',\n  'designs',\n  'nostalgic',\n  'appeal',\n  'capturing',\n  'hearts',\n  'car',\n  'enthusiasts'],\n ['car',\n  'customisation',\n  'allows',\n  'owners',\n  'personalise',\n  'vehicles',\n  'unique',\n  'paint',\n  'jobs',\n  'body',\n  'modifications',\n  'performance',\n  'upgrades'],\n ['car',\n  'sharing',\n  'services',\n  'ride',\n  'hailing',\n  'apps',\n  'transformed',\n  'way',\n  'people',\n  'access',\n  'transportation',\n  'providing',\n  'convenient',\n  'alternatives',\n  'car',\n  'ownership'],\n ['future',\n  'cars',\n  'expected',\n  'bring',\n  'autonomous',\n  'vehicles',\n  'cars',\n  'navigate',\n  'operate',\n  'without',\n  'human',\n  'intervention',\n  'promising',\n  'increased',\n  'safety',\n  'efficiency']]"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised_sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "[['artificial',\n  'intelligence',\n  'ai',\n  'revolutionise',\n  'industry',\n  'transform',\n  'way',\n  'live',\n  'work'],\n ['machine',\n  'learn',\n  'subset',\n  'ai',\n  'enable',\n  'computer',\n  'learn',\n  'make',\n  'prediction',\n  'decision',\n  'without',\n  'explicitly',\n  'program'],\n ['ai',\n  'algorithm',\n  'analyse',\n  'vast',\n  'amount',\n  'data',\n  'uncover',\n  'pattern',\n  'detect',\n  'anomaly',\n  'extract',\n  'valuable',\n  'insight'],\n ['natural',\n  'language',\n  'processing',\n  'nlp',\n  'allow',\n  'machine',\n  'understand',\n  'process',\n  'human',\n  'language',\n  'facilitate',\n  'communication',\n  'human',\n  'computer'],\n ['computer',\n  'vision',\n  'enable',\n  'machine',\n  'interpret',\n  'understand',\n  'visual',\n  'information',\n  'enable',\n  'application',\n  'image',\n  'recognition',\n  'object',\n  'detection'],\n ['ai',\n  'power',\n  'virtual',\n  'assistant',\n  'like',\n  'siri',\n  'alexa',\n  'google',\n  'assistant',\n  'become',\n  'increasingly',\n  'common',\n  'provide',\n  'voice',\n  'base',\n  'interaction',\n  'personalise',\n  'experience'],\n ['ai',\n  'drive',\n  'advancement',\n  'healthcare',\n  'include',\n  'disease',\n  'diagnosis',\n  'drug',\n  'discovery',\n  'personalise',\n  'medicine',\n  'lead',\n  'improved',\n  'patient',\n  'outcome'],\n ['field',\n  'autonomous',\n  'vehicle',\n  'ai',\n  'play',\n  'crucial',\n  'role',\n  'enable',\n  'self',\n  'drive',\n  'car',\n  'perceive',\n  'surroundings',\n  'make',\n  'informed',\n  'decision'],\n ['ai',\n  'transform',\n  'customer',\n  'service',\n  'industry',\n  'utilise',\n  'chatbots',\n  'automate',\n  'system',\n  'provide',\n  'faster',\n  'efficient',\n  'support',\n  'customer'],\n ['ethical',\n  'consideration',\n  'transparency',\n  'fairness',\n  'privacy',\n  'essential',\n  'development',\n  'deployment',\n  'ai',\n  'system',\n  'ensure',\n  'responsible',\n  'accountable',\n  'use'],\n ['car',\n  'revolutionise',\n  'transportation',\n  'provide',\n  'convenient',\n  'efficient',\n  'mean',\n  'travel',\n  'people',\n  'around',\n  'world'],\n ['automobile',\n  'manufacture',\n  'involves',\n  'complex',\n  'process',\n  'design',\n  'engineering',\n  'assemble',\n  'various',\n  'component',\n  'create',\n  'functional',\n  'vehicle'],\n ['safety',\n  'feature',\n  'seat',\n  'belt',\n  'airbags',\n  'anti',\n  'lock',\n  'brake',\n  'system',\n  'greatly',\n  'improve',\n  'overall',\n  'safety',\n  'car'],\n ['electric',\n  'vehicle',\n  'evs',\n  'gain',\n  'popularity',\n  'eco',\n  'friendly',\n  'alternative',\n  'traditional',\n  'petrol',\n  'power',\n  'car',\n  'reduce',\n  'carbon',\n  'emission',\n  'dependence',\n  'fossil',\n  'fuel'],\n ['advanced',\n  'driver',\n  'assistance',\n  'system',\n  'adas',\n  'enhance',\n  'car',\n  'safety',\n  'incorporate',\n  'technology',\n  'like',\n  'adaptive',\n  'cruise',\n  'control',\n  'lane',\n  'keep',\n  'assist'],\n ['sport',\n  'car',\n  'know',\n  'high',\n  'performance',\n  'capability',\n  'offer',\n  'speed',\n  'agility',\n  'exhilarate',\n  'drive',\n  'experience'],\n ['classic',\n  'car',\n  'hold',\n  'special',\n  'place',\n  'automotive',\n  'history',\n  'timeless',\n  'design',\n  'nostalgic',\n  'appeal',\n  'capture',\n  'heart',\n  'car',\n  'enthusiast'],\n ['car',\n  'customisation',\n  'allow',\n  'owner',\n  'personalise',\n  'vehicle',\n  'unique',\n  'paint',\n  'job',\n  'body',\n  'modification',\n  'performance',\n  'upgrade'],\n ['car',\n  'share',\n  'service',\n  'ride',\n  'hail',\n  'apps',\n  'transform',\n  'way',\n  'people',\n  'access',\n  'transportation',\n  'provide',\n  'convenient',\n  'alternative',\n  'car',\n  'ownership'],\n ['future',\n  'car',\n  'expect',\n  'bring',\n  'autonomous',\n  'vehicle',\n  'car',\n  'navigate',\n  'operate',\n  'without',\n  'human',\n  'intervention',\n  'promising',\n  'increase',\n  'safety',\n  'efficiency']]"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN # Default to noun if the POS tag is unknown/ambiguous\n",
    "\n",
    "def lemmatize_sentence(tokenised_sentence):\n",
    "    tagged_tokens = nltk.pos_tag(tokenised_sentence)\n",
    "    lemmatized_words = []\n",
    "    for word, tag in tagged_tokens:\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(word, pos=pos))\n",
    "    return lemmatized_words\n",
    "\n",
    "# refined_sentences = [sentence.strip()]\n",
    "lemmatised_sentences = [lemmatize_sentence(s) for s in tokenised_sentences]\n",
    "lemmatised_sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "VECTOR_SIZE = 100\n",
    "MIN_COUNT = 1\n",
    "WINDOW = 3\n",
    "SG = 1\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=lemmatised_sentences,\n",
    "    vector_size=VECTOR_SIZE,\n",
    "    min_count=MIN_COUNT,\n",
    "    sg=SG\n",
    ")\n",
    "\n",
    "model = Word2Vec(sentences=lemmatised_sentences, min_count=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.052282296"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('ai', 'algorithm')\n",
    "\n",
    "# model.wv.most_similar(positive=['ai'], topn=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\point\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning:\n",
      "\n",
      "The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "\n",
      "C:\\Users\\point\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning:\n",
      "\n",
      "The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "markers",
         "text": [
          "cars",
          "ai",
          "car",
          "vehicles",
          "systems",
          "safety",
          "driving",
          "providing",
          "personalized",
          "like",
          "enabling",
          "alternatives",
          "people",
          "performance",
          "improved",
          "human",
          "process",
          "understand",
          "machines",
          "allows",
          "language",
          "powered",
          "autonomous",
          "way",
          "convenient",
          "enables",
          "efficient",
          "transforming",
          "transportation",
          "computers",
          "without",
          "make",
          "decisions",
          "advancements",
          "diagnosis",
          "provide",
          "disease",
          "including",
          "healthcare",
          "customer",
          "faster",
          "discovery",
          "experiences",
          "interactions",
          "based",
          "voice",
          "support",
          "customers",
          "drug",
          "chatbots",
          "medicine",
          "leading",
          "informed",
          "surroundings",
          "perceive",
          "self",
          "role",
          "crucial",
          "plays",
          "increasingly",
          "field",
          "outcomes",
          "industry",
          "utilizing",
          "patient",
          "service",
          "automated",
          "common",
          "efficiency",
          "becoming",
          "valuable",
          "anomalies",
          "detect",
          "patterns",
          "uncover",
          "data",
          "amounts",
          "vast",
          "analyze",
          "algorithms",
          "programmed",
          "explicitly",
          "predictions",
          "learn",
          "subset",
          "learning",
          "machine",
          "work",
          "live",
          "industries",
          "revolutionizing",
          "intelligence",
          "extract",
          "insights",
          "assistant",
          "natural",
          "google",
          "alexa",
          "siri",
          "assistants",
          "virtual",
          "considerations",
          "detection",
          "object",
          "recognition",
          "image",
          "applications",
          "information",
          "visual",
          "interpret",
          "vision",
          "computer",
          "humans",
          "communication",
          "facilitating",
          "nlp",
          "processing",
          "ethical",
          "essential",
          "transparency",
          "agility",
          "nostalgic",
          "designs",
          "timeless",
          "history",
          "automotive",
          "place",
          "special",
          "hold",
          "classic",
          "experience",
          "thrilling",
          "speed",
          "capturing",
          "offering",
          "capabilities",
          "high",
          "known",
          "sports",
          "assist",
          "keeping",
          "lane",
          "control",
          "cruise",
          "adaptive",
          "appeal",
          "hearts",
          "fairness",
          "hailing",
          "promising",
          "intervention",
          "operate",
          "navigate",
          "bring",
          "expected",
          "future",
          "ownership",
          "access",
          "transformed",
          "apps",
          "ride",
          "enthusiasts",
          "services",
          "sharing",
          "upgrades",
          "modifications",
          "body",
          "jobs",
          "paint",
          "unique",
          "personalize",
          "owners",
          "customization",
          "technologies",
          "incorporating",
          "enhance",
          "world",
          "functional",
          "create",
          "components",
          "various",
          "assembling",
          "engineering",
          "designing",
          "complex",
          "involves",
          "manufacturing",
          "automobile",
          "around",
          "adas",
          "travel",
          "means",
          "revolutionized",
          "use",
          "accountable",
          "responsible",
          "ensure",
          "deployment",
          "development",
          "increased",
          "privacy",
          "vehicle",
          "features",
          "seat",
          "belts",
          "assistance",
          "driver",
          "advanced",
          "fuels",
          "fossil",
          "dependence",
          "emissions",
          "carbon",
          "reducing",
          "gasoline",
          "traditional",
          "friendly",
          "eco",
          "popularity",
          "gaining",
          "evs",
          "electric",
          "overall",
          "greatly",
          "braking",
          "lock",
          "anti",
          "airbags",
          "artificial"
         ],
         "x": [
          -3.5569868087768555,
          -3.345669984817505,
          -1.1631700992584229,
          -4.823192596435547,
          1.4799708127975464,
          -2.5234670639038086,
          3.3336856365203857,
          4.033675193786621,
          -1.097055435180664,
          1.8911492824554443,
          -4.095557689666748,
          -6.807097434997559,
          3.94748592376709,
          1.8565915822982788,
          -6.009923458099365,
          0.7721247673034668,
          -2.0102412700653076,
          -7.494566917419434,
          -5.4318718910217285,
          4.244304180145264,
          1.411681890487671,
          5.025524139404297,
          -3.7039387226104736,
          -8.411649703979492,
          -0.7734807133674622,
          0.2362508475780487,
          2.539975643157959,
          -1.1920018196105957,
          7.879120826721191,
          -4.905570030212402,
          4.294675827026367,
          7.35902738571167,
          -6.065314292907715,
          5.191819190979004,
          -5.435687065124512,
          0.6472213864326477,
          8.186429023742676,
          3.4760055541992188,
          6.416111946105957,
          3.8564202785491943,
          0.012898512184619904,
          -2.008552312850952,
          -3.6230485439300537,
          -4.542641639709473,
          7.342559814453125,
          -0.9007460474967957,
          -5.753945827484131,
          -2.073754072189331,
          -1.8793163299560547,
          -1.26099693775177,
          -5.374608993530273,
          -6.9775896072387695,
          3.9699244499206543,
          1.5151407718658447,
          4.543593883514404,
          -4.996665954589844,
          -6.796648025512695,
          -0.796980082988739,
          -5.379490852355957,
          -0.549635648727417,
          -9.417557716369629,
          -2.632219076156616,
          1.7778784036636353,
          1.6346489191055298,
          -7.801143169403076,
          7.2114739418029785,
          -9.09225082397461,
          0.9202542901039124,
          -1.1646091938018799,
          2.6558477878570557,
          -5.547433853149414,
          1.9413443803787231,
          -3.9593071937561035,
          2.2311630249023438,
          1.97158944606781,
          6.333553791046143,
          -2.814406633377075,
          0.1673775613307953,
          -8.345366477966309,
          3.1074397563934326,
          -3.0089330673217773,
          -4.5951337814331055,
          -3.7368662357330322,
          -5.906049728393555,
          0.728748619556427,
          0.3020936846733093,
          -1.9193578958511353,
          -3.302121877670288,
          -2.283527135848999,
          1.0822992324829102,
          1.084127426147461,
          0.7212051153182983,
          -0.2904839813709259,
          0.0067567480728030205,
          -6.094676494598389,
          -0.402170330286026,
          0.21640515327453613,
          5.0021562576293945,
          -2.4156148433685303,
          4.747771739959717,
          3.1189804077148438,
          6.701613903045654,
          7.1602253913879395,
          6.287492275238037,
          0.8026138544082642,
          6.452852725982666,
          -2.5331742763519287,
          -4.900890827178955,
          2.9736883640289307,
          4.881409645080566,
          -1.4666893482208252,
          -4.851039409637451,
          2.059319257736206,
          2.250230073928833,
          3.969353437423706,
          3.364682912826538,
          -0.05929283797740936,
          -0.6684280037879944,
          5.829762935638428,
          -0.979670524597168,
          -3.566016912460327,
          -1.4442508220672607,
          -2.588890552520752,
          -3.3296337127685547,
          -2.7897987365722656,
          6.3759918212890625,
          1.5154504776000977,
          -8.110496520996094,
          0.17165987193584442,
          -6.858039855957031,
          -3.5445096492767334,
          2.5935659408569336,
          2.885282516479492,
          1.8959251642227173,
          -5.664699077606201,
          -3.4687421321868896,
          -2.4294581413269043,
          -7.189571857452393,
          -2.8525397777557373,
          5.089686870574951,
          0.6043485999107361,
          3.9126408100128174,
          -2.7547128200531006,
          -5.008999824523926,
          -3.657480001449585,
          -9.955078125,
          3.3863651752471924,
          -3.036243200302124,
          4.4467034339904785,
          -0.05079968646168709,
          -0.4907218813896179,
          -3.036567211151123,
          4.6397705078125,
          1.6668617725372314,
          -1.2682503461837769,
          1.3137140274047852,
          -6.264095783233643,
          4.439817905426025,
          -6.315651893615723,
          -5.666043281555176,
          -0.6006585955619812,
          5.461076259613037,
          -0.24642640352249146,
          -2.679663896560669,
          -3.422029495239258,
          -0.013748754747211933,
          4.076957702636719,
          6.95869255065918,
          2.3676674365997314,
          -8.981572151184082,
          -8.151212692260742,
          3.518017053604126,
          -0.759570300579071,
          -0.42632266879081726,
          7.180819034576416,
          1.6771918535232544,
          -3.5160317420959473,
          0.8269298076629639,
          1.8569263219833374,
          2.010329484939575,
          -3.7082324028015137,
          6.012582302093506,
          4.621157169342041,
          0.9195900559425354,
          -1.7041544914245605,
          3.4600207805633545,
          2.8068747520446777,
          -4.070095062255859,
          3.804140567779541,
          -3.074394702911377,
          -4.536877155303955,
          -1.7962968349456787,
          -2.7294981479644775,
          2.9486894607543945,
          -6.354874134063721,
          -8.988039016723633,
          8.563080787658691,
          6.317517280578613,
          -0.9279513359069824,
          -1.9361271858215332,
          -9.039132118225098,
          7.131534099578857,
          2.7934367656707764,
          -5.968392848968506,
          -0.06545160710811615,
          3.7102925777435303,
          2.258908987045288,
          4.861087799072266,
          -0.05108696222305298,
          -4.49846887588501,
          -4.576083660125732,
          -6.251190185546875,
          -3.3793859481811523,
          1.4788758754730225,
          4.066930294036865,
          -0.9180363416671753,
          5.740225315093994,
          -1.3265256881713867,
          5.262291431427002,
          0.7175686359405518,
          2.9360263347625732,
          -7.619258880615234,
          5.261261463165283,
          1.1680265665054321,
          -4.450006008148193,
          5.410119533538818,
          -3.2600202560424805,
          4.232682228088379,
          -6.257768154144287
         ],
         "y": [
          -2.366219997406006,
          1.4157464504241943,
          -8.381683349609375,
          -4.209931373596191,
          -5.979645252227783,
          -9.352097511291504,
          -1.8068146705627441,
          -7.144293785095215,
          6.021297931671143,
          6.609764575958252,
          -1.495269536972046,
          -0.639616072177887,
          -1.1818455457687378,
          -6.758322715759277,
          1.8673317432403564,
          1.815646767616272,
          0.16977642476558685,
          0.9171539545059204,
          -0.28289005160331726,
          -7.2281694412231445,
          0.7534630298614502,
          1.7798832654953003,
          2.778804302215576,
          3.790527105331421,
          -5.351175785064697,
          -3.560941457748413,
          4.08440637588501,
          5.833236217498779,
          -4.1663336753845215,
          0.3943403661251068,
          4.310480117797852,
          0.09831395000219345,
          0.24064360558986664,
          -1.9963525533676147,
          -1.881211519241333,
          4.180385589599609,
          -0.9039490222930908,
          -1.5471729040145874,
          0.5280646681785583,
          2.6742937564849854,
          4.0620808601379395,
          -6.513219356536865,
          -9.472671508789062,
          -1.1084809303283691,
          2.9618706703186035,
          4.339135646820068,
          -5.158017635345459,
          -5.029111862182617,
          4.206965923309326,
          2.9538791179656982,
          5.694890022277832,
          -0.4904268980026245,
          6.932773113250732,
          7.793333053588867,
          -4.360994338989258,
          4.613007545471191,
          2.7395997047424316,
          0.7967098355293274,
          3.114056348800659,
          -10.288593292236328,
          -1.2579964399337769,
          8.089855194091797,
          -2.3647656440734863,
          -3.7294106483459473,
          -2.29291033744812,
          2.8865668773651123,
          -3.3214125633239746,
          -7.906168460845947,
          -8.579545021057129,
          -7.941169261932373,
          -5.88237190246582,
          -2.719790458679199,
          -2.8582077026367188,
          -5.134355545043945,
          1.7824652194976807,
          6.741681098937988,
          5.792266845703125,
          -2.9503769874572754,
          -5.39525842666626,
          -5.694397926330566,
          5.918738842010498,
          -2.7563304901123047,
          -3.7066729068756104,
          -8.668044090270996,
          -5.506378650665283,
          -4.30855131149292,
          2.2824041843414307,
          0.5046464800834656,
          -5.011874675750732,
          -9.552413940429688,
          -7.896369457244873,
          0.7081783413887024,
          3.2146520614624023,
          -6.3050384521484375,
          -2.3505775928497314,
          -4.481626510620117,
          -1.7087410688400269,
          -4.868049621582031,
          -1.1532683372497559,
          0.6817308068275452,
          3.0158650875091553,
          1.4656099081039429,
          -7.470815658569336,
          -3.990213632583618,
          2.7786660194396973,
          -5.667232513427734,
          2.627997875213623,
          1.2886537313461304,
          4.768289566040039,
          -0.20373012125492096,
          -1.107700228691101,
          2.234194040298462,
          2.0606625080108643,
          -0.11353302001953125,
          -4.433785915374756,
          -0.016857877373695374,
          -6.968067646026611,
          -1.3603588342666626,
          -4.8725175857543945,
          -0.11705698072910309,
          -5.689452648162842,
          -3.5955538749694824,
          5.520202159881592,
          -1.0861376523971558,
          2.7821645736694336,
          -1.5905102491378784,
          -0.9805388450622559,
          -5.05893087387085,
          0.7785710096359253,
          4.528586387634277,
          -6.244285583496094,
          -7.922395706176758,
          1.2706235647201538,
          -4.036782741546631,
          -3.101402521133423,
          -8.064379692077637,
          -2.1249098777770996,
          -2.389819622039795,
          -3.1695380210876465,
          4.600878715515137,
          -1.2858613729476929,
          -9.436290740966797,
          -2.359637975692749,
          0.10975052416324615,
          3.9018781185150146,
          0.8384222984313965,
          -8.780622482299805,
          0.42526963353157043,
          1.231698751449585,
          -0.5188021063804626,
          -2.5711145401000977,
          -1.3426936864852905,
          -7.813931465148926,
          -3.1462206840515137,
          -2.4722371101379395,
          -1.0157849788665771,
          -4.328769207000732,
          6.737553596496582,
          -6.855597019195557,
          -2.030759572982788,
          -4.499425411224365,
          1.6545190811157227,
          6.392116546630859,
          -3.9923131465911865,
          3.8821256160736084,
          6.069917678833008,
          -0.25490161776542664,
          -0.4267646372318268,
          1.0929652452468872,
          3.285449504852295,
          1.4321503639221191,
          1.9575225114822388,
          8.271814346313477,
          1.9291417598724365,
          -1.3109941482543945,
          7.994966983795166,
          -5.386274814605713,
          4.301878452301025,
          2.89518141746521,
          6.410716533660889,
          6.38617467880249,
          -1.6168067455291748,
          6.841063976287842,
          0.05820506066083908,
          1.640087604522705,
          -2.8759775161743164,
          4.6040825843811035,
          -6.035190105438232,
          2.152789831161499,
          -3.5014779567718506,
          4.5739216804504395,
          -3.7116336822509766,
          0.06453826278448105,
          -5.643316268920898,
          -3.0912952423095703,
          -1.396384835243225,
          -2.5754761695861816,
          -3.2645411491394043,
          4.257584095001221,
          -6.593636989593506,
          -3.2896602153778076,
          -7.411569118499756,
          -1.3092409372329712,
          -6.6785125732421875,
          1.808240294456482,
          -0.3113737404346466,
          5.845041751861572,
          3.1287853717803955,
          -2.0365591049194336,
          -4.636128902435303,
          2.706270456314087,
          6.708128452301025,
          -9.077188491821289,
          -1.6567959785461426,
          -4.118268013000488,
          0.3064400553703308,
          -0.6685023903846741,
          0.6971976161003113,
          4.906248569488525,
          0.9191274642944336,
          -2.744675636291504,
          -4.471009731292725,
          4.209408283233643,
          3.0564539432525635,
          -7.672701358795166,
          -2.7060420513153076,
          -7.385237693786621,
          -2.326617479324341,
          -1.1584327220916748
         ],
         "type": "scatter"
        }
       ],
       "layout": {
        "template": {
         "data": {
          "histogram2dcontour": [
           {
            "type": "histogram2dcontour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "choropleth": [
           {
            "type": "choropleth",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "histogram2d": [
           {
            "type": "histogram2d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmap": [
           {
            "type": "heatmap",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmapgl": [
           {
            "type": "heatmapgl",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "contourcarpet": [
           {
            "type": "contourcarpet",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "contour": [
           {
            "type": "contour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "surface": [
           {
            "type": "surface",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "mesh3d": [
           {
            "type": "mesh3d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "parcoords": [
           {
            "type": "parcoords",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolargl": [
           {
            "type": "scatterpolargl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "scattergeo": [
           {
            "type": "scattergeo",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolar": [
           {
            "type": "scatterpolar",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "scattergl": [
           {
            "type": "scattergl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatter3d": [
           {
            "type": "scatter3d",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermapbox": [
           {
            "type": "scattermapbox",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterternary": [
           {
            "type": "scatterternary",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattercarpet": [
           {
            "type": "scattercarpet",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ]
         },
         "layout": {
          "autotypenumbers": "strict",
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "hovermode": "closest",
          "hoverlabel": {
           "align": "left"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "bgcolor": "#E5ECF6",
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "ternary": {
           "bgcolor": "#E5ECF6",
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "sequential": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ]
          },
          "xaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "automargin": true,
           "zerolinewidth": 2
          },
          "yaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "automargin": true,
           "zerolinewidth": 2
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "geo": {
           "bgcolor": "white",
           "landcolor": "#E5ECF6",
           "subunitcolor": "white",
           "showland": true,
           "showlakes": true,
           "lakecolor": "white"
          },
          "title": {
           "x": 0.05
          },
          "mapbox": {
           "style": "light"
          }
         }
        },
        "title": {
         "text": "Word2Vec - Visualising embeddings with TSNE"
        }
       },
       "config": {
        "plotlyServerURL": "https://plot.ly"
       }
      },
      "text/html": "<div>                            <div id=\"5c39eac1-352f-4e2a-af06-888c12c0a8bd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5c39eac1-352f-4e2a-af06-888c12c0a8bd\")) {                    Plotly.newPlot(                        \"5c39eac1-352f-4e2a-af06-888c12c0a8bd\",                        [{\"mode\":\"markers\",\"text\":[\"cars\",\"ai\",\"car\",\"vehicles\",\"systems\",\"safety\",\"driving\",\"providing\",\"personalized\",\"like\",\"enabling\",\"alternatives\",\"people\",\"performance\",\"improved\",\"human\",\"process\",\"understand\",\"machines\",\"allows\",\"language\",\"powered\",\"autonomous\",\"way\",\"convenient\",\"enables\",\"efficient\",\"transforming\",\"transportation\",\"computers\",\"without\",\"make\",\"decisions\",\"advancements\",\"diagnosis\",\"provide\",\"disease\",\"including\",\"healthcare\",\"customer\",\"faster\",\"discovery\",\"experiences\",\"interactions\",\"based\",\"voice\",\"support\",\"customers\",\"drug\",\"chatbots\",\"medicine\",\"leading\",\"informed\",\"surroundings\",\"perceive\",\"self\",\"role\",\"crucial\",\"plays\",\"increasingly\",\"field\",\"outcomes\",\"industry\",\"utilizing\",\"patient\",\"service\",\"automated\",\"common\",\"efficiency\",\"becoming\",\"valuable\",\"anomalies\",\"detect\",\"patterns\",\"uncover\",\"data\",\"amounts\",\"vast\",\"analyze\",\"algorithms\",\"programmed\",\"explicitly\",\"predictions\",\"learn\",\"subset\",\"learning\",\"machine\",\"work\",\"live\",\"industries\",\"revolutionizing\",\"intelligence\",\"extract\",\"insights\",\"assistant\",\"natural\",\"google\",\"alexa\",\"siri\",\"assistants\",\"virtual\",\"considerations\",\"detection\",\"object\",\"recognition\",\"image\",\"applications\",\"information\",\"visual\",\"interpret\",\"vision\",\"computer\",\"humans\",\"communication\",\"facilitating\",\"nlp\",\"processing\",\"ethical\",\"essential\",\"transparency\",\"agility\",\"nostalgic\",\"designs\",\"timeless\",\"history\",\"automotive\",\"place\",\"special\",\"hold\",\"classic\",\"experience\",\"thrilling\",\"speed\",\"capturing\",\"offering\",\"capabilities\",\"high\",\"known\",\"sports\",\"assist\",\"keeping\",\"lane\",\"control\",\"cruise\",\"adaptive\",\"appeal\",\"hearts\",\"fairness\",\"hailing\",\"promising\",\"intervention\",\"operate\",\"navigate\",\"bring\",\"expected\",\"future\",\"ownership\",\"access\",\"transformed\",\"apps\",\"ride\",\"enthusiasts\",\"services\",\"sharing\",\"upgrades\",\"modifications\",\"body\",\"jobs\",\"paint\",\"unique\",\"personalize\",\"owners\",\"customization\",\"technologies\",\"incorporating\",\"enhance\",\"world\",\"functional\",\"create\",\"components\",\"various\",\"assembling\",\"engineering\",\"designing\",\"complex\",\"involves\",\"manufacturing\",\"automobile\",\"around\",\"adas\",\"travel\",\"means\",\"revolutionized\",\"use\",\"accountable\",\"responsible\",\"ensure\",\"deployment\",\"development\",\"increased\",\"privacy\",\"vehicle\",\"features\",\"seat\",\"belts\",\"assistance\",\"driver\",\"advanced\",\"fuels\",\"fossil\",\"dependence\",\"emissions\",\"carbon\",\"reducing\",\"gasoline\",\"traditional\",\"friendly\",\"eco\",\"popularity\",\"gaining\",\"evs\",\"electric\",\"overall\",\"greatly\",\"braking\",\"lock\",\"anti\",\"airbags\",\"artificial\"],\"x\":[-3.5569868087768555,-3.345669984817505,-1.1631700992584229,-4.823192596435547,1.4799708127975464,-2.5234670639038086,3.3336856365203857,4.033675193786621,-1.097055435180664,1.8911492824554443,-4.095557689666748,-6.807097434997559,3.94748592376709,1.8565915822982788,-6.009923458099365,0.7721247673034668,-2.0102412700653076,-7.494566917419434,-5.4318718910217285,4.244304180145264,1.411681890487671,5.025524139404297,-3.7039387226104736,-8.411649703979492,-0.7734807133674622,0.2362508475780487,2.539975643157959,-1.1920018196105957,7.879120826721191,-4.905570030212402,4.294675827026367,7.35902738571167,-6.065314292907715,5.191819190979004,-5.435687065124512,0.6472213864326477,8.186429023742676,3.4760055541992188,6.416111946105957,3.8564202785491943,0.012898512184619904,-2.008552312850952,-3.6230485439300537,-4.542641639709473,7.342559814453125,-0.9007460474967957,-5.753945827484131,-2.073754072189331,-1.8793163299560547,-1.26099693775177,-5.374608993530273,-6.9775896072387695,3.9699244499206543,1.5151407718658447,4.543593883514404,-4.996665954589844,-6.796648025512695,-0.796980082988739,-5.379490852355957,-0.549635648727417,-9.417557716369629,-2.632219076156616,1.7778784036636353,1.6346489191055298,-7.801143169403076,7.2114739418029785,-9.09225082397461,0.9202542901039124,-1.1646091938018799,2.6558477878570557,-5.547433853149414,1.9413443803787231,-3.9593071937561035,2.2311630249023438,1.97158944606781,6.333553791046143,-2.814406633377075,0.1673775613307953,-8.345366477966309,3.1074397563934326,-3.0089330673217773,-4.5951337814331055,-3.7368662357330322,-5.906049728393555,0.728748619556427,0.3020936846733093,-1.9193578958511353,-3.302121877670288,-2.283527135848999,1.0822992324829102,1.084127426147461,0.7212051153182983,-0.2904839813709259,0.0067567480728030205,-6.094676494598389,-0.402170330286026,0.21640515327453613,5.0021562576293945,-2.4156148433685303,4.747771739959717,3.1189804077148438,6.701613903045654,7.1602253913879395,6.287492275238037,0.8026138544082642,6.452852725982666,-2.5331742763519287,-4.900890827178955,2.9736883640289307,4.881409645080566,-1.4666893482208252,-4.851039409637451,2.059319257736206,2.250230073928833,3.969353437423706,3.364682912826538,-0.05929283797740936,-0.6684280037879944,5.829762935638428,-0.979670524597168,-3.566016912460327,-1.4442508220672607,-2.588890552520752,-3.3296337127685547,-2.7897987365722656,6.3759918212890625,1.5154504776000977,-8.110496520996094,0.17165987193584442,-6.858039855957031,-3.5445096492767334,2.5935659408569336,2.885282516479492,1.8959251642227173,-5.664699077606201,-3.4687421321868896,-2.4294581413269043,-7.189571857452393,-2.8525397777557373,5.089686870574951,0.6043485999107361,3.9126408100128174,-2.7547128200531006,-5.008999824523926,-3.657480001449585,-9.955078125,3.3863651752471924,-3.036243200302124,4.4467034339904785,-0.05079968646168709,-0.4907218813896179,-3.036567211151123,4.6397705078125,1.6668617725372314,-1.2682503461837769,1.3137140274047852,-6.264095783233643,4.439817905426025,-6.315651893615723,-5.666043281555176,-0.6006585955619812,5.461076259613037,-0.24642640352249146,-2.679663896560669,-3.422029495239258,-0.013748754747211933,4.076957702636719,6.95869255065918,2.3676674365997314,-8.981572151184082,-8.151212692260742,3.518017053604126,-0.759570300579071,-0.42632266879081726,7.180819034576416,1.6771918535232544,-3.5160317420959473,0.8269298076629639,1.8569263219833374,2.010329484939575,-3.7082324028015137,6.012582302093506,4.621157169342041,0.9195900559425354,-1.7041544914245605,3.4600207805633545,2.8068747520446777,-4.070095062255859,3.804140567779541,-3.074394702911377,-4.536877155303955,-1.7962968349456787,-2.7294981479644775,2.9486894607543945,-6.354874134063721,-8.988039016723633,8.563080787658691,6.317517280578613,-0.9279513359069824,-1.9361271858215332,-9.039132118225098,7.131534099578857,2.7934367656707764,-5.968392848968506,-0.06545160710811615,3.7102925777435303,2.258908987045288,4.861087799072266,-0.05108696222305298,-4.49846887588501,-4.576083660125732,-6.251190185546875,-3.3793859481811523,1.4788758754730225,4.066930294036865,-0.9180363416671753,5.740225315093994,-1.3265256881713867,5.262291431427002,0.7175686359405518,2.9360263347625732,-7.619258880615234,5.261261463165283,1.1680265665054321,-4.450006008148193,5.410119533538818,-3.2600202560424805,4.232682228088379,-6.257768154144287],\"y\":[-2.366219997406006,1.4157464504241943,-8.381683349609375,-4.209931373596191,-5.979645252227783,-9.352097511291504,-1.8068146705627441,-7.144293785095215,6.021297931671143,6.609764575958252,-1.495269536972046,-0.639616072177887,-1.1818455457687378,-6.758322715759277,1.8673317432403564,1.815646767616272,0.16977642476558685,0.9171539545059204,-0.28289005160331726,-7.2281694412231445,0.7534630298614502,1.7798832654953003,2.778804302215576,3.790527105331421,-5.351175785064697,-3.560941457748413,4.08440637588501,5.833236217498779,-4.1663336753845215,0.3943403661251068,4.310480117797852,0.09831395000219345,0.24064360558986664,-1.9963525533676147,-1.881211519241333,4.180385589599609,-0.9039490222930908,-1.5471729040145874,0.5280646681785583,2.6742937564849854,4.0620808601379395,-6.513219356536865,-9.472671508789062,-1.1084809303283691,2.9618706703186035,4.339135646820068,-5.158017635345459,-5.029111862182617,4.206965923309326,2.9538791179656982,5.694890022277832,-0.4904268980026245,6.932773113250732,7.793333053588867,-4.360994338989258,4.613007545471191,2.7395997047424316,0.7967098355293274,3.114056348800659,-10.288593292236328,-1.2579964399337769,8.089855194091797,-2.3647656440734863,-3.7294106483459473,-2.29291033744812,2.8865668773651123,-3.3214125633239746,-7.906168460845947,-8.579545021057129,-7.941169261932373,-5.88237190246582,-2.719790458679199,-2.8582077026367188,-5.134355545043945,1.7824652194976807,6.741681098937988,5.792266845703125,-2.9503769874572754,-5.39525842666626,-5.694397926330566,5.918738842010498,-2.7563304901123047,-3.7066729068756104,-8.668044090270996,-5.506378650665283,-4.30855131149292,2.2824041843414307,0.5046464800834656,-5.011874675750732,-9.552413940429688,-7.896369457244873,0.7081783413887024,3.2146520614624023,-6.3050384521484375,-2.3505775928497314,-4.481626510620117,-1.7087410688400269,-4.868049621582031,-1.1532683372497559,0.6817308068275452,3.0158650875091553,1.4656099081039429,-7.470815658569336,-3.990213632583618,2.7786660194396973,-5.667232513427734,2.627997875213623,1.2886537313461304,4.768289566040039,-0.20373012125492096,-1.107700228691101,2.234194040298462,2.0606625080108643,-0.11353302001953125,-4.433785915374756,-0.016857877373695374,-6.968067646026611,-1.3603588342666626,-4.8725175857543945,-0.11705698072910309,-5.689452648162842,-3.5955538749694824,5.520202159881592,-1.0861376523971558,2.7821645736694336,-1.5905102491378784,-0.9805388450622559,-5.05893087387085,0.7785710096359253,4.528586387634277,-6.244285583496094,-7.922395706176758,1.2706235647201538,-4.036782741546631,-3.101402521133423,-8.064379692077637,-2.1249098777770996,-2.389819622039795,-3.1695380210876465,4.600878715515137,-1.2858613729476929,-9.436290740966797,-2.359637975692749,0.10975052416324615,3.9018781185150146,0.8384222984313965,-8.780622482299805,0.42526963353157043,1.231698751449585,-0.5188021063804626,-2.5711145401000977,-1.3426936864852905,-7.813931465148926,-3.1462206840515137,-2.4722371101379395,-1.0157849788665771,-4.328769207000732,6.737553596496582,-6.855597019195557,-2.030759572982788,-4.499425411224365,1.6545190811157227,6.392116546630859,-3.9923131465911865,3.8821256160736084,6.069917678833008,-0.25490161776542664,-0.4267646372318268,1.0929652452468872,3.285449504852295,1.4321503639221191,1.9575225114822388,8.271814346313477,1.9291417598724365,-1.3109941482543945,7.994966983795166,-5.386274814605713,4.301878452301025,2.89518141746521,6.410716533660889,6.38617467880249,-1.6168067455291748,6.841063976287842,0.05820506066083908,1.640087604522705,-2.8759775161743164,4.6040825843811035,-6.035190105438232,2.152789831161499,-3.5014779567718506,4.5739216804504395,-3.7116336822509766,0.06453826278448105,-5.643316268920898,-3.0912952423095703,-1.396384835243225,-2.5754761695861816,-3.2645411491394043,4.257584095001221,-6.593636989593506,-3.2896602153778076,-7.411569118499756,-1.3092409372329712,-6.6785125732421875,1.808240294456482,-0.3113737404346466,5.845041751861572,3.1287853717803955,-2.0365591049194336,-4.636128902435303,2.706270456314087,6.708128452301025,-9.077188491821289,-1.6567959785461426,-4.118268013000488,0.3064400553703308,-0.6685023903846741,0.6971976161003113,4.906248569488525,0.9191274642944336,-2.744675636291504,-4.471009731292725,4.209408283233643,3.0564539432525635,-7.672701358795166,-2.7060420513153076,-7.385237693786621,-2.326617479324341,-1.1584327220916748],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Word2Vec - Visualising embeddings with TSNE\"}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('5c39eac1-352f-4e2a-af06-888c12c0a8bd');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "def reduce_dimensions(model):\n",
    "    num_components = 2  # number of dimensions to keep after compression\n",
    "\n",
    "    # extract vocabulary from model and vectors in order to associate them in the graph\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)\n",
    "\n",
    "    # apply TSNE\n",
    "    tsne = TSNE(n_components=num_components, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "def plot_embeddings(x_vals, y_vals, labels):\n",
    "    import plotly.graph_objs as go\n",
    "    fig = go.Figure()\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='markers', text=labels)\n",
    "    fig.add_trace(trace)\n",
    "    fig.update_layout(title=\"Word2Vec - Visualising embeddings with TSNE\")\n",
    "    fig.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "\n",
    "plot = plot_embeddings(x_vals, y_vals, labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
