SPEAKER 0
Hi, everyone. Welcome back to Introduction to Image Processing. Today's next we're going to cover on digital images and processing. Let's begin. So what are the learning outcomes for today's lecture? So we're going to identify what digital image formations are occurring and using colour images, colour spaces and intensity transform. We begin with digital image formation. Now information acquisition is key. Okay. Now, imagine this. You're looking at an environment, a scene, and you want to capture that element of that scene. Now, this is where we need to form the relevant image scenery that we're going to capture, acquire it, and then converted into a digitised image, which will then we'll be able to perform further manipulation on it. Now two important processes plays a fundamental role in performing a digital image formation. It is called sampling and Quantisation. Sampling is actually the process of digitisation of the spatial coordinate. Whereas Quantisation is actually the process of digitisation of the light intensity function. Now, I know the jargon might be a bit confusing. So let's put this into perspective. Now sampling determines the spatial resolution referring to this piece. Whereas the quantisation determines the grey level colour or the radiometric resolution. So looking more at the intensity of the colour. Not very profound. Simply a famous question that tends to pop up most of the time is how many simplicity I me how many pixels in the image. Okay, now let's dive into this further. So samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed, meaning that when you capture the samples, the rate at which you're capturing it must be maintained throughout. Okay. Now, one problem that might occur if the rate is not maintained is under simply no sampling at a rate that is to corals or is below the necklace. We can cause a problem. This problem is what we call a rising. It's caused by artefacts that result from undue sampling. Now let's get a better understanding of these two allies. So a lasing occurs when two signal images in our context here become unrecognisable when assembled. In our case, the two signals are the true image, the image that would be seen if there were no quantisation and the one reconstructed by the human vision system from a sample image. So let's look at this two frequency example here. The top one, which is captured by using adequately simple signal. You see, every one of the signal is evenly distributed, whereas the bottom one is a result of and assembly which causes the signal to be allies because of it. So this is something that you want to avoid when we are performing sampling. When it comes to Quantisation. Another question also pops up How many grey levels to store? Good question. Right. Do you think now these process is to determine the number of levels of colour intensity to be represented at each pixel? Now, remember that I mentioned in last week's class that each pixel carries three colour channel green in blue. Now this freak in blue also determines the greyscale of an image. So let's look at this, for example. So when you are doing quantisation, the sampling in the quality occurs naturally during image acquisition, but can also be applied to existing images, especially during resizing compression. So by changing the level of colour intensity, it also changes the representation of the entire image altogether. So for this example here you can see that the four different images having different level of colour intensity, one having two, five, six, one having 64, one having 16 and one having four. If you guys can look at the four images and you look at the different intensity of the output image resulting, for me, it's pretty clear that having a form may not be the best level because you tend to lose a lot of information you need. So reassembling and resizing is one of the most famous of basic form of processing. Now, when done simply you need to compute a summary pixel value from each local area. So look at this example here. You have four values in the rate box and you only need to pick one. And these can either be done by looking at these mean or weighted mean. No less of empty allies. So now we have the audio signal. We have a smooth signal key. Now, a lazy can be introduced when an image is very simple. So if the if the sampling rate on the new image is less than the Nyquist rate of the original, that is where you will have an assembly, right? So smooth out high frequency signals before sampling. So is it possible to see the Elias? Now using the same example, maybe use in a couple of slides ago. Now let's look at what happens when I'm sampling is is occurring. So when I'm sampling the need to interpolate from the known values to produce an estimate at the unknown pixel. As you can see here, we're having several pixels having a question mark. Okay. So one way of doing this would be to average the known values in the local region, set that on each unknown pixel, feed some kind of function to non values. This will we'll try to get a proper representation of this neighbouring and also make sure that the picture still has information that makes sense to the viewer. No record position. Okay, so the pixel values are integers in a fixed range. Grey level resolution can be dropped by draining each pixel value by a constant. But there is a side effect. It's always you can't increase grey level resolution of a single image. Okay. So super resolution methods exist and combine multiple exposures of the same scene and so have more than one measurement of each pixel. So when it comes to digital information, what are the key points that we need to remember? Sample composition are often under user control. Yes, you can do the image capture and have a significant effect on the image. Both can be changed to some extent of the capture, but the resulting images are often approximations. Be aware of the night quiz rule. When choosing an image resolution, it can introduce artefacts that are difficult to remove. Now let's look at acquiring and using colour images. Now, something important that everyone should be aware of is the fundamental understanding of image processing. Each pixel is a grey level image containing one value colony, and the head contains three. As I mentioned several times between last week and this week's lecture, it r g B stands for green and blue, not red. Green and blue model inspired by the retina is an obvious choice for the using cameras. Some expensive cameras use more complex optics and tree ccds to capture each colour channel independently necessary for some scientific purposes. Not for general use, of course, but a single CCD colour camera used by pens in which you can see the example on your right. No. Each CCD cell lies under a red green on blue filter. So let's better understand what a buyer betting is. Now, one colour value is measured. But to estimate the pixel look here B is measured. Representing blue green algae is the mean of the four naevus and R is the mean of the diagonals. No more green elements. I as eyes is more sensitive to green light than red and blue. That's why I think many of you you're looking at is by impacted. The eyes are drawn to the green. Okay. Because our eyes are more sensitive to green light. The pattern is repetitive, so there are a finite number of sets of filters. They can surround each sensor. Now representing colour in RG three green and blue. Now some interesting fact for everyone is the most common starting point. Retinal cells are sensitive to treat primary colours. Yes, you guessed it everyone. Green in blue. It is light and not pigments. It's why it's so famous. No. RG b is additive, meaning that other colours can be made from mixing this together. Now in this slide we're looking at it. Example RGV. Now the image is starting. From the left is a colour image. And red, green and blue is actually focusing in only one colour channel. Now colour images are 3D arrays. Yes, the 3D here refers to green in blue. Display software interprets them to produce a colour view. Now a famous point of view would be the colour was just greyscale. Why do we need greyscale? Sometimes we want a single value each pixel. For certain reasons. One be it makes processing easier. It reduces the amount of information from as compared to having it intrigue. And it makes some opportunities simpler. Not many image processing methods were developed for a single value image. The value here usually is called intensity or grey level. Now early CCDs only produce grey level where it's very random. But we can calculate the grey value using a simple average of rate green in blue. But our eyes are more sensitive to the green light. So what do we do? We convert RGV image to a greyscale using this formula. And if you can see the formula, a higher weightage is put onto the green channel cos our eyes are more sensitive to the green colour channel. Now let's look at other commerce pieces. Let's walk down memory lane. Just a few minutes ago. So we know that our TV is based on the fact that our retinal cells are very sensitive to these three primary colours is the most common starting point light and not pigment. And it's mainly used to create the colours by mixing it together. So IGB is the fundamental for everything that we build upon right now. Of course, authority cargo spaces exists. RG BS Okay. No about it. But there are other general advocates of specific cargo spaces. So for instance, looking at the image on your right here, if you want to work with plants, which is what is being shown an image, you might want to use. G. G. Which is greenness, though that's the only important thing in the image, the greenness of the plant. So this formula comes into play. Now, another famous of space that many people tend to use is HSV. Now, HSV is more based on colour rather than light. HSV stands for hue saturation in many. You know, hue focuses on what general colour saturation, how strongly colour is and value, how bright, what it is. Now that we know what he as me stands for. Now these are slight with a much more better understanding as to what he, as we speak in general are opposite. Now that may have been introduced to space, he chose v. G b. Next question is what makes his be so different in our TV space? He has v separates kind of from intensity, making it less sensitive to illumination changes. Now you might ask what on earth is illumination changes, but illumination changes occurs due to the fluorescent light that is normally in rooms that we are sitting in or moving around. Okay. So that can cause changes that will affect RGV colour space more than he case we colour space. Because of the illumination changes, it does affect our skin colour. Okay. Now, of course, this will cost r.g because these two go really off. Okay. Now, that is white. HSV presents a better opportunity. But we do always need all three values. Okay. Human skin is tightly clustered in the hygiene space. That is why we tend to focus on the hue and saturation only. Okay. Now, the middle image you see here is what we call a coloured histogram Recoveries in the future lectures. But is a very good representation of how each space represents human skin. Okay. And from that we can basically reproduce the output image on the right, which is the histogram back projection. Now heavy looking colours. I see a few key parts that we need to take away and remember. And one colour space transformation is an example of a point process. Automotive 3D cargo spaces exist. And not all application require all three colour planes to be considered and application specific colour spaces exist. Now let's look at intensity transform. Intensely stressful. Hmm. What is it about? To better understand this lesson. Use an example to help convey the message. Okay, so we have two images here. One is a soft image on the left, and one is a target image on the right. Now, target pixel value depends only on the source pixel and maybe some parameters. Now, in order for us to get that. That is what we call intensity transfer, meaning that we are transforming the intensity from the source pixel to the target pixel. And we only need to worry about that particular pixel value in that particular location. Of course, with some parameters adding into it as well. So when we talk about intensity transform, we to talk about linear transform. So two commonly used point processes are multiplication by an addition of a constant here being represented by A and B respectively. Now what this and B do. So E is the game. It controls the contrast, whereas the B is the bias which controls the brightness. Okay. Now, these may not be clear in this slide. So why don't we try to put this into an image so that you can see the effect of both of it? So in this light, we're looking at the game, but we're keeping bias at a constant zero. So we put in gain as 1.1. We may not see much of a change in the output given that gaining control of the contrast to the contrast looks pretty similar. However. We can see it change in the colour histogram when we manipulate bias. So here bias is increased to 60. Whereas we're keeping a gain at 1.1. And you can immediately see that the colour histogram is very, very different. There's a reaction in it because the brightness has been affected by the manipulation to bias. Now here we look at negation and the process in on how we we produce a negation. Of course, the image here would be a greyscale. You can see on the left and when we perform this mathematical equation, we will get the image on in the middle. So that is a negation output image. Now, negation is a very, very good process. Merely because it's often used to make fine details more visible. A good example is being used in digital mammograms. Keep. Now let's look at the dynamic range. We know digital images are simple. They contain a fixed number of guitar values. Digital image representation can only store a fixed number of values. Intensity transform to produce values that are outside that range and can be stored clustered in a small part of that range and is hard to distinguish. However, some intensity transform the data in that particular range. So what must we do? Contrast, stretching is one of the solution. Wade's purpose is to convert a source image using intensities ranges from one range to another. Now, in order for you to perform the contract strategy, this formula must be implemented. And the two images that you see in front of you are the result of it. So you have the image on the left, which is in one range. And after you perform contrast stretching, you can see that the contrast is much modern, brighter, nicer, and that produces the output image on the right. Now let's do it. Nonlinear transform. It's called thresholding. It can be done to highlight grey levels or as the simplest form of image segmentation. So you have a threshold value, and any values that fall above that threshold is accepted. And any values that fall below that threshold is rejected. So this slide we're looking at grade levels, Lexi. So we highlight a specific range of intensity and you can serve two purpose. One, it can reduce either great levels to lower level or preserve them. Now, what happens when you apply this to images? This slide presents an output of applying the brain that works like this. The input image is the emission of OLAF. The middle image is when high intensity selected others are reduced to low level key. Now the image on the right, on the other hand, is selected. Intensities set to black are this are preserved. So you can see that by using green level slicing you can either reduce the other grey levels to a lower level or preserve certain selected intensity. This slide looks at common currency. Just to be clear. Comma here is in no reference to the one that created the incredible. Okay, it's different. Yeah. Okay. Got my correction. What's it about? So when an image is displayed on the screen, the highway use effectively apply to intensity transfer. You said a voltage proportional to the intensity of a pixel. The screen displays intensity that is related to that, but not how you may think. Now here you can see the example from from the left to the right whereby the 2.5 voltage is depending on the device. Now, how do we perform these gamma correction? You mean us? In performing a correction, we need to transform the image so that he generates a voltage which will display what we want to. In doing so, we are actually going to create a new image. So in order to create a new image, we normally would have to use gamma correction in the form of this equation here, the one in the bottom, right. So by producing the output image using that formula, we will get a gamma correct output which. Now let's look at key points to remember. We know that point processing operates on each pixel independently. Linear processes change the appearance of the whole image, whereas nonlinear processes can differentiate object or image regions. In summary, what we have covered today is we have looked in great detail on digital image formation, acquiring and using colour images, the different colour spaces available and also intensity transform. As always, if you have any questions, you know where to find. With that brings us to the end of today's lecture. Now, next week, we're going to call on linear filters so that everyone stays here. And I'll see you guys in just.
