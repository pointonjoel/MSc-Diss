SPEAKER 0
Hi, everyone.

SPEAKER 1
Welcome back to another lecture on Introduction to Image Processing. Today we're going to cover derivatives and edges. Let's get started. So in today's lecture, we're going to defy derivative filters sharpening what is detection? Detection using the first derivatives, edge detection using the second derivative and the famous canny operator. So let's get a better understanding of what derivative filters are. So to understand gravity, we first must take a few step back in a one dimension. Image features are often categorised by changes in intensity. And you can look at this in much more clearer by the diagram below here. Here you have an image strip whereby the grey level profile on the left and you can see that, you can see the dots and the lines are changing, whereas it's going down and it's going up. So this is showing the changes in intensity. Now, in order for us to understand in regions of the image where the H is, it is where the intensity change. So that's why knowing this information is very, very important. And that brings us to our first derivative. So how do we perform the first derivative? Of course, the first derivative can be approximated by the following equation. Okay. The difference between the neighbouring values and measures the rate of change of the function. So in order for you to know the changes in intensity, you need to look at your neighbours. Now they have a better understanding of how to apply the mathematical equation given the previous slide. This diagram will provide with a clearer understanding. The first sequence here refers to the raw data, and the second sequence refers to the first derivative. Okay, so the raw data, you can see that when we look at each of the neighbours, you can see there is a change in intensity at a certain point. At a certain point it maintains. So by applying the first derivative, you can find out at which juncture there is a massive change in its intensity. By knowing that you it might give us a clear indication is there h appearing there? As you can see from here, there are a steady decline and suddenly a big jump and a decline again and so forth. So this is where the intensity changes are drastically happening. Now the first derivative presents some form of change in intensity, but it may be too sensitive. And if a noise is introduced, it might consider it as an edge and it will kind of flag out as there's a change in intensity. So that's why the second derivative was introduced. Now the second derivative is calculated by using this function. It simply takes into account the values of both the before and after of the current value, making it a bit more consistent in how it reports the results. Now derived from estimating the first derivative at x plus 0.5 and x -0.5 and computing the derivative of the resulting data. So the second derivative is obtained by building from the first derivative. Now let's put this into some perspective. So you have the changes in intensity. Okay. And you're looking at plus 0.5 -0.5. Right. So by applying this formula here, you are actually getting the actual intensity change of location one by looking at zero and two. So by applying the second derivative, you are getting a better sense of the intensity at that particular location. Now let's revisit our diagram early on. Again, the first sequence is the raw data. The second sequence now presents the second derivative. We know that the first sequence here presents to us the intensity, but now with the second derivative, you can see that there is a much more consistent. I look at places where the intensity really experienced a drastic change. Instances that you see there is zero, meaning there is no change. Whereas at juncture like here, you can see there's a drastic change of intensity and it's the highest among all. So by providing the second derivative, it kind of gives us a much more consistent representation of the intensity changes. Now derivatives in two dimensions. So earlier on we are just looking at 1D. Now second derivative generalised to 2D is quite easy. Implementing a first derivative in 2D is a bit more complex. Okay. So in order for you to do that, you need to perform the following function. So for a function f, x, y, the gradient of f at coordinate x, y is given as the column vector. Okay, so this is the gradient. Now remember, computation of the first derivative can be done by convolution alone. Now let's let's explore this further. So when you perform the first derivative filtering, what can you do? So in order for us to do that, we need to do a bit more mathematical, what you call a breakdown. So the magnitude for the first derivative vector can be done like this, which can be simplified even further. So this so the gradient of x and Y will need to be absolute. And by computing the summation of gradient x and Y, that's what you will get the derivative. Now, there's many first derivative filters that have been proposed along the years. Among them, Roberts cross operators and the famous Sobel operators. Now, most of these operators are commonly associated with edge detection, as you can see from the diagram. They are different masks for X and Y. Now let's look at image sharpening. So why do we want to perform image sharpening? So edges are important, but sometimes we want to enhance them without affecting the rest of the image. So how do we do that? We take the original image. We perform Gaussian, which I think many of you is an expert of it by now. You subtract the smooth version from the original to make the unsharp mask. And then you add the mask to the original to make the edge appear more obvious. Now let's apply this for steps into a diagram for clearer understanding. So you have the original signal. Okay. You have the Gaussian, which you kind of blur the edges a bit. Then you subtract the smooth version from the original one, which will give you the sharp mask. You apply the unsharp mask onto the original, and that's where you will get the sharpened signal. Now let's apply this to an image. So by performing this, you're actually are making edges noticeably sharper, even if they are noise. But sometimes too much, you can make it a bit more sharper. Okay. So you can see from this example here, you can see the sharpen one on the left and the original one on the right. Same goes for this one here. The top one is the original. The bottom one is sharper. Now let's visit our dirty filters once again. So in our short filter enhances the ages by comparing the original with a smooth image. It relies on the smoothing effect of a Gaussian function, introducing a difference between original and processed image Parameterised by a sigma. Okay. Simple but effect is hard to predict. So hard to parameterise. Okay. Because the sigma will determine how sharp or unsharp it may be. Now a more direct way to highlight edges and other features associated with a high image gradient is to estimate its derivatives.

SPEAKER 2
How do you do that?

SPEAKER 1
So image sharpening with derivative. Let's have a look. The second derivative is more useful for image enhancement than the first. Okay. Stronger response to fine details. Simpler to implement. Now the most common and famously used sharpening filter is the Laplacian. It's isotropic one of the simplest sharpening filter. Straightforward digital implementation near convolution. Okay, now let's look at Laplacian in more detail. So this is the formula that you would use if you want to implement a Laplacian. Extending it further will give you this, which will eventually give you this mass filtering which you can use to perform the convolution. Now, what happens when you use this mask on to an image? Let's have a look. So by performing Laplacian, you're highlighting the edges and other discontinuities using original image. You're performing a filtered image. And this is how the output will be. Okay. Now this is the image scale for display. So by scaling it to ensure that it's within the 255 range of a colour and this is what you get. But the one that we want to know is and the one we want to really, really see is the one here in the middle here. A Laplacian filter image. Now, the good thing about this is that it's a single enhanced operator. Okay. You're doing it only one time. Now, the good thing about this is that it performs a single operator. The convolution with this operator performs image sharpening in a single step. So you have your input image. You apply your convolutional filter and then you get the output, which is a sharper image. And this is done by only having one step. Now, let's take a closer look at this. So you see that the images there are certain regions here that is blur on the left hand side, but is less blur on the right. Clearly showing the effect of the single enhancement operator. Now some formations take second derivative measure across the diagonal into account. Diagonal means the one and the sides. Okay, so you have the basic one and you also have the extended. So when you apply the extended one, you can see that the image become even more sharper because you're taking into account the diagonals of the of the of the calculation. Now let's look into what is detection. Now let's get a better understanding of detection. So in many image analysis and computer vision processes and application edge detection acts as the first step to do what to mark points at which image intensity changes sharply. Now this is defined as the edges. Okay. Now sharp changes in image properties reflect events and changes in the world. This is only an assumption, but it is usually true. So let's look at the theory of detection. So an idealist image of an age, which is what we want after the moment, the first directive, we have a peak. But to detect the ages, find the picks in the first derivative will not be enough. Therefore, you apply a second derivative or intensity or zero crossing which will detect the changes in the intensity. So the second derivative is actually performing a zero crossing building on the first derivative operation. And this is an example of detection by using a Matlab code. So you can see that this image is the input image and you want it to extract the edges of the image. And after applying a technique called canny, which you will cover in a short while, this is the output that we get. Now let's look at each section using the first derivative. Now mentioned earlier, there are two famous types to compute the gradient in the X and Y direction. The robot cross operator and the Sobel operator now applied separately, and the results are combined to estimate the magnitude. Now some you take note of significant peaks in magnitude of. First derivatives are high. Okay. Apply a threshold all peaks higher than the threshold value are significant while all others are ignored. So applying a first derivative detection, you may need to apply a thresholding approach to ensure that your output image is more visible or clearer. But having said that, if you apply a threshold which is too low, you incorporate noise into your output. Whereas if you kind of apply a too high of a threshold, information of the object will disappear. Okay, so it become part of the background. Now let's look at magnitude and direction. So a bit of mathematics here for an image function XY. The gradient magnitude y is given by this. Okay. Looks familiar to you. Isn't this similar to the Euclidean distance formula? Interesting to compute the gradient direction. Okay. Give the direction to the steepest image gradient. Here you will use a tangent and you find the difference in the y direction divided by the difference in the x direction. Now this gives the direction of a line perpendicular to the edge. Now let's look at Roberts cross operator. It's very quick to compute for pixels. Only subtraction and addition. But it is very sensitive to noise and only gives a strong response to very sharp edges. So have a look at this. You have the input image. You have the cross operator which has noise in it. You threshold it hoping to get a much more better output. But at the same time, you do sacrifice certain information from the original image. So what happens if you put Roberts versus Sobel? Now both use a super supplied threshold. Sobel is still in use till today. Robots, on the other hand, less common. Now larger server operators are more stable in. And if you can see from this example here, robots tend to miss out on important edges, whereas Sobel captures it, making it much more reliable as compared to robots. Okay, now let's do it. Detection using the second derivative filters. Again, a quick recap. We know for a fact that the first derivative is actually finding the pig. So the second derivative now will be finding the zero crossing. Let's find out more about this. So one of the famous secondary filter is Mark Hildreth. It's a biologically inspired approach. Gaussian smooth compute Laplacian or cobbles with a Laplacian of Gaussian. And you apply this formula to obtain. Okay. So as we are talking about Gaussian, of course, we are looking at the normal distribution. Okay. And your sigma will determine how far away are you looking at from the mean? Now let's look at the practical implementation of those. You have two ways of implementing it, either by a five by 5 or 17 by 17. You have the input image filter. Then this filtered image will be applied against the Laplacian of Gaussian mass, producing a zero crossing output, as you can see on your right here. Now difference of Gaussian. So the Gaussian can be approximated by difference between two Gaussian function. Okay, so this is the approximation and this is the actual Laplacian. So you can find there are a difference between two Gaussian function. Now let's get a better understanding of the difference of Gaussian filter. Okay. By looking at the formula here, we know that the Sigma one and two will play a big role in getting the required output, after which you will find the difference. Okay. You will subtract one against the other producing the final output. Now, by using Lina as our test image, you can see that in one image. She has a sigma of one being applied, second one having a sigma of two. Look at the image. Both image closely. You can see the second image here. Lina is a bit more blur compared to the first one. Certain features in the second image here, you can see it's not as sharp as the first one. Okay, so research have shown that a ratio for best approximation is about 1.6 in between the two. Some people like square root of two. Okay. Once you have done that, you perform the subtraction and you get the subsequent result here. Now that we have a better understanding of the difference of Gaussian. Let's look back at Mar Hydra. Now the choices of Sigma gives flexibility. Okay, so if the input image being applied with different sigma values and you can see the resulting image here. If a sigma value is too low, you incorporate noise. Whereas if you put a sigma value is too high, certain information of the object is also removed. So finding the middle sigma value here between 2 and 3 will produce the ideal result from the input image. Now let's look at the difference between the first and second derivative methods. So the first derivative focuses on the pigs, whereas the second derivative focussed on the zero crossing. So in the first derivative, strong response at ages, but also responds to noise, pick detection and threshold selection need care. As for second derivative well defined, easy to detect must form smooth connected contours and tend to run off corners.

SPEAKER 3
However.

SPEAKER 1
The first derivative methods are much more common in practical application. Why, you might ask, in part because of a person called John Kenny. Which brings us to the famous canny operator.

SPEAKER 4
Now, what can he do?

SPEAKER 1
So John Carney tried to find an optimal age detector, assuming a perfect set age in Gaussian noise. So optimum in his definition is a good detection. It should mark all the ages and only all the ages. It should be good in localisation, meaning the points mark should be close to the real ages as possible. It should have minimum response. Each age should be reported only once. So canny. Use calculus of variation. Find the function which best satisfies some functional. Now let's look at canny operator in more detail. So the optimal detector was a sum of four exponential two, but it's very closely approximated by the first derivative of a Gaussian, meaning the Gaussian smooth image. Okay, now it gives a cleaner response to a noisy edge than square operators. Most implementations are 2D, Gaussian smoothing plus robot style derivatives. So having a look at canny output here and and comparing it with robots, you can immediately see there is a much more smoother and cleaner what you call output when you apply. Now let's look at the non maximum suppression. The canny operators response is cleaner than Sobel and Roberts. But it needs an explicit step to enforce the minimum response. Let's find out more. Now, look at this example here. How to turn this thick regions of the gradient into curves. Now thresholding raw operator response would leave thick lines. So check if the pixel is a local maximum along the gradient direction. Select a single maximum across the width of the image. And this is how you do it. Okay, so you're finding a single maximum across the width of the edge. So how do we perform thresholding with hysteresis. We know that simple thresholding tests each pixel independently. Ages are not really independent. They make up lines. The industry standard age thresholding method allows a band of variation but assumes continuous ages. Now users still select parameters, but it's easier, less precise. Now the effect is to keep with ages. If they connect to strong ages as long as one. The strong ages are really strong and two, the weak ages are not really weak. Is an example of a weak and strong age whereby the thick one being the stronger one and.

UNKNOWN
Thinner one being the weaker one.

SPEAKER 1
Now by performing thresholding, it does present a problem pixel along these edges. As you can see here, did not survive the threshold. So you can see that certain information are missing. Now, the idea here is to perform his services because his services fills in most of the gaps. So let's put this into perspective with this example here. Okay. So by using a low or high threshold, the east side effects with a low threshold, you introduce unwanted noise with a higher threshold. You lose valuable information of the object. So by performing hysteresis, you get the best of both worlds. You reduce the unwanted noise, yet still preserving the important information that defines the given object. So what can he do? Can be sure that the first derivative of a Gaussian smooth image is the optimal way to detect step edges in noise. Explain why. First, derivatives are a good idea. Okay. Design the industry standard thresholding method. A non maximum suppression and a thresholding with hysteresis. These two approaches effectively solve the detection problem. EPI. In summary. In today's lecture, we covered derivative filters sharpening. We got to know what age detection is. And how it uses different derivatives to compute the ages, either using the first derivative or second derivative, and also got into a better understanding of the famous canny operator. As always, if you have any question, you know where to find me. With that. Everyone brings us to the end of today's lecture. Next week we're going to talk about whole image methods. Till then, take care of Stacy, and I'll see you guys soon. Cheers.
